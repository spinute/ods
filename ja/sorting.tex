\chapter{整列アルゴリズム}
\chaplabel{sorting}

この章では、大きさ#n#の集合を整列するアルゴリズムを紹介する。
データ構造の教科書なのに、整列アルゴリズムの説明が始まるなんて、奇妙に思うかもしれない。これにはいくつか理由がある。
例えば、この章で紹介するクイックソートはランダム二分探索木と深い関係があるし、ヒープソートはデータ構造のヒープと深い関係がある。

\secref{sort-by-comparison}では、比較に基づく整列アルゴリズムを3つ紹介する。
3つとも、整列にかかる実行時間が$O(#n#\log #n#)$であるような整列アルゴリズムである。
さらに、この実行時間が漸近的に最適であることを示す。
つまり、比較に基づく整列アルゴリズムでは、最悪実行時間にせよ平均実行時間にせよ、最低でも約$#n#\log#n#$回の比較が必要なのだ。

なお、これまでの章で説明してきた整列済み集合#SSet#や優先度付き#Queue#であれば、どれを使っても$O(#n#\log #n#)$の時間で整列可能なアルゴリズムを実装できる。
例えば、#n#個の要素を#BinaryHeap#または#MeldableHeap#に#add(x)#し、続いて#remove()#を#n#回繰り返せば、順番に要素を取り出せる。
あるいは、何らかの二分探索木に対して#add(x)#を#n#回実行し、そのあと行きがけ順（\excref{tree-traversal}）で木を巡回すれば、整列された順で要素が見つかる。
ただし、どちらの例でも、わざわざ作ったデータ構造の限られた機能しか使っておらず、かなり無駄なことをしている。% TODO caprice ここ自然に訳したい
整列は、可能な限り速く、単純で、省メモリな手法をあえて開発するだけの価値がある、極めて重要な問題なのである。

章の後半では、比較のほかに使える操作があれば、さらに優れた実行時間で整列が可能なことを見る。
実際、配列のランダムアクセスを使うことで、#n#個の要素$\{0,\ldots,#n#^c-1\}$を$O(c#n#)$の時間で整列できることを説明する。

\section{比較に基づく整列}
\seclabel{sort-by-comparison}

\ejindex{comparison-based sorting}{ひかくにもとづくせいれつ@比較に基づく整列}%
\ejindex{sorting algorithm!comparison-based}{せいれつあるごりずむ@整列アルゴリズム!ひかくにもとづく@比較に基づく}%
この節では、マージソート、クイックソート、ヒープソートという3つの整列アルゴリズムを紹介する。
3つとも、配列#a#を入力すると、$O(#n#\log #n#)$の（期待）実行時間で#a#の要素を昇順に整列する。
いずれも\emph{比較に基づく}アルゴリズムである。
\javaonly{第二引数#c#は#Comparator#である。
\index{Comparator@#Comparator#}%
これは#compare(a,b)#メソッドを実装するオブジェクトである。
\index{compare@#compare(a,b)#}%
}
整列するデータの型はなんでもよい。
ただし、データの比較を実行するメソッドがなければならない。
\secref{sset}と同様に、以降では#compare(a,b)#というメソッドがあるとする。
#compare(a,b)#は、$#a#<#b#$なら負の値を、$#a#>#b#$なら正の値を、$#a#=#b#$ならゼロを返すものとする。

\subsection{マージソート}
\seclabel{merge-sort}

\ejindex{merge-sort}{まーじそーと@マージソート}%
\emph{マージソート（merge sort）}は、再帰的な分割統治法の例として古典的なアルゴリズムである。
\ejindex{divide-and-conquer}{ぶんかつとうちほう@分割統治法}%
配列#a#の長さが1以下なら、#a#はすでに整列されており、何もする必要はない。
そうでなければ、配列#a#を半分ずつ、すなわち、配列$#a0#=#a[0]#,\ldots,#a[n/2-1]#$および配列$#a1#=#a[n/2]#,\ldots,#a[n-1]#$に分ける。
#a0#と#a1#をそれぞれ再帰的に整列し、整列済みの#a0#と#a1#とを併合することで、整列済みの配列#a#を得る
\javaonly{\footnote{訳注：Javaの#Arrays.copyOfRange(T[] original, int from, int to)#メソッドは、#to#をコピー対象として含まないことに注意。そのため、擬似コードでは#a.length/2#が#from#と#to#で1回ずつ用いられている。}}
。

\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
\pcodeimport{ods/Algorithms.mergeSort(a)}
\figref{merge-sort}にマージソートの例を示す。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
%XXX: mergeSort(a, c) only for cpp edition?
  \caption{#mergeSort(a,c)#を実行する様子}
  \figlabel{merge-sort}
\end{figure}

#a0#と#a1#の併合は、整列と比べればかなり簡単である。
#a#に要素を1つずつ加えていけばよい。
#a0#か#a1#がどちらかが空になったら、空でないほうの配列の残りの要素をすべて#a#に加える。
それまでは、#a0#の次の要素と#a1#の次の要素のうち、小さいほうを#a#に加えていく。
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}
\pcodeimport{ods/Algorithms.merge(a0,a1,a)}
% XXX: 原著、nの説明がなかったので定理11.1の内容を参考に補った
#a0#と#a1#の要素数の合計を#n#とすると、#merge(a0,a1,a,c)#では、#a0#または#a1#が空になる前に最大で$#n#-1$回の比較を行う。

マージソートの実行時間を求めるには、\figref{mergesort-recursion}のような再帰的な木を考えるとよい。
いま、#n#が2の冪乗であると仮定する。
このとき、$#n#=2^{\log #n#}$であり、$\log #n#$は整数となる。
マージソートでは、#n#個の要素の整列問題を、「$#n#/2$個の要素の並び替え」という2つの問題に分割して考える。
分割した2つの問題をそれぞれさらに2つの問題に分割することで、「$#n#/4$個の要素の並び替え」が4つになる。
この4つの問題をそれぞれさらに分割することで、「$#n#/8$個の要素の並べ替え」が8つになる。
これを繰り返して、「2個の要素の並べ替え」が$#n#/2$個になり、最終的には「1個の要素の並べ替え」が#n#個になる。
大きさ$#n#/2^{i}$の問題を解くには、すでに解かれた2つの部分問題の解答をマージしながらコピーするのに、$O(#n#/2^i)$の時間がかかる。
大きさ$#n#/2^i$の問題が$2^i$個あるので、
大きさ$2^i$の問題のために必要な時間の合計は次のようになる（まだ再帰的に数え上げていないことに注意）。
\[
       2^i\times O(#n#/2^i) = O(#n#)
\]
よって、マージソートに必要な時間の合計は次のようになる。
\[
   \sum_{i=0}^{\log #n#} O(#n#) = O(#n#\log #n#)
\]

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{マージソートの再帰木}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}

次の定理は、以上の解析に基づいて証明できる。ただし、#n#が2の冪乗でない場合も扱うので、少しだけ注意が必要になる。
\begin{thm}
  \javaonly{#mergeSort(a,c)#}\cpponly{mergeSort(a)}\pcodeonly{merge\_sort(a)}の実行時間は$O(#n#\log #n#)$であり、最大で$#n#\log #n#$回の比較を行う。
\end{thm}

\begin{proof}
$#n#$についての帰納法により証明する。
$#n#=1$の場合は自明である。
配列の長さが0または1のときは、単に配列を返すだけで、比較を行わない。

長さの合計が$#n#$である2つのリストを併合するときは、最大で$#n#-1$回の比較が必要である。
%XXX: mergeSort(a, c) only for cpp edition?
長さ#n#の配列#a#に対して#mergeSort(a,c)#を実行するときに必要な比較回数の最大値を$C(#n#)$とする。
$#n#$が偶数なら、それぞれの部分問題に対して帰納法の仮定を適用し、次のように計算できる。
\begin{align*}
  C(#n#)
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n#
\end{align*}
$#n#$が奇数の場合は、やや複雑になる。
この場合は、次に示す2つの不等式を使う。
まず、任意の$x\ge 1$について次の不等式が成り立つ。
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace \eqlabel{log-ineq-a}
\end{equation}
また、任意の$x\ge 1/2$について次の不等式が成り立つ。
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace \eqlabel{log-ineq-b}
\end{equation}
いずれの不等式も簡単に検証できる。
まず、不等式\myeqref{log-ineq-a}は、$\log(x)+1 = \log(2x)$が成り立つことからいえる。
不等式\myeqref{log-ineq-b}は、$\log$が上に凸な関数であるであることによりいえる。
これらの不等式を利用することで、奇数#n#について次の式が成り立つ。
\begin{align*}
  C(#n#)
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2)
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2)
           - \log (#n#/2-1/2)) \\
  % XXX: n >= 3 を利用している？（n=1 は base case として確認しているので、正しいとは思うけど）
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace \qedhere
\end{align*}
\end{proof}

\subsection{クイックソート}

\ejindex{quicksort}{くいっくそーと@クイックソート}%
\emph{クイックソート（quicksort）}も古典的な分割統治アルゴリズムである。
クイックソートでは、2つの部分問題を解いたあとで結果を併合するマージソートとは違い、事前にすべての処理を済ませてしまう。

クイックソートの説明は単純だ。
まず、#a#からランダムに\emph{軸（pivot）}となる要素#x#を選ぶ。
\ejindex{pivot element}{じく@軸}%
そして、#x#より小さい要素、#x#と同じ要素、#x#より大きい要素の3つに#a#を分割する。
そして、分割の1つめと3つめを再帰的に整列する。
\figref{quicksort}に例を示す。
% TODO このアルゴリズムのコメント行が何故かバグっている -- spinute: どこ？ "??" のこと？だとすると、これはバグではなくて、大きいか小さいかわかっていないことを表す記号ではないかと思われる
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\pcodeimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{\javaonly{#quickSort(a,0,14,c)#}\cpponly{#quickSort(a,0,14)#}\pcodeonly{#quickSort(a,0,14)#}の実行例}
    \figlabel{quicksort}
  \end{center}
\end{figure}
% XXX: in-placeの訳語 % TALK in-placeは日本語wikipediaでもin-placeだった -- spinute: それでよさそうです

すべての処理はin-place\footnote{訳注：in-placeとは、入力配列#a#以外にはどの時点においても定数個の変数しか使わない、ということを表す。前節のマージソートの実装は、#a#の一部をコピーするために別の配列を必要としたので、in-placeなアルゴリズムではない。ただし、マージソートをin-placeに行う方法も存在する。}で実行される。
したがって、整列のために配列の各部をコピーすることはない。
その代わり、\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}では、配列の一部$#a[i]#,\ldots,#a[i+n-1]#$だけを整列する。
このメソッドを、最初に\cpponly{#quickSort(a,0,n)#}\pcodeonly{#quickSort(a,0,n,c)#}\javaonly{#quickSort(a,0,n,c)#}として呼び出すわけである。

クイックソートのアルゴリズムでもっとも肝心なポイントは、in-placeな分割を行うアルゴリズムである。
このアルゴリズムでは、余計な領域を使わずに、#a#の中の要素を入れ替え、次の制約を満たすような添え字#p#と#q#を計算する。
\[
   #a[i]# \begin{cases}%
         {}< #x# & \text{if $0\le #i#\le #p#$} \\
         {}= #x# & \text{if $#p#< #i# < #q#$} \\
         {}> #x# & \text{if $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
コード中の#while#ループでは、繰り返しのたびに、これらの制約の1つめと3つめの条件を保ちながら#p#が増加し、#q#は減少していく。
繰り返しの各ステップで、#j#番めの位置にある要素は、前に動くか、その場に留まるか、後ろに動く。
前に動くか、その場に留まる場合は、#j#を1増やす。後ろに動く場合は、#j#番めの要素は未処理ということなので、#j#を増やさない。

クイックソートは、\secref{rbst}で学んだランダム二分探索木と深い関係がある。
実は、相異なる要素をクイックソートに入力した場合の再帰木は、ランダム二分探索木と見なせるのである。
ランダム二分探索木を作るときは、まずランダムに要素#x#を選び、これを根にしたうえで各要素を#x#と比較して、最終的に小さい要素を左の部分木とし、大きい要素を右の部分木とした。
そのことを思い出せば、両者の関係が見えてくるだろう。

クイックソートでは、ランダムに要素#x#を選び、#x#と全要素とを比較したうえで、#x#より小さい要素を配列の前方に、大きい要素を配列の後方に集める。
それから、前方の配列と後方の配列をそれぞれ再帰的に整列する。
同じように、ランダム二分木では、小さい要素を左の部分木、大きい要素を右の部分木とする操作を再帰的に繰り返した。

ランダム二分探索木とクイックソートにこのような対応があるということは、\lemref{rbs}をクイックソートの言葉で言い換えられるということである。

\begin{lem}\lemlabel{quicksort}
クイックソートで整数$0,\ldots,#n#-1$を含む配列を整列するとき、要素#i#と軸とが比較される回数の期待値は、$H_{#i#+1} + H_{#n#-#i#}$以下である。
\end{lem}

調和数の計算により、クイックソートの実行時間に関する次の定理が得られる。

\begin{thm}\thmlabel{quicksort-i}
#n#個の相異なる要素をクイックソートで整列するとき、実行される比較の回数の期待値は$2#n#\ln #n# + O(#n#)$以下である。
\end{thm}

\begin{proof}
#n#個の相異なる要素をクイックソートで整列するときに実行される比較の回数を$T$とする。
\lemref{quicksort}と期待値の線形性より次が成り立つ。
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

\thmref{quicksort}は、整列する要素が互いに相異なる場合についての定理である。
入力の配列#a#が重複する要素を含むとき、クイックソートの実行時間の期待値は悪くはならず、むしろ向上する場合さえある。
これは、重複する要素#x#が軸に選ばれた場合に#x#がまとめられ、2つの部分問題のいずれにも含まれないからである。

\begin{thm}\thmlabel{quicksort}
\javaonly{#quickSort(a,c)#}\cpponly{#quickSort(a)#}\pcodeonly{#quickSort(a,c)#}の実行時間の期待値は$O(#n#\log #n#)$である。
また、実行される比較の回数の期待値は$2#n#\ln #n# +O(#n#)$以下である。
\end{thm}

\subsection{ヒープソート}
\seclabel{heapsort}

\ejindex{heap-sort}{ひーぷそーと@ヒープソート}%

\emph{ヒープソート (heap sort)}もin-placeで処理を行う整列アルゴリズムである。
ヒープソートでは\secref{binaryheap}で説明した二分ヒープを使う。
#BinaryHeap#ではひとつの配列を使ってヒープを表現していたことを思い出そう。
ヒープソートは入力の配列をヒープに変換した後で、最小値を取り出すことを繰り返す。

具体的には、#n#個の要素を配列#a#に格納する。
各要素は$#a[0]#,\ldots,#a[n-1]#$に入っており、最小値は根、すなわち#a[0]#である。
ヒープソートは次の操作を繰り返す。
#a#を#BinaryHeap#に変換した後、#a[0]#と#a[n-1]#を入れ替え、#n#を1減らし、#trickleDown(0)#を呼んで$#a[0]#,\ldots,#a[n-2]#$を再度ヒープにする。
$#n#=0$となってこの処理が終了すると、#a#の要素は降順に並んでいる。
よって、#a#を逆順にすれば最終的な整列された状態になる。
\footnote{#compare(x,y)#を修正すれば、結果が直接昇順に並ぶようにすることもできる}
\figref{heapsort}は#heapSort(a,c)#の実行の様子を表している。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption{#heapSort(a,c)#の実行中のある瞬間の様子。色の付いている部分は既に整列済みの配列である。色の付いていない部分は#BinaryHeap#になっている。次の繰り返しでは、要素$5$が配列の位置$8$に移される。}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}
\pcodeimport{ods/Algorithms.heapSort(a)}

% TALK the constructor の訳はWikipediaに倣いコンストラクタとした
ヒープソートで肝となるのは、未整列の配列#a#からヒープを構築する処理である。
これを$O(#n#\log#n#)$の時間で行うのは容易い。#BinaryHeap#の#add(x)#を繰り返し実行すればよい。
しかし、ボトムアップなやり方でこのヒープの構築はもっと効率よく行える。
#BinaryHeap#において、#a[i]#の子は#a[2i+1]#と#a[2i+2]#に入っていることを思い出そう。 % TODO: YJ binary heapの和訳がない？
そのため、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は子を持っていない。
別の言い方をすれば、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は大きさ1の部分ヒープである。
逆に考えると、$#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$に対して順番に#trickleDown(i)#を呼べる。
#trickleDown(i)#を呼ぶとき#a[i]#の子はいずれも部分ヒープの根なのでこれは可能で、#trickleDown(i)#を呼ぶと#a[i]#は次の部分ヒープの根になる。
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

興味深いことに、ボトムアップなやり方は#add(x)#を#n#回実行するよりも効率的である。
それを理解するためには、葉の位置に存在する$#n#/2$個の要素のためにはなにもする必要がなく、葉の親である$#n#/4$個の要素のためには#a[i]#を根とする部分ヒープに対して#trickleDown(i)#を一度呼べばよく、$#n#/8$個の要素のためにはこの高さの部分ヒープに対して#trickleDown(i)#を二度呼べばよい...という処理の流れに注目すればよい。
#trickleDown(i)#の実行時間は高々#a[i]#を根とする部分ヒープの高さのオーダーなので、全実行時間の合計は高々次の値である。
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#)
\]
最後から二番目の等号は、期待値の定義より、$\sum_{i=1}^{\infty} i/2^{i}$とコインを投げる（表が出る回も含めた）回数とが等しいことと、\lemref{coin-tosses}から成り立つ。

次の定理は#heapSort(a,c)#の性能を説明する。
\begin{thm}
  #heapSort(a,c)#の実行時間は$O(#n#\log #n#)$であり、このメソッドは最大$2#n#\log #n# + O(#n#)$の比較を実行する。
\end{thm}

\begin{proof}
このアルゴリズムには3つのステップがある。
(1)~#a#をヒープに変形し、(2)~#a#の最小値を繰り返し取り出し、(3)~#a#を逆順にする。
ステップ1の実行時間は$O(#n#)$で、$O(#n#)$回の比較を行う。
ステップ3の実行時間は$O(#n#)$で、比較は行わない。
ステップ2では#trickleDown(0)#を#n#回呼ぶ。
$i$番目の呼び出しは大きさ$#n#-i$のヒープに対するもので、最大$2\log(#n#-i)$回の比較を行う。
$i$についての和を取ると次の値が得られる。
\[
   % XXX: 原著 #n#-i だが、#n#-1ではないか
   % \sum_{i=0}^{#n#-i} 2\log(#n#-i)
   \sum_{i=0}^{#n#-1} 2\log(#n#-i)
   % \le \sum_{i=0}^{#n#-i} 2\log #n#
   \le \sum_{i=0}^{#n#-1} 2\log #n#
   =  2#n#\log #n#
\]
3つのステップにおいて比較の実行回数を足し合わせると、証明が完成する。
\end{proof}

\subsection{比較ベースの整列における下界}

\ejindex{lower-bound}{かかい@下界}%
\ejindex{sorting lower-bound}{せいれつあるごりずむのかかい@整列アルゴリズムの下界}%
3つの比較に基づく整列アルゴリズムの実行時間がいずれも$O(#n#\log #n#)$であることを見てきた。
ここで気になるのはもっと速いアルゴリズムがあるのかどうかである。
端的に答えると、これは存在しない。
#a#の要素に実行できる操作が比較だけなら、$#n#\log #n#$回程度の比較が必要なのである。
これを示すのは難しくないが、そのためには想像力が必要だ。
最終的にこれは次の事実から導かれる。
\[
   \log(#n#!)
     = \log #n# + \log (#n#-1) + \dots + \log(1)
     = #n#\log #n# - O(#n#)
\]
（この事実の証明を\excref{log-factorial}とした。）

まずは、マージソートやヒープソートのような決定的なアルゴリズムを、ある特定の値#n#について実行した場合について考える。
このようなアルゴリズムで#n#個の相異なる要素を整列する場面を想像せよ。
下界を示すために注目すべきは、#n#を固定すると決定的なアルゴリズムが最初に比較する要素は常に決まったペアであることである。
例えば#heapSort(a,c)#では#n#が偶数なら最初に#trickleDown(i)#を呼ぶとき#i=n/2-1#であり、最初に行われるのは#a[n/2-1]#と#a[n-1]#との比較である。

入力は相異なるので最初の比較の結果は二通りである。
二番目の比較は最初の比較の結果に依存して行われるかもしれない。
三度目の比較は最初のふたつの比較の結果に依存するかもしれず、以降も同様である。
こうして、任意の決定的な比較に基づく整列アルゴリズムを根付き二分\emph{比較木 (comparison tree)}とみなせる。
\ejindex{comparison tree}{ひかくぎ@比較木}%
この木の各内部ノード#u#は添え字のペア#u.i#と#u.j#でラベル付けられている。
$#a[u.i]#<#a[u.j]#$ならアルゴリズムは左の部分木に進み、そうでないなら右の部分木に進む。
この木の各葉#w#は$0,\ldots,#n#-1$のある置換$#w.p[0]#,\ldots,#w.p[n-1]#$でラベル付けられている。
この置換は比較木がこの葉に到達するとき#a#を整列するのに必要な比較を表現している。
これは次のものである。
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]#
\]
大きさ#n=3#である配列の比較木の例を\figref{comparison-tree}に示す。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption{長さ#n=3#の配列$#a[0]#,#a[1]#,#a[2]#$を整列するときの比較木}
  \figlabel{comparison-tree}
\end{figure}

整列アルゴリズムの比較木を見ることで、そのアルゴリズムのすべてがわかる。
#n#個の相異なる要素からなる任意の入力配列#a#について必要な比較の列や、アルゴリズムが#a#を整列するためにどういう順で並べ替えを行うかがわかるのである。
そのため比較木は少なくとも$#n#!$個の葉を持つ。
もしそうでなければ、相異なる置換であって同じ葉に到達するものが存在してしまう。
するとそのアルゴリズムは、同じ葉に到達する置換のうち少なくともどれか１つを正しく整列できないことになる。

例えば\figref{comparison-tree-2}に示した比較木は$4(< 3!=6)$個の葉を持つ。
この木を見ると、ふたつの入力$3,1,2$と$3,2,1$はいずれも右端の葉に到達することがわかる。
入力$3,1,2$は正しく$#a[1]#=1,#a[2]#=2,#a[0]#=3$を出力する。
しかし入力$3,2,1$の出力は$#a[1]#=2,#a[2]#=1,#a[0]#=3$となり正しくない。
この議論から比較に基づくアルゴリズムの本質的な下界が得られる。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{正しく整列できない入力が存在する比較木}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
比較に基づく任意の決定的な整列アルゴリズム$\mathcal{A}$と、任意の整数$#n#\ge 1$について、ある長さ#n#の入力配列#a#が存在し、$\mathcal{A}$が#a#を整列するとき$\log(#n#!) = #n#\log#n#-O(#n#)$回の比較を実行する。
\end{thm}

\begin{proof}
これまでの議論から$\mathcal{A}$の比較木は少なくとも$#n#!$個の葉を持つ必要がある。
帰納法により簡単に$k$個の葉を持つ二分木の高さが$\log k$以上であることを示せる。
よって$\mathcal{A}$の比較木には深さ$\log(#n#!)$以上の葉#w#が存在し、ある入力配列#a#が存在し、この葉に到達する。
この#a#に対して$\mathcal{A}$は$\log(#n#!)$回以上の比較を実行する。
\end{proof}

\thmref{deterministic-sorting-lower-bound}はマージソートやヒープソートなどの決定的なアルゴリズムに関するものであり、クイックソートのようなランダムなアルゴリズムに関してはなにもわからない。
ランダム性を利用するアルゴリズムは比較回数の下界$\log(#n#!)$を打ち破れるのだろうか。
これもやはりできないのである。
これを示すには、ランダムなアルゴリズムについて違った視点から考えてみればよい。

以下の議論では決定木は次の意味で「整理されてある」と仮定する。  % "cleaned up": 整理されてある 散らかっていない
どのような入力配列#a#によっても到達できないノードは削除される。
このとき木にはちょうど$#n#!$個だけの葉が含まれる。
葉の数が$#n#!$以上なのは、そうでないと整列を正しく行えないからである。
一方、葉の数が$#n#!$以下なのは、#n#個の相異なる要素の置換は$#n#!$通りであり、それぞれが決定木における根から葉への経路をちょうど一つ辿るためである。

ランダムなソートアルゴリズム$\mathcal{R}$は、ふたつの入力を取る決定的なアルゴリズムだと考えられる。
整列すべき入力配列#a#と、$[0,1]$内のランダムな実数の長い列$b=b_1,b_2,b_3,\ldots,b_m$である。
このランダムな数によってアルゴリズムはランダム化される。
アルゴリズムがコインを投げてランダムな選択をしたくなったとき、$b$の要素を使ってこれを行う。
例えばクイックソートにおける最初の軸を選ぶとき、アルゴリズムは式$\lfloor n b_1\rfloor$を使う。

$b$を特定の列$\hat{b}$に替えると、$\mathcal{R}$は決定的なアルゴリズムになる。
このアルゴリズムを$\mathcal{R}(\hat{b})$とし、これによる比較木を$\mathcal{T}(\hat{b})$とする。
また、#a#を$\{1,\ldots,#n#\}$の置換からランダムに選ぶのは、$\mathcal{T}(\hat{b})$の$#n#!$個の葉のうちのひとつをランダムに選ぶのは同じである。

\excref{randomized-lower-bound}で示す必要があったのは、$k$個の葉を持つ二分木の葉をランダムに選ぶとき、葉の深さの期待値が$\log k$以上であることであった。
以上より、$\{1,\ldots,n\}$の置換からランダムに選んだものを入力するとき、（決定的な）アルゴリズム$\mathcal{R}(\hat{b})$が実行する比較の回数の期待値は$\log(#n#!)$以上である。
結局任意の$\hat{b}$についてこれが成り立つので、$\mathcal{R}$についても同じことが成り立つ。
ランダムなアルゴリズムについての下界がこうして示された。

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
任意の整数$n\ge 1$と、任意の（決定的でもランダムでもよい）比較に基づく整列アルゴリズム$\mathcal{A}$について、$\{1,\ldots,n\}$の置換からランダムに選んだ入力を整列するときに実行する比較の回数の期待値は$\log(#n#!) = #n#\log#n#-O(#n#)$以上である。
\end{thm}

\section{計数ソートと基数ソート}

この節では比較に基づくものではない整列アルゴリズムを2つ紹介する。
小さい整数を整列することに特化し、要素（の一部）を配列の添え字として使うことで、この節で紹介するアルゴリズムは\thmref{deterministic-sorting-lower-bound}の下界に縛られない。
次の文を考えよう。
\[
  #c[a[i]]# = 1
\]
この文は定数時間で実行できるが、#a[i]#は#c.length#種類の値を取りうる。
% XXX: 次の文がなにを言っているのかよくわからない
これはこのような文を使うアルゴリズムは二分木でモデル化できないということである。
突き詰めるとこれこそが、この節で紹介するアルゴリズムが比較に基づくアルゴリズムよりも速く整列をこなす理由なのである。

\subsection{計数ソート}

$0,\ldots,#k#-1$の範囲の要素#n#個からなる入力の配列#a#があるとする。
\emph{計数ソート (counting sort)}はカウンタの補助配列#c#を使って#a#を整列する。
\ejindex{counting-sort}{けいすうそーと@計数ソート}%
そして、整列された#a#を補助配列#b#として返す。

計数ソートのアイデアは単純だ。
任意の$#i#\in\{0,\ldots,#k#-1\}$について、#i#の出て来る回数を数え、これを#c[i]#に入れておく。
整列の後では、出力は#c[0]#個の0からはじまり、#c[1]#個の1が続き、#c[2]#個の2が続き、\ldots、#c[k-1]#個の#k-1#で終わる。
このコードは巧妙である。実行の様子を\figref{countingsort}に示す。
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{$0,\ldots,#k#-1=9$のいずれかを格納する、長さ$#n#=20$である配列に対する、計数ソートの操作}
  \figlabel{countingsort}
\end{figure}

このコードと最初の#for#ループでは各カウンタ#c[i]#が#a#において#i#が何回現れるかを数えている。
#a#の値を添え字として使うことで、この処理を合計$O(#n#)$のひとつの#for#ループで終えている。
続いて、#c#を使って直接出力配列#b#を埋めていける。
しかし、#a#にデータが関連づけられているときはこれはできない。
よって、#a#から#b#に要素をコピーするための追加の仕事が必要になる。

次の#for#ループでは、$O(#k#)$の時間をかけてカウンタを順に足しこんでいる。
こうすると#c[i]#は#a#における#i#以下の要素の個数になる。
特に、任意の$#i#\in\{0,\ldots,#k#-1\}$について、出力配列#b#は次の式を満たす。
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i#
\]
最後に#a#を逆に辿って、要素を#b#に入れる。
#a#を辿りながら、要素#a[i]=j#は#b[c[j]-1]#に入れられ、#c[j]#をひとつ減らす。

\begin{thm}
#countingSort(a,k)#は$\{0,\ldots,#k#-1\}$の要素#n#個からなる配列#a#を$O(#n#+#k#)$の時間で整列する。
\end{thm}

計数ソートは\emph{安定性 (stable)}という良い性質を持つ。
\ejindex{stable sorting algorithm}{あんていしたせいれつあるごりずむ@安定した整列アルゴリズム}%
これは元の配列における等しい要素同士の相対的な位置を、整列後の配列でも保つ性質である。
すなわち、要素#a[i]#と#a[j]#が等しい値で、$#i#<#j#$であるとき、#b#においても#a[i]#が#a[j]#の前に来るのである。
この性質は次の節で役立つ。

\subsection{基数ソート}

計数ソートは配列の長さ#n#が、配列内の最大値$#k#-1$と比べてそれほど小さくないなら、非常に効率的である。
今から説明する\emph{基数ソート (radix sort)}は複数回の計数ソートにより、最大値が比較的大きな場合でも効率的なアルゴリズムである。
\ejindex{radix-sort}{きすうそーと@基数ソート}%

基数ソートは#w#ビットの整数を$#w#/#d#$回の計数ソートにより、一度に#d#ビットずつ整列する。
\footnote{#d#は#w#を割り切れると仮定する。もしそうでないときは#w#を$#d#\lceil #w#/#d#\rceil$に増やす必要がある。}
より正確に言うと、基数ソートはまず整数の最下位#d#ビットだけを見て整列する。続いて、次の#d#ビットだけを見て整列する。このような処理を繰り返し、最後には整数の最高位#d#ビットだけを見て整列する。
\codeimport{ods/Algorithms.radixSort(a)}
（このコードでは#(a[i]>>d*p)&((1<<d)-1)#と書いて、#a[i]#の2進数表記において$(#p#+1)#d#-1,\ldots,#p##d#$ビット目を抜き出して並べた整数を取り出している。）
このアルゴリズムの例を\figref{radixsort}に示した。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{基数ソートにより$#w#=8$ビットの整数を整列する。$#d#=2$ビットの整数を計数ソートによって整列する処理を4回行う。}
  \figlabel{radixsort}
\end{figure}

このアルゴリズムが正しく整列を行えるのは、計数ソートが安定なソートアルゴリズムだからである。
#a#のふたつの要素#x#と#y#が$#x# < #y#$を満たし、#x#と#y#が添え字$r$の位置で異なるなら、$\lfloor r/#d#\rfloor$回目の整列で#x#は#y#より前に置かれる。
そして、以降は#x#と#y#の相対的な位置は変わらない。

基数ソートは#w/d#回の計数ソートを行う。
各計数ソートの実行時間は$O(#n#+2^{#d#})$である。
よって、基数ソートの性能は次の定理のようになる。
\begin{thm}\thmlabel{radix-sort}
任意の整数$#d#>0$について、#radixSort(a,k)#は#n#個の#w#ビット整数を含む配列#a#を$O((#w#/#d#)(#n#+2^{#d#}))$の時間で整列する。
\end{thm}

配列の要素は$\{0,\ldots,#n#^c-1\}$の範囲の整数であり、$#d#=\lceil\log#n#\rceil$だとすれば、\thmref{radix-sort}は次のように整理できる。
\begin{cor}\corlabel{radix-sort-poly}
#radixSort(a,k)#は、$\{0,\ldots,#n#^c-1\}$の範囲の#n#個の整数からなる配列#a#を、$O(c#n#)$の時間で整列する。
\end{cor}

\section{ディスカッションと練習問題}

整列は計算機科学における基本的なアルゴリズムであり、長い歴史がある。
Knuth \cite{k97v3}によればマージソートはvon~Neumann (1945)が考案したものだという。
クイックソートはHoare \cite{h61}が考案した。
はじめにヒープソートを考案したのは
Williams \cite{w64}だが、本節で説明した$O(#n#)$の時間でボトムアップにヒープを構築する方法はFloyd \cite{f64}によるものである。
比較に基づくソートの下界は古くから言い継がれてきたものである。
次の表は比較に基づくアルゴリズムの性能をまとめたものだ。

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
      & \multicolumn{3}{c|}{時間計算量} & in-place  \\ \hline
    マージソート & $#n#\log #n#$ & &  最悪実行時間 & No  \\
    クイックソート & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & 期待実行時間 & Yes \\
    ヒープソート & $2#n#\log #n#$ & ${}+ O(#n#)$ & 最悪実行時間 & Yes \\ \hline
  \end{tabular}
\end{center}

これらの比較に基づくアルゴリズムにはそれぞれの長所・短所がある。
マージソートは比較の回数が最も少なく、ランダムでもない。
しかしマージのときに補助配列が必要だ。
この配列を確保するのはコストが高く、またメモリの制限によりソートに失敗する可能性もある。
クイックソートは入力配列の中だけで処理を済ませ、比較の回数も二番目に少ないか、ランダムなアルゴリズムなので実行時間の保証が常には成り立たない。
\ejindex{in-place algorithm}{in-place なアルゴリズム}%
ヒープソートは比較の回数は最も多いが、入力配列だけを使った決定的なアルゴリズムである。

マージソートが明らかに一番優れている場面がある。
これは連結リストを整列するときである。
この場合は補助的な配列が必要ないのである。
ふたつの整列済みの連結リストはポインタ操作によって簡単に併合でき、整列済みの一つの配列が得られる。
（\excref{list-merge-sort}を参照せよ。）

この章で説明した計数ソートと基数ソートはSeward \cite[Section~2.4.6]{s54}によるものである。
しかし、基数ソートの一種が1920年代からパンチカードを整列するために機械によって使われていた。
この機械はカードの山を、ある場所に穴が空いているかどうかを判定してふたつの山に分けた。
色々な穴の位置についてこの処理を繰り返すことで、基数ソートになる。

最後に、計数ソート・基数ソートはいずれも非負整数以外の数も整列できることを確認しておく。
計数ソートを$\{a,\ldots,b\}$の範囲の整数を整列できるよう素直に修正すれば、実行時間は$O(#n#+b-a)$になる。
同様に基数ソートは同じ範囲の整数を$O(#n#(\log_{#n#}(b-a))$の時間で整列できる。
また、いずれのアルゴリズムも、IEEE754形式の浮動小数点数を整列するのにも使える。
これはIEEE 754の形式は数の大きさに符号が付いた二進整数表現だと見なして比較ができるように設計されているからである。
\cite{ieee754}

\begin{exc}
$1,7,4,6,2,8,3,5$からなる配列を入力とする、マージソートとヒープソートの実行の様子を描け。
また同じ配列について、クイックソートを実行の様子として、ありえるもののうちのひとつを描け。
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
マージソートの一種で、補助配列を使わずに#DLList#を整列するものを実装せよ。
（\excref{dllist-sort}を参照せよ。）
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#の実装には、軸として常に#a[i]#を選ぶものがある。
この実装が、$\binom{#n#}{2}$回の比較を実行する長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#の実装には、軸として常に#a[i+n/2]#を選ぶものがある。
この実装が、$\binom{#n#}{2}$回の比較を実行する長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
どのような#quickSort(a,i,n,c)#の実装でも、軸を決定的に選び、$#a[i]#,\ldots,#a[i+n-1]#$を先に見ないならば、長さ#n#のある入力が存在し、$\binom{#n#}{2}$回の比較を実行することを示せ。
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#を実行するとき$\binom{#n#}{2}$回の比較を実行する#Comparator# #c#を設計せよ。
（ヒント：comparatorは比較する値を実際に見なくてもよい。）
\end{exc}

\begin{exc}
\thmref{quicksort}の証明よりも細かくクイックソートが実行する比較の回数の期待値を解析せよ。
具体的には、比較の回数の期待値が$2#n#H_#n# -#n# + H_#n#$であることを示せ。
\end{exc}

\begin{exc}
ヒープソートを実行するときの、比較の回数が$2#n#\log #n#-O(#n#)$回になる入力配列を与え、そのことを説明せよ。
\end{exc}

\javaonly{
\begin{exc}
この章で説明したヒープソートの実装では、要素を逆順に並び替え、その後で順番を逆にしていた。
入力の#Comparator# #c#の結果を否定して新しい#Comparator#を定義すれば、最後のステップを省ける。
これがよい最適化でない理由を説明せよ。
（ヒント：配列を逆さまにするのと比べて、否定のためにどのくらい時間が必要になるかを考えよ。）
\end{exc}
}

\begin{exc}
\figref{comparison-tree-2}の比較木によって正しく整列できない$1,2,3$の別の置換を見つけよ。
\end{exc}

\begin{exc}\exclabel{log-factorial}
$\log #n#! = #n#\log #n#-O(#n#)$を示せ。
\end{exc}

\begin{exc}
$k$個の葉を持つ二分木の高さは$\log k$以上であることを示せ。
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
$k$個の葉を持つ二分木の葉をランダムに選ぶとき、その葉の高さの期待値は$\log k$以上であることを示せ。
%  (Hint: Use induction along with the inequality $(k_1/k)\log k_1 +
%  (k_2/k)\log k_2) \ge  \log k-1$, when $k_1+k_2=k$.)
\end{exc}

% This was a stupid exercise, since IEEE 754 floating point numbers 
% are correctly ordered when they are treated as signed magnitude integers
% [i.e., a sign bit followed by an integer ]
%\begin{exc}
%  Simple floating point numbers are numbers of the form $x\cdot10^{y}$,
%  where $0\le x\le 1$ and $y$ is an integer and each of $x$ and $y$ can
%  be represented by at most $k$ bits.  Describe a version of radix-sort
%  that can be used to sort simple floating point numbers.
%
%  Extend your version of radix sort so that it can handle signed values
%  of both $x$ and $y$ and implement a version of radix-sort that sorts
%  arrays of type #float#.
%\end{exc}

\begin{exc}
この章で説明した#radixSort(a,k)#は入力配列#a#が非負整数だけからなるとき動作する。
入力配列が負の整数を含むときにも、正しく動作するように実装を修正せよ。
\end{exc}

