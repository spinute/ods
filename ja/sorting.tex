\chapter{整列アルゴリズム}
\chaplabel{sorting}

この章では#n#個の要素の集合を整列するアルゴリズムを紹介する。
データ構造の教科書にこの題材が入っているのは変に思うかもしれないが、これにはいくつか理由がある。
例えばここで紹介する整列アルゴリズム（クイックソートとヒープソート）はこれまでに学んだデータ構造と深い関係がある。（ランダム二分探索木とヒープ）

\secref{sort-by-comparison}では比較だけを使った整列アルゴリズムであって、実行時間が$O(#n#\log #n#)$であるものを3つ紹介する。
またこれらの3つはいずれも漸近的に最適であることも示す。
つまり比較だけを使うアルゴリズムでは、最悪実行時間にせよ平均実行時間にせよ、最低でも$#n#\log#n#$回程度の比較が必要なのである。


ここで、これまでの章で説明した、整列済みの集合#SSet#や優先度付き#Queue#の実装はいずれも$O(#n#\log #n#)$の時間で整列アルゴリズムを実装するのに使えることを注意しておく。
例えば#n#個の要素を、#BinaryHeap#または#MeldableHeap#における#n#回の#add(x)#に続く#n#回の#remove()#操作で整列できる。
あるいは、二分探索木のどれかに#n#回の#add(x)#を実行し、そのあと行きがけ順（\excref{tree-traversal}）で要素を整列された順に取り出すことができる。
しかしいずれの場合も完全に活用するわけでないデータ構造を構築するための無駄がかなり生じる。 % TODO caprice ここ自然に訳したい
整列は重要な問題なので、可能な限り速く、単純で、省メモリな手法を開発する価値がある。

この章の後半では比較以外の操作も使える場合には話が変わることを見ていく。
実際に配列のランダムアクセスを使って、$\{0,\ldots,#n#^c-1\}$の要素である#n#個の整数の整列を$O(c#n#)$の時間で実行できることを説明する。

\section{比較に基づく整列}
\seclabel{sort-by-comparison}

\ejindex{comparison-based sorting}{ひかくにもとづくせいれつ@比較に基づく整列}%
\ejindex{sorting algorithm!comparison-based}{せいれつあるごりずむ@整列アルゴリズム!ひかくにもとづく@比較に基づく}%
この節では3つの整列アルゴリズム、マージソート・クイックソート・ヒープソートを紹介する。
いずれも配列#a#を入力すると、 $O(#n#\log #n#)$の（期待）実行時間で#a#の要素を昇順に整列する。
どれも\emph{比較に基づく}アルゴリズムである。
\javaonly{第二引数#c#は#Comparator#である。
\index{Comparator@#Comparator#}%
これは#compare(a,b)#メソッドを実装するオブジェクトである。
\index{compare@#compare(a,b)#}%
}
整列するデータの型はなんでもよい。
ただ、データの比較のための#compare(a,b)#メソッドを持っていなければならない。
\secref{sset}でも説明したが、#compare(a,b)#は、$#a#<#b#$なら負の値を、$#a#>#b#$なら正の値を、$#a#=#b#$ならゼロを返す。

\subsection{マージソート}
\seclabel{merge-sort}

\ejindex{merge-sort}{まーじそーと@マージソート}%
\emph{マージソート}は再帰的な分割統治法の古典的な例である。
\ejindex{divide-and-conquer}{ぶんかつとうちほう@分割統治法}%
配列#a#の長さが1以下なら、#a#は既に整列されており、なにもする必要はない。
そうでなければ、配列#a#を半分ずつ配列$#a0#=#a[0]#,\ldots,#a[n/2-1]#$と配列$#a1#=#a[n/2]#,\ldots,#a[n-1]#$に分ける。
再帰的に#a0#と#a1#を整列し、そして（整列済みの）#a0#と#a1#とを併合し、完全に整列された配列#a#を得る
\footnote{訳注：JavaのArrays.copyOfRange(T[] original, int from, int to)メソッドは、toをコピー対象として含まないことに注意。そのため擬似コードではa.length/2がfromとtoで一度ずつ用いられている。}
。
\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
\pcodeimport{ods/Algorithms.mergeSort(a)}
\figref{merge-sort}に例を示した。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
%XXX: mergeSort(a, c) only for cpp edition?
  \caption{#mergeSort(a,c)#を実行する様子}
  \figlabel{merge-sort}
\end{figure}

整列と比べて、#a0#と#a1#を併合するのは簡単である。
#a#に要素を一つずつ加えていく。
もし#a0#か#a1#が空になれば、空でない方の配列の残りの要素をすべて#a#に加える。
そうでなければ、#a0#の次の要素と#a1#の次の要素のうち、小さい方を#a#に加える。
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}
\pcodeimport{ods/Algorithms.merge(a0,a1,a)}
#merge(a0,a1,a,c)#では#a0#または#a1#が空になる前に最大で$#n#-1$回の比較を行う。

マージソートの実行時間を求めるためには\figref{mergesort-recursion}に示したような再帰木で考えるのがよい。
#n#が2の冪乗であると仮定する。
すなわち、$#n#=2^{\log #n#}$であり、$\log #n#$は整数である。
マージソートは#n#個の要素の整列問題を、２つの「$#n#/2$個の要素の並び替え問題」に変換する。
次に、これらの部分問題はそれぞれふたつの問題に変換され、４つの「$#n#/4$個の要素の並び替え問題」になる。
そして、この4つの部分問題は８つの「大きさ$#n#/8$の問題」になる。
このようなことを繰り返す。
最終ステップでは、$#n#/2$個の「大きさ2の部分問題」が、#n#個の「大きさ1の問題」に変換される。
大きさ$#n#/2^{i}$の問題を解く際に、既に解かれた２つの部分問題の解答をマージしながらコピーするのにかかる時間は$O(#n#/2^i)$である。
$2^i$個の大きさ$#n#/2^i$の問題があるので、
大きさ$2^i$の問題のために必要な時間の合計は次のようになる。（これはまだ再帰的には数えていないことに注意する。）
\[
       2^i\times O(#n#/2^i) = O(#n#)
\]
よって、マージソートに必要な時間の合計は次のようになる。
\[
   \sum_{i=0}^{\log #n#} O(#n#) = O(#n#\log #n#)
\]

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{マージソートの再帰木}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}

次の定理の証明は以上の解析に基づくが、#n#が2の冪乗でない場合を扱うためもう少し注意することがある。
\begin{thm}
  \javaonly{#mergeSort(a,c)#}\cpponly{mergeSort(a)}\pcodeonly{merge\_sort(a)}の実行時間は$O(#n#\log #n#)$であり、最大で$#n#\log #n#$回の比較を行う。
\end{thm}

\begin{proof}
$#n#$についての帰納法により証明する。
$#n#=1$の場合は自明である。
配列の長さが0または1のときには単に配列を返すだけで、比較を行わない。

長さの合計が$#n#$であるふたつのリストを併合するとき、最大で$#n#-1$回の比較が必要である。
%XXX: mergeSort(a, c) only for cpp edition?
$C(#n#)$を長さ#n#の配列#a#に対して#mergeSort(a,c)#を実行するときに必要な比較の最大値とする。
$#n#$が偶数なら、それぞれの部分問題に対して帰納法の仮定を適用し、次のように計算できる。
\begin{align*}
  C(#n#)
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n#
\end{align*}
$#n#$が奇数のときはもう少し複雑である。
この場合ふたつの簡単に確認できる不等式を使う。
任意の$x\ge 1$について次の式が成り立つ。
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace \eqlabel{log-ineq-a}
\end{equation}
また、任意の$x\ge 1/2$について次の式が成り立つ。
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace \eqlabel{log-ineq-b}
\end{equation}
不等式\myeqref{log-ineq-a}は$\log(x)+1 = \log(2x)$が成り立つことによる。
不等式\myeqref{log-ineq-b}は$\log$が凹関数（上に凸な関数）であるであることによる。
これを利用して、奇数#n#について次の式が成り立つ。
\begin{align*}
  C(#n#)
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2)
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2)
           - \log (#n#/2-1/2)) \\
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace \qedhere
\end{align*}
\end{proof}
% TODO この奇数の場合まだきちんと読んでない
\subsection{クイックソート}

\ejindex{quicksort}{くいっくそーと@クイックソート}%
\emph{クイックソート}も古典的な分割統治アルゴリズムの一つである。
ふたつの部分問題を解いてから結果を併合するマージソートとは違い、クイックソートはもっと直接的に仕事を行う。

クイックソートの説明は単純である。
#a#からランダムに\emph{軸}となる要素#x#を選ぶ。
\ejindex{pivot element}{じく@軸}%
#a#を#x#より小さい要素、#x#と同じ要素、#x#より大きい要素に分割する。
そして、一つめと三つめの分割を再帰的に整列する。
\figref{quicksort}に例を示した。
% TODO このアルゴリズムのコメント行が何故かバグっている
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\pcodeimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{\javaonly{#quickSort(a,0,14,c)#}\cpponly{#quickSort(a,0,14)#}\pcodeonly{#quickSort(a,0,14)#}の実行例}
    \figlabel{quicksort}
  \end{center}
\end{figure}
XXX: in-placeの訳語 % TALK in-placeは日本語wikipediaでもin-placeだった

すべての処理は配列の部分的なコピーを取るのではなくin-place \footnote{訳注：in-placeとは、入力配列#a#以外にはどの時点においても定数個の変数しか使わない、ということを表す。「前掲のマージソートは」#a#全体をコピーするような別の配列を必要としたため、in-placeなアルゴリズムではない。ただし、マージソートをin-placeに行う方法も存在する。}で実行され、#quickSort(a,i,n,c)#は$#a[i]#,\ldots,#a[i+n-1]#$を整列する。
最初にこのメソッドを#quickSort(a,0,a.length,c)#と呼び出す。

クイックソートアルゴリズムの肝は、in-placeで分割を行うことである。
このアルゴリズムは余分な領域を使わず#a#の要素を入れ替え、次のような添え字#p#と#q#を計算する。
\[
   #a[i]# \begin{cases}
         {}< #x# & \text{if $0\le #i#\le #p#$} \\
         {}= #x# & \text{if $#p#< #i# < #q#$} \\
         {}> #x# & \text{if $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
この分割処理は#while#ループの中で、#p#を増やし、#q#を減らし、上の制約を保ちながら繰り返し実行される。
各ステップで#j#番目の位置にある要素は前に動くか、その場に留まるか、後ろに動く。
初めのふたつの場合は#j#を1増やし、最後の場合は#j#番目の要素は未処理なので#j#は増やさない。

クイックソートは\secref{rbst}で学んだランダム二分探索木と深い関係がある。
実は#n#個の相異なる要素を入力すると、クイックソートの再帰木はランダム二分探索木なのである。
これを確認するためには、ランダム二分探索木を作るとき、まずはランダムに要素#x#を選び、これを根にしたことを思い出そう。
続いて、各要素を#x#と比較し、最終的に小さい要素を左の部分木に、大きい要素を右の部分木としたのだった。

クイックソートではランダムに要素#x#を選び、直ちに全要素#x#と比較し、小さい要素を配列の前方に、大きい要素を配列の後方に集める。
続いて、再帰的に配列の前方・後方をそれぞれ整列する。
一方、ランダム二分木では再帰的に小さい要素を左の部分木、大きい要素を右の部分木とすることを繰り返す。

ランダム二分木とクイックソートとの上の対応から、\lemref{rbs}をクイックソートの場合に置き換えて考えてみる。

\begin{lem}\lemlabel{quicksort}
クイックソートは整数$0,\ldots,#n#-1$を含む配列を整列するために呼ばれるとき、要素#i#が軸と比較される回数の期待値は$H_{#i#+1} + H_{#n#-#i#}$以下である。
\end{lem}

調和数を計算するとクイックソートの実行時間に関する次の定理が得られる。

\begin{thm}\thmlabel{quicksort-i}
#n#個の相異なる要素をクイックソートで整列するとき、実行される比較の回数の期待値は$2#n#\ln #n# + O(#n#)$以下である。
\end{thm}

\begin{proof}
$T$を#n#個の相異なる要素をクイックソートで整列するときに実行される比較の回数とする。
\lemref{quicksort}と期待値の線形性より次が成り立つ。
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

\thmref{quicksort}は整列する要素が互いに異なる場合についてのものである。
入力の配列#a#が重複する要素を含むとき、クイックソートの実行時間の期待値は悪くはならず、むしろよくなることさえある。
重複する要素#x#が軸に選ばれると、#x#はまとめられ、ふたつの部分問題のいずれにも含まれないためである。

\begin{thm}\thmlabel{quicksort}
\javaonly{#quickSort(a,c)#} \cpponly{#quickSort(a)#}\pcodeonly{#quickSort(a,c)#}の実行時間の期待値は$O(#n#\log #n#)$である。
また、実行される比較の回数の期待値は$2#n#\ln #n# +O(#n#)$以下である。
\end{thm}

\subsection{ヒープソート}
\seclabel{heapsort}

\ejindex{heap-sort}{ひーぷそーと@ヒープソート}%
XXX: in-placeの訳語

\emph{ヒープソート}もin-placeで処理を行う整列アルゴリズムである。
ヒープソートは\secref{binaryheap}で説明した二分ヒープを使う。
#BinaryHeap#はひとつの配列を使ってヒープを表現していたことを思い出そう。
ヒープソートは入力の配列をヒープに変換した後で、最小値を取り出すことを繰り返す。

具体的には、#n#個の要素を配列#a#に格納する。
各要素は$#a[0]#,\ldots,#a[n-1]#$に入っており、最小値は根、すなわち#a[0]#である。
ヒープソートは次の操作を繰り返す。
#a#を#BinaryHeap#に変換した後、#a[0]#と#a[n-1]#を入れ替え、#n#を1減らし、#trickleDown(0)#を呼んで$#a[0]#,\ldots,#a[n-2]#$を再度ヒープにする。
$#n#=0$となってこの処理が終了すると、#a#の要素は降順に並んでいる。
よって、#a#を逆順にすれば最終的な整列された状態になる。
\footnote{#compare(x,y)#を修正すれば、結果が直接昇順に並ぶようにすることもできる}
\figref{heapsort}は#heapSort(a,c)#の実行の様子を表している。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption{#heapSort(a,c)#の実行中のある瞬間の様子。色の付いている部分は既に整列済みの配列である。色の付いていない部分は#BinaryHeap#になっている。次の繰り返しでは、要素$5$が配列の位置$8$に移される。}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}
\pcodeimport{ods/Algorithms.heapSort(a)}

% TALK the constructor の訳はWikipediaに倣いコンストラクタとした
ヒープソートで肝となる処理のひとつは、未整列の配列#a#からヒープを構築する、というものである。
これを$O(#n#\log#n#)$の時間で行うのは容易い。#BinaryHeap#の#add(x)#を繰り返し実行すればよい。
しかし、ボトムアップなやり方を使うとより早い実行時間で達成できる。
#BinaryHeap#において、#a[i]#の子は#a[2i+1]#と#a[2i+2]#に入っていることを思い出そう。 % TODO: YJ binary heapの和訳がない？
そのため、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は子を持っていない。
別の言い方をすれば、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は大きさ1の部分ヒープである。
逆に考えると、$#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$に対して#trickleDown(i)#を呼べる。
#trickleDown(i)#を呼ぶとき#a[i]#の子はいずれも部分ヒープの根なのでこれは可能で、#trickleDown(i)#を呼ぶと#a[i]#は次の部分ヒープの根になる。
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

興味深いことに、ボトムアップなやり方は#add(x)#を#n#回実行するよりも効率的である。
それを理解するためには、葉の位置に存在する$#n#/2$個の要素のためにはなにもする必要がなく、葉の親である$#n#/4$個の要素のためには#a[i]#を根とする部分ヒープに対して#trickleDown(i)#を一度呼べばよく、$#n#/8$個の要素のためには、この高さの部分ヒープに対して#trickleDown(i)#を二度呼べばよい...と言った流れに注目すればよい。
#trickleDown(i)#によって実行される処理は#a[i]#を根とする部分ヒープの高さに比例するので、全体として必要な処理の量は高々次の値である。
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#)
\]
最後から二番目の等号は、期待値の定義より、$\sum_{i=1}^{\infty} i/2^{i}$とコインを投げる（表が出る回も含む）回数が等しいことと、\lemref{coin-tosses}から成り立つ。

次の定理は#heapSort(a,c)#の性能を説明する。
\begin{thm}
  #heapSort(a,c)#の実行時間は$O(#n#\log #n#)$であり、このメソッドは最大$2#n#\log #n# + O(#n#)$の比較を実行する。
\end{thm}

\begin{proof}
このアルゴリズムには3つのステップがある。
(1)~#a#をヒープに変形し、(2)~#a#の最小値を繰り返し取り出し、(3)~#a#を逆順にする。
ステップ1の実行時間は$O(#n#)$で、$O(#n#)$回の比較を行う。
ステップ3の実行時間は$O(#n#)$で、比較は行わない。
ステップ2では#trickleDown(0)#を#n#回呼ぶ。
$i$番目の呼び出しは大きさ$#n#-i$のヒープに対するもので、最大$2\log(#n#-i)$回の比較を行う。
$i$についての和を取ると次の値が得られる。
\[
   \sum_{i=0}^{#n#-i} 2\log(#n#-i)
   \le \sum_{i=0}^{#n#-i} 2\log #n#
   =  2#n#\log #n#
\]
3つのステップにおいて実行される比較の回数を足し合わせると、証明が完成する。
\end{proof}

\subsection{比較ベースの整列における下界}

\ejindex{lower-bound}{かかい@下界}%
\ejindex{sorting lower-bound}{せいれつあるごりずむのかかい@整列アルゴリズムの下界}%
3つの比較に基づく整列アルゴリズムの実行時間がいずれも$O(#n#\log #n#)$であることを見てきた。
ここで気になるのはもっと速いアルゴリズムがあるかどうかである。
端的に答えるとこれは存在しない。
#a#の要素に実行できる操作が比較だけなら、$#n#\log #n#$回程度の比較を必ずすることになるのである。
これを示すのは難しくないが、そのためには想像力が必要だ。
最終的にこれは次の事実から導かれる。
\[
   \log(#n#!)
     = \log #n# + \log (#n#-1) + \dots + \log(1)
     = #n#\log #n# - O(#n#)
\]
（この事実の証明を\excref{log-factorial}とした。）

まずは、マージソートやヒープソートのような決定的なアルゴリズムを、ある特定の値#n#について実行した場合について考える。
このようなアルゴリズムで#n#個の相異なる要素を整列する場面を想像せよ。
下界を示すために注目すべきは、#n#を固定すると決定的なアルゴリズムが最初に比較する要素は常に決まったペアであることである。
例えば#heapSort(a,c)#では#n#が偶数なら最初に#trickleDown(i)#を呼ぶとき#i=n/2-1#であり、最初に行われるのは#a[n/2-1]#と#a[n-1]#との比較である。

入力は相異なるので最初の比較の結果は二通りである。
二番目の比較は最初の比較の結果に依存して行われるかもしれない。
三度目の比較は最初のふたつの比較の結果に依存するかもしれず、以降も同様である。
こうして、任意の決定的な比較に基づく整列アルゴリズムを根付き二分\emph{比較木}とみなせる。
\ejindex{comparison tree}{ひかくぎ@比較木}%
この木の各内部ノード#u#は添え字のペア#u.i#と#u.j#でラベル付けられている。
$#a[u.i]#<#a[u.j]#$ならアルゴリズムは左の部分木に進み、そうでないなら右の部分木に進む。
この木の各葉#w#は$0,\ldots,#n#-1$のある置換$#w.p[0]#,\ldots,#w.p[n-1]#$でラベル付けられている。
この置換は比較木がこの葉に到達するとき#a#を整列するのに必要な比較を表現している。
これは次のものである。
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]#
\]
大きさ#n=3#である配列の比較木の例を\figref{comparison-tree}に示す。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption{長さ#n=3#の配列$#a[0]#,#a[1]#,#a[2]#$を整列するときの比較木}
  \figlabel{comparison-tree}
\end{figure}

整列アルゴリズムの比較木を見ることで、そのアルゴリズムのすべてがわかる。
#n#個の相異なる要素からなる任意の入力配列#a#について必要な比較の列や、アルゴリズムが#a#を整列するためにどういう順で並べ替えを行うかがわかるのである。
そのため比較木は少なくとも$#n#!$個の葉を持つ。
もしそうでなければ、相異なる置換であって同じ葉に到達するものが存在してしまう。
するとそのアルゴリズムは、同じ葉に到達する置換のうち少なくともどれか１つを正しく整列できないことになる。

例えば\figref{comparison-tree-2}に示した比較木は$4< 3!=6$個の葉を持つ。
この木を見ると、ふたつの入力$3,1,2$と$3,2,1$はいずれも右端の葉に到達することがわかる。
入力$3,1,2$は正しく$#a[1]#=1,#a[2]#=2,#a[0]#=3$を出力する。
しかし入力$3,2,1$の出力は$#a[1]#=2,#a[2]#=1,#a[0]#=3$となり正しくない。
この議論から比較に基づくアルゴリズムの本質的な下界が得られる。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{正しく整列できない入力が存在する比較木}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
比較に基づく任意の決定的な整列アルゴリズム$\mathcal{A}$と、任意の整数$#n#\ge 1$について、ある長さ#n#の入力配列#a#が存在し、$\mathcal{A}$が#a#を整列するとき$\log(#n#!) = #n#\log#n#-O(#n#)$回の比較を実行する。
\end{thm}

\begin{proof}
これまでの議論から$\mathcal{A}$の比較木は少なくとも$#n#!$個の葉を持つ必要がある。
帰納法により簡単に$k$個の葉を持つ二分木の高さが$\log k$以上であることを示せる。
よって$\mathcal{A}$の比較木には深さ$\log(#n#!)$以上の葉#w#が存在し、ある入力配列#a#が存在し、この葉に到達する。
この#a#に対して$\mathcal{A}$は$\log(#n#!)$回以上の比較を実行する。
\end{proof}

\thmref{deterministic-sorting-lower-bound}はマージソートやヒープソートなどの決定的なアルゴリズムに関するものであり、クイックソートのようなランダムなアルゴリズムに関してはなにもわからない。
ランダム性を利用するアルゴリズムは比較回数の下界$\log(#n#!)$を打ち破れるのだろうか。
これもやはりできないのである。
これを示すには、ランダムなアルゴリズムについて違った視点から考えてみればよい。

XXX: decision treeの訳語 % YJ 決定木でよいかと

以下の議論では決定木は次の意味で「整理されてある」と仮定する。  % "cleaned up": 整理されてある 散らかっていない
どのような入力配列#a#によっても到達できないノードは削除される。
このとき木にはちょうど$#n#!$個だけの葉が含まれる。
葉の数が$#n#!$以上なのは、そうでないと整列を正しく行えないからである。
一方、葉の数が$#n#!$以下なのは、#n#個の相異なる要素の置換は$#n#!$通りであり、それぞれが決定木における根から葉への経路をちょうど一つ辿るためである。

ランダムなソートアルゴリズム$\mathcal{R}$は、ふたつの入力を取る決定的なアルゴリズムだと考えられる。
整列すべき入力配列#a#と、$[0,1]$内のランダムな実数の長い列$b=b_1,b_2,b_3,\ldots,b_m$である。
このランダムな数によってアルゴリズムはランダム化される。
アルゴリズムがコインを投げてランダムな選択をしたくなったとき、$b$の要素を使ってこれを行う。
例えばクイックソートにおける最初の軸を選ぶとき、アルゴリズムは式$\lfloor n b_1\rfloor$を使う。

$b$を特定の列$\hat{b}$に替えると、$\mathcal{R}$は決定的なアルゴリズムになる。
このアルゴリズムを$\mathcal{R}(\hat{b})$とし、これによる比較木を$\mathcal{T}(\hat{b})$とする。
また、#a#を$\{1,\ldots,#n#\}$の置換からランダムに選ぶのは、$\mathcal{T}(\hat{b})$の$#n#!$個の葉のうちのひとつをランダムに選ぶのは同じである。

\excref{randomized-lower-bound}で示す必要があったのは、$k$個の葉を持つ二分木の葉をランダムに選ぶとき、葉の深さの期待値が$\log k$以上であることであった。
以上より、$\{1,\ldots,n\}$の置換からランダムに選んだものを入力するとき、（決定的な）アルゴリズム$\mathcal{R}(\hat{b})$が実行する比較の回数の期待値は$\log(#n#!)$以上である。
結局任意の$\hat{b}$についてこれが成り立つので、$\mathcal{R}$についても同じことが成り立つ。
ランダムなアルゴリズムについての下界がこうして示された。

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
任意の整数$n\ge 1$と、任意の（決定的でもランダムでもよい）比較に基づく整列アルゴリズム$\mathcal{A}$について、$\{1,\ldots,n\}$の置換からランダムに選んだ入力を整列するときに実行する比較の回数の期待値は$\log(#n#!) = #n#\log#n#-O(#n#)$以上である。
\end{thm}

\section{計数ソートと基数ソート}

この節では比較に基づいたものでないアルゴリズムをふたつ紹介する。
小さい整数に特化し、要素（の一部）を配列の添え字として使うことで、これらのアルゴリズムは\thmref{deterministic-sorting-lower-bound}の下界に縛られない。
次の文を考えよう。
\[
  #c[a[i]]# = 1
\]
この文は定数時間で実行できるが、#a[i]#の値により#c.length#の値は異なる。
これはこのような文を使うアルゴリズムは二分木でモデル化できないということである。
突き詰めると、これこそがこの節のアルゴリズムが比較に基づくアルゴリズムよりも速く整列をこなせる理由なのである。

\subsection{計数ソート}

$0,\ldots,#k#-1$の範囲の要素#n#個からなる入力の配列#a#があるとする。
\emph{計数ソート}はカウンタの補助配列#c#を使って#a#を整列する。
\ejindex{counting-sort}{けいすうそーと@計数ソート}%
そして、整列された#a#を補助配列#b#として返す。

計数ソートのアイデアは単純だ。
任意の$#i#\in\{0,\ldots,#k#-1\}$について、#i#の出て来る回数を数え、これを#c[i]#に入れておく。
整列の後では、出力は#c[0]#個の0からはじまり、#c[1]#個の1が続き、#c[2]#個の2が続き、\ldots、#c[k-1]#個の#k-1#で終わる。
このコードは巧妙である。実行の様子を\figref{countingsort}に示す。
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{$0,\ldots,#k#-1=9$のいずれかを格納する、長さ$#n#=20$である配列に対する、計数ソートの操作}
  \figlabel{countingsort}
\end{figure}

このコードと最初の#for#ループでは各カウンタ#c[i]#が#a#において#i#が何回現れるかを数えている。
#a#の値を添え字として使うことで、この処理を合計$O(#n#)$のひとつの#for#ループで終えている。
続いて、#c#を使って直接出力配列#b#を埋めていける。
しかし、#a#にデータが関連づけられているときはこれはできない。
よって、#a#から#b#に要素をコピーするための追加の仕事が必要になる。

次の#for#ループでは、$O(#k#)$の時間をかけてカウンタを順に足しこんでいる。
こうすると#c[i]#は#a#における#i#以下の要素の個数になる。
特に、任意の$#i#\in\{0,\ldots,#k#-1\}$について、出力配列#b#は次の式を満たす。
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i#
\]
最後に#a#を逆に辿って、要素を#b#に入れる。
#a#を辿りながら、要素#a[i]=j#は#b[c[j]-1]#に入れられ、#c[j]#をひとつ減らす。

\begin{thm}
#countingSort(a,k)#は$\{0,\ldots,#k#-1\}$の要素#n#個からなる配列#a#を$O(#n#+#k#)$の時間で整列する。
\end{thm}

計数ソートは\emph{安定性}という良い性質を持つ。
\ejindex{stable sorting algorithm}{あんていしたせいれつあるごりずむ@安定した整列アルゴリズム}%
これは等しい要素の相対的な位置を保つ性質である。
すなわち、要素#a[i]#と#a[j]#が等しい値で、$#i#<#j#$であるとき、#b#においても#a[i]#が#a[j]#の前に来るのである。
この性質は次の節で役立つ。

\subsection{基数ソート}

計数ソートは配列の長さ#n#が、配列内の最大値$#k#-1$と比べてそれほど小さくないなら、非常に効率的である。
今から説明する\emph{基数ソート}は複数回の計数ソートにより、もっと大きな範囲の最大値があっても大丈夫なアルゴリズムである。
\ejindex{radix-sort}{きすうそーと@基数ソート}%

基数ソートは#w#ビットの整数を$#w#/#d#$回の計数ソートにより、一度に#d#ビットずつ整列する。
\footnote{#d#は#w#を割り切れると仮定する。もしそうでないときは#w#を$#d#\lceil #w#/#d#\rceil$に増やす必要がある。}
より正確に言うと、基数ソートはまず整数の最下位#d#ビットだけを見て整列する。続いて、次の#d#ビットだけを見て整列する。このような処理を繰り返し、最後には整数の最高位#d#ビットだけを見て整列する。
\codeimport{ods/Algorithms.radixSort(a)}
（このコードでは#(a[i]>>d*p)&((1<<d)-1)#と書いて、#a[i]#の2進数表記において$(#p#+1)#d#-1,\ldots,#p##d#$ビット目である整数を抽出している。）
このアルゴリズムの例を\figref{radixsort}に示した。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{基数ソートにより$#w#=8$ビットの整数を整列する。$#d#=2$ビットの整数を計数ソートによって整列する処理を4回行う。}
  \figlabel{radixsort}
\end{figure}

この注目に値するアルゴリズムが正しく整列を行うのは、計数ソートが安定なソートアルゴリズムであるからである。
#a#のふたつの要素#x#と#y#が$#x# < #y#$を満たし、#x#と#y#が添え字$r$の位置で異なるなら、$\lfloor r/#d#\rfloor$回目の整列で#x#は#y#より前に置かれる。
そして、以降は#x#と#y#の相対的な位置は変わらない。

基数ソートは#w/d#回の計数ソートを行う。
各計数ソートの実行時間は$O(#n#+2^{#d#})$である。
よって、基数ソートの性能は次の定理のようになる。
\begin{thm}\thmlabel{radix-sort}
任意の整数$#d#>0$について、#radixSort(a,k)#は#n#個の#w#ビット整数を含む配列#a#を$O((#w#/#d#)(#n#+2^{#d#}))$の時間で整列する。
\end{thm}

配列の要素は$\{0,\ldots,#n#^c-1\}$の範囲の整数であり、$#d#=\lceil\log#n#\rceil$だとすれば、\thmref{radix-sort}は次のように整理できる。
\begin{cor}\corlabel{radix-sort-poly}
#radixSort(a,k)#は、$\{0,\ldots,#n#^c-1\}$の範囲の#n#個の整数からなる配列#a#を、$O(c#n#)$の時間で整列する。
\end{cor}

\section{ディスカッションと練習問題}

整列は計算機科学における基本的なアルゴリズムであり、長い歴史がある。
Knuth \cite{k97v3}によればマージソートはvon~Neumann (1945)が考案したものだという。
クイックソートはHoare \cite{h61}が考案した。
はじめにヒープソートを考案したのは
Williams \cite{w64}だが、本節で説明した$O(#n#)$の時間でボトムアップにヒープを構築する方法はFloyd \cite{f64}によるものである。
比較に基づくソートの下界は言い継がれてきたものである。
次の表は比較に基づくアルゴリズムの性能をまとめたものだ。

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
      & \multicolumn{3}{c|}{時間計算量} & in-place  \\ \hline
    マージソート & $#n#\log #n#$ & &  最悪実行時間 & No  \\
    クイックソート & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & 期待実行時間 & Yes \\
    ヒープソート & $2#n#\log #n#$ & ${}+ O(#n#)$ & 最悪実行時間 & Yes \\ \hline
  \end{tabular}
\end{center}

これらの比較に基づくアルゴリズムにはそれぞれの長所・短所がある。
マージソートは比較の回数が最も少なく、ランダムでもない。
しかしマージのときに補助配列が必要だ。
この配列を確保するのはコストが高く、またメモリの制限によりソートに失敗する可能性もある。
クイックソートは入力配列の中だけで処理を済ませ、比較の回数も二番目に少ないか、ランダムなアルゴリズムなので実行時間の保証が常には成り立たない。
\ejindex{in-place algorithm}{in-place なアルゴリズム}%
ヒープソートは比較の回数は最も多いが、入力配列だけを使った決定的なアルゴリズムである。

マージソートが明らかに一番優れている場面がある。
これは連結リストを整列するときである。
この場合は補助的な配列が必要ないのである。
ふたつの整列済みの連結リストはポインタ操作によって簡単に併合でき、整列済みの一つの配列が得られる。
（\excref{list-merge-sort}を参照せよ。）

この章で説明した計数ソートと基数ソートはSeward \cite[Section~2.4.6]{s54}によるものである。
しかし、基数ソートの一種が1920年代からパンチカードを整列するために機械によって使われていた。
この機械はカードの山を、ある場所に穴が空いているかどうかを判定してふたつの山に分けた。
色々な穴の位置についてこの処理を繰り返すことで、基数ソートになる。

最後に、計数ソート・基数ソートはいずれも非負整数以外の数も整列できることを確認しておく。
計数ソートを$\{a,\ldots,b\}$の範囲の整数を整列できるよう素直に修正すれば、実行時間は$O(#n#+b-a)$になる。
同様に基数ソートは同じ範囲の整数を$O(#n#(\log_{#n#}(b-a))$の時間で整列できる。
また、いずれのアルゴリズムも、IEEE754形式の浮動小数点数を整列するのにも使える。
これはIEEEの形式では数の大きさに符号が付いた二進整数表現だと見なして比較ができるように設計されているからである。
\cite{ieee754}

\begin{exc}
$1,7,4,6,2,8,3,5$からなる配列を入力とする、マージソートとヒープソートの実行の様子を描け。
また同じ配列について、クイックソートを実行の様子として、ありえるもののうちのひとつを描け。
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
マージソートの一種で、補助配列を使わずに#DLList#を整列するものを実装せよ。
（\excref{dllist-sort}を参照せよ。）
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#の実装には、軸として常に#a[i]#を選ぶものがある。
この実装が、$\binom{#n#}{2}$回の比較を実行する長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#の実装には、軸として常に#a[i+n/2]#を選ぶものがある。
この実装が、$\binom{#n#}{2}$回の比較を実行する長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
どのような#quickSort(a,i,n,c)#の実装でも、軸を決定的に選び、$#a[i]#,\ldots,#a[i+n-1]#$を先に見ないならば、長さ#n#のある入力が存在し、$\binom{#n#}{2}$回の比較を実行することを示せ。
\end{exc}

\begin{exc}
#quickSort(a,i,n,c)#を実行するとき$\binom{#n#}{2}$回の比較を実行する#Comparator# #c#を設計せよ。
（ヒント：comparatorは比較する値を実際に見なくてもよい。）
\end{exc}

\begin{exc}
\thmref{quicksort}の証明よりも細かくクイックソートが実行する比較の回数の期待値を解析せよ。
具体的には、比較の回数の期待値が$2#n#H_#n# -#n# + H_#n#$であることを示せ。
\end{exc}

\begin{exc}
ヒープソートを実行するときの、比較の回数が$2#n#\log #n#-O(#n#)$回になる入力配列を与え、そのことを説明せよ。
\end{exc}

\javaonly{
\begin{exc}
この章で説明したヒープソートの実装では、要素を逆順に並び替え、その後で順番を逆にしていた。
入力の#Comparator# #c#の結果を否定して新しい#Comparator#を定義すれば、最後のステップを省ける。
これがよい最適化でない理由を説明せよ。
（ヒント：配列を逆さまにするのと比べて、否定のためにどのくらい時間が必要になるかを考えよ。）
\end{exc}
}

\begin{exc}
\figref{comparison-tree-2}の比較木によって正しく整列できない$1,2,3$の別の置換を見つけよ。
\end{exc}

\begin{exc}\exclabel{log-factorial}
$\log #n#! = #n#\log #n#-O(#n#)$を示せ。
\end{exc}

\begin{exc}
$k$個の葉を持つ二分木の高さは$\log k$以上であることを示せ。
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
$k$個の葉を持つ二分木の葉をランダムに選ぶとき、その葉の高さの期待値は$\log k$以上であることを示せ。
%  (Hint: Use induction along with the inequality $(k_1/k)\log k_1 +
%  (k_2/k)\log k_2) \ge  \log k-1$, when $k_1+k_2=k$.)
\end{exc}

% This was a stupid exercise, since IEEE 754 floating point numbers 
% are correctly ordered when they are treated as signed magnitude integers
% [i.e., a sign bit followed by an integer ]
%\begin{exc}
%  Simple floating point numbers are numbers of the form $x\cdot10^{y}$,
%  where $0\le x\le 1$ and $y$ is an integer and each of $x$ and $y$ can
%  be represented by at most $k$ bits.  Describe a version of radix-sort
%  that can be used to sort simple floating point numbers.
%
%  Extend your version of radix sort so that it can handle signed values
%  of both $x$ and $y$ and implement a version of radix-sort that sorts
%  arrays of type #float#.
%\end{exc}

\begin{exc}
この章で説明した#radixSort(a,k)#は入力配列#a#が非負整数だけからなるとき動作する。
入力配列が負の整数を含むときにも、正しく動作するように実装を修正せよ。
\end{exc}

