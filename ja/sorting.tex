\chapter{整列アルゴリズム}
\chaplabel{sorting}

この章では、大きさ#n#の集合を整列するアルゴリズムを紹介する。
データ構造の教科書なのに、整列アルゴリズムの説明が始まるなんて、奇妙に思うかもしれない。これにはいくつか理由がある。
例えば、この章で紹介するクイックソートはランダム二分探索木と深い関係があるし、ヒープソートはデータ構造のヒープと深い関係がある。

\secref{sort-by-comparison}では、比較に基づく整列アルゴリズムを3つ紹介する。
3つとも、整列にかかる実行時間が$O(#n#\log #n#)$であるような整列アルゴリズムである。
さらに、この実行時間が漸近的に最適であることを示す。
つまり、比較に基づく整列アルゴリズムでは、最悪実行時間にせよ平均実行時間にせよ、最低でも約$#n#\log#n#$回の比較が必要なのだ。

なお、これまでの章で説明してきた整列済み集合#SSet#や優先度付き#Queue#であれば、どれを使っても$O(#n#\log #n#)$の時間で整列可能なアルゴリズムを実装できる。
例えば、#n#個の要素を#BinaryHeap#または#MeldableHeap#に#add(x)#し、続いて#remove()#を#n#回繰り返せば、順番に要素を取り出せる。
あるいは、何らかの二分探索木に対して#add(x)#を#n#回実行し、そのあと行きがけ順（\excref{tree-traversal}）で木を巡回すれば、整列された順で要素が見つかる。
ただし、どちらの例でも、わざわざ作ったデータ構造の限られた機能しか使っておらず、かなり無駄なことをしている。% TODO caprice ここ自然に訳したい
整列は、可能な限り速く、単純で、省メモリな手法をあえて開発するだけの価値がある、極めて重要な問題なのである。

章の後半では、比較のほかに使える操作があれば、さらに優れた実行時間で整列が可能なことを見る。
実際、配列のランダムアクセスを使うことで、#n#個の要素$\{0,\ldots,#n#^c-1\}$を$O(c#n#)$の時間で整列できることを説明する。

\section{比較に基づく整列}
\seclabel{sort-by-comparison}

\ejindex{comparison-based sorting}{ひかくにもとづくせいれつ@比較に基づく整列}%
\ejindex{sorting algorithm!comparison-based}{せいれつあるごりずむ@整列アルゴリズム!ひかくにもとづく@比較に基づく}%
この節では、マージソート、クイックソート、ヒープソートという3つの整列アルゴリズムを紹介する。
3つとも、配列#a#を入力すると、$O(#n#\log #n#)$の（期待）実行時間で#a#の要素を昇順に整列する。
いずれも\emph{比較に基づく}アルゴリズムである。
\javaonly{第二引数#c#は#Comparator#である。
\index{Comparator@#Comparator#}%
これは#compare(a,b)#メソッドを実装するオブジェクトである。
\index{compare@#compare(a,b)#}%
}
整列するデータの型は何でもよい。
ただし、データの比較を実行するメソッドがなければならない。
\secref{sset}と同様に、以降では#compare(a,b)#というメソッドがあるとする。
#compare(a,b)#は、$#a#<#b#$なら負の値を、$#a#>#b#$なら正の値を、$#a#=#b#$ならゼロを返すものとする。

\subsection{マージソート}
\seclabel{merge-sort}

\ejindex{merge-sort}{まーじそーと@マージソート}%
\emph{マージソート（merge sort）}は、再帰的な分割統治法の例として古典的なアルゴリズムである。
\ejindex{divide-and-conquer}{ぶんかつとうちほう@分割統治法}%
配列#a#の長さが1以下なら、#a#はすでに整列されており、何もする必要はない。
そうでなければ、配列#a#を半分ずつ、すなわち、配列$#a0#=#a[0]#,\ldots,#a[n/2-1]#$および配列$#a1#=#a[n/2]#,\ldots,#a[n-1]#$に分ける。
#a0#と#a1#をそれぞれ再帰的に整列し、整列済みの#a0#と#a1#とを併合することで、整列済みの配列#a#を得る
\javaonly{\footnote{訳注：Javaの#Arrays.copyOfRange(T[] original, int from, int to)#メソッドは、#to#をコピー対象として含まないことに注意。そのため、擬似コードでは#a.length/2#が#from#と#to#で1回ずつ用いられている。}}
。

\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
\pcodeimport{ods/Algorithms.mergeSort(a)}
\figref{merge-sort}にマージソートの例を示す。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
%XXX: mergeSort(a, c) only for cpp edition?
  \caption{#mergeSort(a,c)#を実行する様子}
  \figlabel{merge-sort}
\end{figure}

#a0#と#a1#の併合は、整列と比べればかなり簡単である。
#a#に要素を1つずつ加えていけばよい。
#a0#か#a1#がどちらかが空になったら、空でないほうの配列の残りの要素をすべて#a#に加える。
それまでは、#a0#の次の要素と#a1#の次の要素のうち、小さいほうを#a#に加えていく。
\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}
\pcodeimport{ods/Algorithms.merge(a0,a1,a)}
% XXX: 原著、nの説明がなかったので定理11.1の内容を参考に補った
#a0#と#a1#の要素数の合計を#n#とすると、#merge(a0,a1,a,c)#では、#a0#または#a1#が空になる前に最大で$#n#-1$回の比較を行う。

マージソートの実行時間を求めるには、\figref{mergesort-recursion}のような再帰的な木を考えるとよい。
いま、#n#が2の冪乗であると仮定する。
このとき、$#n#=2^{\log #n#}$であり、$\log #n#$は整数となる。
マージソートでは、#n#個の要素の整列問題を、「$#n#/2$個の要素の並べ替え」という2つの問題に分割して考える。
分割した2つの問題をそれぞれさらに2つの問題に分割することで、「$#n#/4$個の要素の並べ替え」が4つになる。
この4つの問題をそれぞれさらに分割することで、「$#n#/8$個の要素の並べ替え」が8つになる。
これを繰り返して、「2個の要素の並べ替え」が$#n#/2$個になり、最終的には「1個の要素の並べ替え」が#n#個になる。
大きさ$#n#/2^{i}$の問題を解くには、すでに解かれた2つの部分問題の解答をマージしながらコピーするのに、$O(#n#/2^i)$の時間がかかる。
大きさ$#n#/2^i$の問題が$2^i$個あるので、
大きさ$2^i$の問題のために必要な時間の合計は次のようになる（まだ再帰的に数え上げていないことに注意）。
\[
       2^i\times O(#n#/2^i) = O(#n#)
\]
よって、マージソートに必要な時間の合計は次のようになる。
\[
   \sum_{i=0}^{\log #n#} O(#n#) = O(#n#\log #n#)
\]

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{マージソートの再帰木}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}

次の定理は、以上の解析に基づいて証明できる。ただし、#n#が2の冪乗でない場合も扱うので、少しだけ注意が必要になる。
\begin{thm}
  \javaonly{#mergeSort(a,c)#}\cpponly{mergeSort(a)}\pcodeonly{merge\_sort(a)}の実行時間は$O(#n#\log #n#)$であり、最大で$#n#\log #n#$回の比較を行う。
\end{thm}

\begin{proof}
$#n#$についての帰納法により証明する。
$#n#=1$の場合は自明である。
配列の長さが0または1のときは、単に配列を返すだけで、比較を行わない。

長さの合計が$#n#$である2つのリストを併合するときは、最大で$#n#-1$回の比較が必要である。
%XXX: mergeSort(a, c) only for cpp edition?
長さ#n#の配列#a#に対して#mergeSort(a,c)#を実行するときに必要な比較回数の最大値を$C(#n#)$とする。
$#n#$が偶数なら、それぞれの部分問題に対して帰納法の仮定を適用し、次のように計算できる。
\begin{align*}
  C(#n#)
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n#
\end{align*}
$#n#$が奇数の場合は、やや複雑になる。
この場合は、次に示す2つの不等式を使う。
まず、任意の$x\ge 1$について次の不等式が成り立つ。
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace \eqlabel{log-ineq-a}
\end{equation}
また、任意の$x\ge 1/2$について次の不等式が成り立つ。
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace \eqlabel{log-ineq-b}
\end{equation}
いずれの不等式も簡単に検証できる。
まず、不等式\myeqref{log-ineq-a}は、$\log(x)+1 = \log(2x)$が成り立つことからいえる。
不等式\myeqref{log-ineq-b}は、$\log$が上に凸な関数であるであることによりいえる。
これらの不等式を利用することで、奇数#n#について次の式が成り立つ。
\begin{align*}
  C(#n#)
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2)
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2)
           - \log (#n#/2-1/2)) \\
  % XXX: n >= 3 を利用している？（n=1 は base case として確認しているので、正しいとは思うけど）
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace \qedhere
\end{align*}
\end{proof}

\subsection{クイックソート}

\ejindex{quicksort}{くいっくそーと@クイックソート}%
\emph{クイックソート（quicksort）}も古典的な分割統治アルゴリズムである。
クイックソートでは、2つの部分問題を解いたあとで結果を併合するマージソートとは違い、事前にすべての処理を済ませてしまう。

クイックソートの説明は単純だ。
まず、#a#からランダムに\emph{軸（pivot）}となる要素#x#を選ぶ。
\ejindex{pivot element}{じく@軸}%
そして、#x#より小さい要素、#x#と同じ要素、#x#より大きい要素の3つに#a#を分割する。
そして、分割の1つめと3つめを再帰的に整列する。
\figref{quicksort}に例を示す。
% TODO このアルゴリズムのコメント行が何故かバグっている -- spinute: どこ？ "??" のこと？だとすると、これはバグではなくて、大きいか小さいかわかっていないことを表す記号ではないかと思われる
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\pcodeimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{\javaonly{#quickSort(a,0,14,c)#}\cpponly{#quickSort(a,0,14)#}\pcodeonly{#quickSort(a,0,14)#}の実行例}
    \figlabel{quicksort}
  \end{center}
\end{figure}
% XXX: in-placeの訳語 % TALK in-placeは日本語wikipediaでもin-placeだった -- spinute: それでよさそうです

すべての処理はin-place\footnote{訳注：in-placeとは、入力配列#a#以外にはどの時点においても定数個の変数しか使わない、ということを表す。前節のマージソートの実装は、#a#の一部をコピーするために別の配列を必要としたので、in-placeなアルゴリズムではない。ただし、マージソートをin-placeに行う方法も存在する。}で実行される。
したがって、整列のために配列の各部をコピーすることはない。
その代わり、\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}では、配列の一部$#a[i]#,\ldots,#a[i+n-1]#$だけを整列する。
このメソッドを、最初に\cpponly{#quickSort(a,0,n)#}\pcodeonly{#quickSort(a,0,n,c)#}\javaonly{#quickSort(a,0,n,c)#}として呼び出すわけである。

クイックソートのアルゴリズムで最も肝心なポイントは、in-placeな分割を行うアルゴリズムである。
このアルゴリズムでは、無駄な領域を使わずに#a#の中の要素を入れ替え、次の制約を満たすような添字#p#と#q#を計算する。
\[
   #a[i]# \begin{cases}%
         {}< #x# & \text{if $0\le #i#\le #p#$} \\
         {}= #x# & \text{if $#p#< #i# < #q#$} \\
         {}> #x# & \text{if $#q#\le #i# \le #n#-1$}
     \end{cases}
\]
コード中の#while#ループでは、繰り返しのたびに、これらの制約の1つめと3つめの条件を保ちながら#p#が増加し、#q#は減少していく。
繰り返しの各ステップで、#j#番めの位置にある要素は、前に動くか、その場に留まるか、後ろに動く。
前に動くか、その場に留まる場合は、#j#を1増やす。後ろに動く場合は、#j#番めの要素は未処理ということなので、#j#を増やさない。

クイックソートは、\secref{rbst}で学んだランダム二分探索木と深い関係がある。
実は、相異なる要素をクイックソートに入力した場合の再帰木は、ランダム二分探索木とみなせるのである。
ランダム二分探索木を作るときは、まずランダムに要素#x#を選び、これを根にしたうえで各要素を#x#と比較して、最終的に小さい要素を左の部分木とし、大きい要素を右の部分木とした。
そのことを思い出せば、両者の関係が見えてくるだろう。

クイックソートでは、ランダムに要素#x#を選び、#x#と全要素とを比較したうえで、#x#より小さい要素を配列の前方に、大きい要素を配列の後方に集める。
それから、前方の配列と後方の配列をそれぞれ再帰的に整列する。
同じように、ランダム二分木では、小さい要素を左の部分木、大きい要素を右の部分木とする操作を再帰的に繰り返した。

ランダム二分探索木とクイックソートにこのような対応があるということは、\lemref{rbs}をクイックソートの言葉で言い換えられるということである。

\begin{lem}\lemlabel{quicksort}
クイックソートで整数$0,\ldots,#n#-1$を含む配列を整列するとき、要素#i#と軸とが比較される回数の期待値は、$H_{#i#+1} + H_{#n#-#i#}$以下である。
\end{lem}

調和数の計算により、クイックソートの実行時間に関する次の定理が得られる。

\begin{thm}\thmlabel{quicksort-i}
#n#個の相異なる要素をクイックソートで整列するとき、実行される比較の回数の期待値は$2#n#\ln #n# + O(#n#)$以下である。
\end{thm}

\begin{proof}
#n#個の相異なる要素をクイックソートで整列するときに実行される比較の回数を$T$とする。
\lemref{quicksort}と期待値の線形性より次が成り立つ。
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

\thmref{quicksort}は、整列する要素が互いに相異なる場合についての定理である。
入力の配列#a#が重複する要素を含むとき、クイックソートの実行時間の期待値は悪くはならず、むしろ向上する場合さえある。
これは、重複する要素#x#が軸に選ばれた場合に#x#がまとめられ、2つの部分問題のいずれにも含まれないからである。

\begin{thm}\thmlabel{quicksort}
\javaonly{#quickSort(a,c)#}\cpponly{#quickSort(a)#}\pcodeonly{#quickSort(a,c)#}の実行時間の期待値は$O(#n#\log #n#)$である。
また、実行される比較の回数の期待値は$2#n#\ln #n# +O(#n#)$以下である。
\end{thm}

\subsection{ヒープソート}
\seclabel{heapsort}

\ejindex{heap-sort}{ひーぷそーと@ヒープソート}%

\emph{ヒープソート（heap sort）}もin-placeで処理を行う整列アルゴリズムである。
ヒープソートでは、\secref{binaryheap}で説明した二分木の#BinaryHeap#を使う。
#BinaryHeap#では、ヒープを表現するのに配列を1つ使っていたことを思い出そう。
入力の配列をヒープに変換したあとで最小値を取り出すという操作を繰り返すのがヒープソートである。

具体的には、#n#個の要素を配列#a#に格納する。
各要素は$#a[0]#,\ldots,#a[n-1]#$に入っており、最小値は根、すなわち#a[0]#である。
#a#を#BinaryHeap#に変換したあとで、次の操作を繰り返すのがヒープソートだ。
すなわち、#a[0]#と#a[n-1]#を入れ替え、#n#を1減らし、$#a[0]#,\ldots,#a[n-2]#$を再びヒープにするために#trickleDown(0)#を呼ぶ。
繰り返しが終了した段階、すなわち$#n#=0$になった段階で、#a#の要素は降順に並んでいる。
よって、#a#を逆順にすれば、最終的な整列された状態になる
\footnote{#compare(x,y)#を定義し直せば、繰り返しの結果がそのまま昇順に並ぶようにもできる。}。
\figref{heapsort}に#heapSort(a,c)#の実行の様子を示す。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption{#heapSort(a,c)#の実行中のある瞬間の様子。色がついている部分はすでに整列済みの配列である。色がついていない部分は#BinaryHeap#になっている。次の繰り返しでは、要素$5$が配列の位置$8$に移る}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}
\pcodeimport{ods/Algorithms.heapSort(a)}

% TALK the constructor の訳はWikipediaに倣いコンストラクタとした
ヒープソートにおいて最も重要な処理は、未整列の配列#a#からヒープを構築する部分である。
#BinaryHeap#の#add(x)#を繰り返し実行することで、これを$O(#n#\log#n#)$の時間で簡単に済ませてもいいだろう。
しかし、ボトムアップなアルゴリズムを採用することで、さらに効率良くヒープを構築できる。
#BinaryHeap#では、#a[i]#の子が#a[2i+1]#と#a[2i+2]#に入っていた。% TODO: YJ binary heapの和訳がない？
ということは、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は子を持っていない。
別の言い方をすれば、$#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$は、それぞれ大きさ1の部分ヒープである。
そこで、逆順に、$#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$に対して順番に#trickleDown(i)#を呼べばよい。
これでうまくいくのは、#trickleDown(i)#を呼ぶ時点までに、#a[i]#の子がいずれも部分ヒープの根になっているからだ。
したがって、#trickleDown(i)#を呼ぶと、#a[i]#が次の部分ヒープの根になる。
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

このボトムアップな方法には、#add(x)#を#n#回実行するよりも効率的という特徴がある。
それを見るには、葉の位置に存在する要素は$#n#/2$個あり、これらの要素には何もする必要がないこと、
葉の親であって、#a[i]#を根とする高さが1の部分ヒープは$#n#/4$個あり、これらに対しては#trickleDown(i)#を一度だけ呼べばよいこと、
高さが2の部分ヒープは$#n#/8$個あり、これらに対しては#trickleDown(i)#を二度呼べばよいこと……、に注目すればよい。
#trickleDown(i)#の実行時間は、#a[i]#を根とする部分ヒープの高さに比例するので、実行時間の合計は高々次の値である。
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#)
\]
最後から2つめの等号は、$\sum_{i=1}^{\infty} i/2^{i}$がコインを投げて表が出るまでに要する回数（表が出た回を含む）の期待値に等しいこと（期待値の定義）、および、\lemref{coin-tosses}から成り立つ。

#heapSort(a,c)#の性能は、次の定理によって説明できる。
\begin{thm}
  #heapSort(a,c)#の実行時間は$O(#n#\log #n#)$であり、その際に実行する比較の回数は、最大でも$2#n#\log #n# + O(#n#)$である。
\end{thm}

\begin{proof}
このアルゴリズムには3つのステップがある。
すなわち、（1）#a#をヒープに変形し、（2）#a#の最小値を繰り返し取り出し、（3）#a#を逆順にする。
ステップ1の実行時間は$O(#n#)$で、$O(#n#)$回の比較を行う。
ステップ3の実行時間は$O(#n#)$で、比較は行わない。
ステップ2では#trickleDown(0)#を#n#回呼ぶ。
$i$番めの呼び出しは、大きさ$#n#-i$のヒープに対するもので、最大で$2\log(#n#-i)$回の比較を行う。
$i$についての和を取ると次の値が得られる。
\[
   % XXX: 原著 #n#-i だが、#n#-1ではないか
   % \sum_{i=0}^{#n#-i} 2\log(#n#-i)
   \sum_{i=0}^{#n#-1} 2\log(#n#-i)
   % \le \sum_{i=0}^{#n#-i} 2\log #n#
   \le \sum_{i=0}^{#n#-1} 2\log #n#
   =  2#n#\log #n#
\]
3つのステップにおける比較の実行回数を足し合わせれば証明が完成する。
\end{proof}

\subsection{比較ベースの整列における下界}

\ejindex{lower-bound}{かかい@下界}%
\ejindex{sorting lower-bound}{せいれつあるごりずむのかかい@整列アルゴリズムの下界}%
比較に基づく3つの整列アルゴリズムの実行時間は、いずれも$O(#n#\log #n#)$であった。
ここで気になるのは、もっと速いアルゴリズムがあるかどうかである。
端的に答えると、これは存在しない。
#a#の要素に対して実行できる操作が比較だけなら、$#n#\log #n#$回程度の比較が必要なのである。
これを示すのは難しくないが、そのためには想像力が必要だ。
最終的は次の事実から導かれる。
\[
   \log(#n#!)
     = \log #n# + \log (#n#-1) + \dots + \log(1)
     = #n#\log #n# - O(#n#)
\]
（この事実の証明を\excref{log-factorial}とした。）

まずは、マージソートやヒープソートのような決定的なアルゴリズムを、ある特定の値#n#について実行した場合について考える。
このようなアルゴリズムで#n#個の相異なる要素を整列する場面を想像してみよう。
下界を示すには、#n#を固定したとき、決定的なアルゴリズムで最初に比較する要素が常に決まったペアであることに注目する。
例えば#heapSort(a,c)#では、#n#が偶数の場合、最初に#trickleDown(i)#を呼ぶときは#i=n/2-1#であり、#a[n/2-1]#と#a[n-1]#とを最初に比較する。

入力の要素はすべて相異なっているので、最初の比較の結果は2通りしかない。
2回めの比較の結果は、最初の比較の結果に依存するだろう。
3回めの比較は、その前の2回の比較の結果に依存するだろう。
以降も同様に考えると、比較に基づく決定的な整列アルゴリズムは、根を持った二分木になる。
この二分木を\emph{比較木（comparison tree）}と呼ぶことにする。
\ejindex{comparison tree}{ひかくぎ@比較木}%
比較木のノード#u#には、#u.i#と#u.j#という添字の組が対応していて、$#a[u.i]#<#a[u.j]#$ならば整列アルゴリズムが左の部分木に進むものと考える。
そうでなければ、整列アルゴリズムが右の部分木に進むものと考える。
比較木の葉#w#には、$0,\ldots,#n#-1$の置換$#w.p[0]#,\ldots,#w.p[n-1]#$のいずれかが対応している。
整列アルゴリズムが、比較木の特定の葉に到達するということは、その葉に対応する置換で#a#の整列を表現できることを意味する。
つまり、#a#は次のように整列される。
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]#
\]
大きさ#n=3#の配列に対する比較木の例を\figref{comparison-tree}に示す。
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption{長さ#n=3#の配列$#a[0]#,#a[1]#,#a[2]#$を整列するときの比較木}
  \figlabel{comparison-tree}
\end{figure}

整列アルゴリズムの比較木を見ることで、そのアルゴリズムのすべてがわかる。
#n#個の相異なる要素からなる配列#a#を入力とした場合に実行される一連の比較が正確にわかり、#a#を整列するためにアルゴリズムがどうやって#a#を並べ替えているかが見えるのである。
必然的に、比較木には少なくとも$#n#!$個の葉がある。
もしそうでなければ、同じ葉に到達する別々の入力が2つ存在してしまう。
その場合、そのアルゴリズムでは、同じ葉に到達する入力のうち少なくとも1つを正しく整列できないことになる。

例えば\figref{comparison-tree-2}に示した比較木には$4$つ（$< 3!=6$）の葉がある。
この木を見ると、2つの入力$3,1,2$と$3,2,1$がいずれも右端の葉に到達することがわかる。
入力$3,1,2$に対する出力は$#a[1]#=1,#a[2]#=2,#a[0]#=3$であり、これは正しく整列されている。
しかし、入力$3,2,1$に対する出力は$#a[1]#=2,#a[2]#=1,#a[0]#=3$となり、正しく整列されていない。
以上の議論より、比較に基づくアルゴリズムの本質的な下界が得られる。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{正しく整列できない入力が存在する比較木}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
比較に基づく任意の決定的な整列アルゴリズム$\mathcal{A}$と、任意の整数$#n#\ge 1$について、ある長さ#n#の入力配列#a#が存在して$\mathcal{A}$が#a#を整列するとき、$\log(#n#!) = #n#\log#n#-O(#n#)$回の比較が実行される。
\end{thm}

\begin{proof}
これまでの議論から、$\mathcal{A}$の比較木には少なくとも$#n#!$個の葉が必要である。
帰納法により、$k$個の葉を持つ二分木の高さが$\log k$以上であることを簡単に示せる。
よって、$\mathcal{A}$の比較木には深さ$\log(#n#!)$以上の葉#w#が存在し、ある入力配列#a#がこの葉に到達する。
この#a#に対して、$\mathcal{A}$では$\log(#n#!)$回以上の比較が実行される。
\end{proof}

\thmref{deterministic-sorting-lower-bound}は、マージソートやヒープソートなどの決定的なアルゴリズムに関するものである。
クイックソートのようなランダム性を利用するアルゴリズムに関しては、\thmref{deterministic-sorting-lower-bound}では何もわからない。
ランダム性を利用するアルゴリズムであれば、比較回数の下界$\log(#n#!)$を打ち破れるだろうか。
実は、やはりできないのである。
これを示すには、ランダム性を利用するアルゴリズムを別の観点から考えてみればよい。

以下の議論では、決定木は次の意味で「整理されている」と仮定する。% "cleaned up": 整理されてある 散らかっていない
具体的には、どのような入力配列#a#によっても到達できないノードは取り除かれているとする。
このとき、木にはちょうど$#n#!$個だけの葉が含まれる。
葉の数が$#n#!$以上であることは、そうでないと正しく整列できないことからわかる。
葉の数が$#n#!$以下であることは、#n#個の相異なる要素の置換は$#n#!$通りであり、それぞれが決定木における根から葉への経路をちょうど1つ辿ることからわかる。

ランダムなソートアルゴリズム$\mathcal{R}$は、2つの入力を取る決定的なアルゴリズムだと考えられる。
その2つの入力とは、整列すべき入力配列#a#と、$[0,1]$内のランダムな実数の長い列$b=b_1,b_2,b_3,\ldots,b_m$である。
$b$は、アルゴリズムが利用するランダム性のために必要になる。
すなわち、アルゴリズムでコイン投げによるランダムな選択が必要になったときに$b$の要素を使う。
例えば、クイックソートにおいて最初の軸を選ぶとき、アルゴリズムでは式$\lfloor n b_1\rfloor$を使う。

$b$として、ある固定的な列$\hat{b}$を使うと、$\mathcal{R}$は決定的なアルゴリズムになる。
このアルゴリズムを$\mathcal{R}(\hat{b})$とし、これによる比較木を$\mathcal{T}(\hat{b})$とする。
#a#を$\{1,\ldots,#n#\}$の置換からランダムに選ぶことは、$\mathcal{T}(\hat{b})$の$#n#!$個の葉のうちの1つをランダムに選ぶことと同じである点に注意してほしい。

\excref{randomized-lower-bound}の証明では、$k$個の葉を持つ二分木の葉をランダムに選ぶときに、葉の深さの期待値が$\log k$以上であることが必要だった。
したがって、$\{1,\ldots,n\}$の置換からランダムに選んだものを入力とするとき、決定的なアルゴリズム$\mathcal{R}(\hat{b})$で実行される比較の回数の期待値は$\log(#n#!)$以上である。
結局、任意の$\hat{b}$についてこれが成り立つので、$\mathcal{R}$についても同じことが成り立つ。
こうしてランダム性を利用するアルゴリズムについても下界が示された。

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
任意の整数$n\ge 1$と、任意の比較（決定的でもランダムでもよい）に基づく整列アルゴリズム$\mathcal{A}$について、$\{1,\ldots,n\}$の置換からランダムに選んだ入力を整列するときに実行する比較の回数の期待値は$\log(#n#!) = #n#\log#n#-O(#n#)$以上である。
\end{thm}

\section{計数ソートと基数ソート}

この節では、比較に基づくものではない整列アルゴリズムを2つ紹介する。
この節で紹介するアルゴリズムは、小さい整数を整列することに特化し、要素（の一部）を配列の添字として使うことで、\thmref{deterministic-sorting-lower-bound}の下界に制約されない。
次のような文を考えてみよう。
\[
  #c[a[i]]# = 1
\]
この文は定数時間で実行できるが、#a[i]#の値が何であるかに応じて、#c.length#種類の値を取りうる。
% XXX: 次の文が何を言っているのかよくわからない <- YJ: 比較に基づく整列アルゴリズムは図１１．５にある比較木（二分木）で表すことが出来る。これは比較の出力が２値であり、ある処理の次に行う処理は２通りしかないからである。一方countingの出力は整数値なので、次に行う処理もそれだけの場合分けがありうる。
つまり、このような文を使うアルゴリズムは、比較木のような二分木ではモデル化できない。
突き詰めると、これこそが、この節で紹介するアルゴリズムが比較に基づくアルゴリズムよりも速く整列をこなせる理由なのである。

\subsection{計数ソート}

$0,\ldots,#k#-1$の範囲の要素#n#個からなる入力の配列#a#があるとする。
\emph{計数ソート（counting sort）}では、#a#を整列するのに、カウンタを保持する補助配列#c#を使う。
\ejindex{counting-sort}{けいすうそーと@計数ソート}%
そして、整列された#a#を、補助配列#b#として返す。

計数ソートの背後にあるアイデアは単純だ。
各$#i#\in\{0,\ldots,#k#-1\}$について#i#が#a#に登場する回数を数え、それを#c[i]#に入れていく。
整列が終わったときには、#c[0]#個の0が続き、#c[1]#個の1が続き、#c[2]#個の2が続き、…、#c[k-1]#個の#k-1#が続くような出力になっているだろう。
この操作を巧妙なコードで実行する。実行の様子を\figref{countingsort}に示す。
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{$0,\ldots,#k#-1=9$のいずれかを格納する、長さ$#n#=20$の配列に対する計数ソートの操作}
  \figlabel{countingsort}
\end{figure}

このコードの最初の#for#ループでは、それぞれカウンタ#c[i]#を設置し、#a#において#i#が何回現れるかを数えている。
#a#の値を添字として使うことにより、すべての処理を1つの#for#ループにより$O(#n#)$で終えている。
この段階で、#c#から直接、出力配列#b#を埋めていくこともできるだろう。
しかし、#a#の要素に何かしらのデータが関連づけられている場合、それだとうまくいかない。
そこで、#a#から#b#へと要素をコピーする作業が追加で必要になる。

次の#for#ループでは、#c[i]#が#a#における#i#以下の要素の個数になるように、カウンタを順に足しこんでいる。
これには$O(#k#)$の時間がかかる。
任意の$#i#\in\{0,\ldots,#k#-1\}$について、出力配列#b#は特に次の式を満たす。
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i#
\]
最後に、要素を#b#へと順番に入れていくため、#a#を逆順に辿る。
その際は、要素#a[i]=j#を#b[c[j]-1]#に入れ、#c[j]#を1減らす。

\begin{thm}
#countingSort(a,k)#は、集合$\{0,\ldots,#k#-1\}$に含まれる#n#個の要素からなる配列#a#を$O(#n#+#k#)$の時間で整列する。
\end{thm}

計数ソートには\emph{安定性（stable）}という良い性質がある。
\ejindex{stable sorting algorithm}{あんていしたせいれつあるごりずむ@安定した整列アルゴリズム}%
これは、元の配列において等しい要素同士の相対的な位置が、整列後の配列でも保たれるという性質である。
すなわち、要素#a[i]#と#a[j]#が等しい値で、$#i#<#j#$であるとき、#b#においても#a[i]#が#a[j]#の前にくる。
この性質は次の項で役に立つ。

\subsection{基数ソート}

計数ソートは、配列内の要素の最大値$#k#-1$と比べて配列の長さ#n#がそれほど小さくない場合には非常に効率的である。
これから説明する\emph{基数ソート（radix sort）}では、最大値が比較的大きな場合でも効率的になるように、計数ソートを複数回利用する。
\ejindex{radix-sort}{きすうそーと@基数ソート}%

基数ソートでは、#w#ビットの整数を整列させるのに、一度に#d#ビットずつ、$#w#/#d#$回の計数ソートを実行する
\footnote{#d#は#w#を割り切れると仮定する。もしそうでないときは、#w#を$#d#\lceil #w#/#d#\rceil$に増やす必要がある。}。
より正確に言うと、基数ソートでは、まず整数の最下位#d#ビットだけを見て整列する。続いて、次の#d#ビットだけを見て整列する。これを繰り返し、最後には整数の最高位#d#ビットだけを見て整列する。
\codeimport{ods/Algorithms.radixSort(a)}
（このコードでは、#(a[i]>>d*p)&((1<<d)-1)#と書くことで、#a[i]#の二進表記において$(#p#+1)#d#-1,\ldots,#p##d#$ビットめを抜き出して並べた整数を取り出している。）

このアルゴリズムの例を\figref{radixsort}に示す。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{基数ソートにより$#w#=8$ビットの整数を整列する。$#d#=2$ビットの整数を計数ソートによって整列する処理を4回行う}
  \figlabel{radixsort}
\end{figure}

このアルゴリズムで正しく整列できるのは、計数ソートが安定なソートアルゴリズムだからである。
#a#の2つの要素#x#と#y#が$#x# < #y#$を満たし、#x#と#y#が添字$r$の位置で異なるなら、$\lfloor r/#d#\rfloor$回めの整列で#x#は#y#より前に置かれる。
そして、以降は#x#と#y#の相対的な位置は変わらない。

基数ソートでは$#w#/#d#$回の計数ソートを実行する。
各計数ソートの実行時間は$O(#n#+2^{#d#})$である。
よって、基数ソートの性能は次の定理のようになる。
\begin{thm}\thmlabel{radix-sort}
任意の整数$#d#>0$について、#radixSort(a,k)#は、#n#個の#w#ビット整数を含む配列#a#を、$O((#w#/#d#)(#n#+2^{#d#}))$の時間で整列する。
\end{thm}

配列の要素が$\{0,\ldots,#n#^c-1\}$の範囲の整数であり、$#d#=\lceil\log#n#\rceil$だとすれば、\thmref{radix-sort}は次のように整理できる。
\begin{cor}\corlabel{radix-sort-poly}
#radixSort(a,k)#は、$\{0,\ldots,#n#^c-1\}$の範囲の#n#個の整数からなる配列#a#を、$O(c#n#)$の時間で整列する。
\end{cor}

\section{ディスカッションと練習問題}

整列は計算機科学における基本的なアルゴリズムであり、長い歴史がある。
Knuthによれば、マージソートはvon~Neumannが1945年に考案したものだという\cite{k97v3}。
クイックソートはHoareが考案した\cite{h61}。
最初にヒープソートを考案したのはWilliamsである\cite{w64}。
ただし、本節で説明した$O(#n#)$の時間でボトムアップにヒープを構築する方法は、Floydによるものである\cite{f64}。
比較に基づくソートの下界は古くから知られていた。
次の表に、比較に基づくアルゴリズムの性能をまとめる。

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
      & \multicolumn{3}{c|}{時間計算量} & in-place  \\ \hline
    マージソート & $#n#\log #n#$ & &  最悪実行時間 & No  \\
    クイックソート & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & 期待実行時間 & Yes \\
    ヒープソート & $2#n#\log #n#$ & ${}+ O(#n#)$ & 最悪実行時間 & Yes \\ \hline
  \end{tabular}
\end{center}

これらの比較に基づくアルゴリズムには、それぞれ長所と短所がある。
マージソートは比較の回数が最も少なく、ランダムでもない。
しかし、併合に補助配列が必要だ。
この配列を確保するコストが高い。また、メモリの制限によりソートに失敗する可能性もある。
クイックソートは、入力配列だけで処理が可能であり、比較の回数も2番めに少ないが、ランダム性を利用するアルゴリズムなので実行時間の保証が常には成り立たない。
\ejindex{in-place algorithm}{in-place なアルゴリズム}%
ヒープソートは、比較の回数は最も多いが、入力配列だけを使った決定的なアルゴリズムである。

マージソートが明らかに一番優れている場面がある。
それは、連結リストを整列する場合である。
この場合は補助的な配列が必要ない。
2つの整列済みの連結リストは、ポインタ操作によって簡単に併合でき、整列済みの配列が1つ得られる
（\excref{list-merge-sort}を参照）。

この章で説明した計数ソートと基数ソートはSewardによるものである\cite[Section~2.4.6]{s54}。
ただし、1920年代から、パンチカードを整列するために基数ソートの一種が使われていた機械がある。
この機械では、カードの山を、ある場所に穴が空いているかどうかを判定して2つの山に分けた。
さまざまな穴の位置についてこの処理を繰り返すことで、基数ソートになる。

最後に、計数ソートと基数ソートがいずれも非負整数以外の数も整列できることを確認しておく。
計数ソートを単純に修正して、$\{a,\ldots,b\}$の範囲の整数を整列できるようにすれば、実行時間は$O(#n#+b-a)$になる。
同様に、基数ソートは同じ範囲の整数を$O(#n#(\log_{#n#}(b-a))$の時間で整列するように修正できる。
なお、いずれのアルゴリズムも、IEEE754形式の浮動小数点数の整列にも使える。
これは、IEEE 754形式は、数の大きさに符号が付いた二進整数表現とみなして比較できるように設計されているからである\cite{ieee754}。

\begin{exc}
$1,7,4,6,2,8,3,5$からなる配列を入力とする、マージソートとヒープソートを実行する様子を描け。
また、同じ配列について、クイックソートを実行する様子としてありえるものを1つ描け。
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
マージソートの一種で、補助配列を使わずに#DLList#を整列するものを実装せよ
（\excref{dllist-sort}を参照）。
\end{exc}

\begin{exc}
\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}の実装には、常に#a[i]#を軸として選ぶものがある。
この実装で$\binom{#n#}{2}$回の比較が実行されるような、長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}の実装には、常に#a[i+n/2]#を軸として選ぶものがある。
この実装で$\binom{#n#}{2}$回の比較が実行されるような、長さ#n#の入力の例を示せ。
\end{exc}

\begin{exc}
軸を決定的に選び、最初に$#a[i]#,\ldots,#a[i+n-1]#$を見ることがない\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}の実装には、どんな実装であっても、$\binom{#n#}{2}$回の比較が実行されるような長さ#n#のある入力が存在することを示せ。
\end{exc}

\begin{exc}
\cpponly{#quickSort(a,i,n)#}\pcodeonly{#quickSort(a,i,n,c)#}\javaonly{#quickSort(a,i,n,c)#}に渡す#Comparator# #c#で、
クイックソートにおいて$\binom{#n#}{2}$回の比較が実行されるようなものを設計せよ
（ヒント：#Comparator#は比較する値を実際に見なくてもよい）。
\end{exc}

\begin{exc}
クイックソートが実行する比較の回数の期待値を、\thmref{quicksort}の証明より細かく解析せよ。
具体的には、比較の回数の期待値が$2#n#H_#n# -#n# + H_#n#$であることを示せ。
\end{exc}

\begin{exc}
ヒープソートを実行する際の比較の回数が$2#n#\log #n#-O(#n#)$になる入力配列を与え、そのことを説明せよ。
\end{exc}

\javaonly{
\begin{exc}
この章で説明したヒープソートの実装では、逆順に要素を整列し、そのあとで順番を逆にしていた。
入力の#Comparator# #c#の結果を否定して新しい#Comparator#を定義すれば、最後のステップを省けるだろう。
これが良い最適化にならない理由を説明せよ
（ヒント：配列を逆順にするのと比べて、否定のためにどれくらい時間が必要になるかを考えよ）。
\end{exc}
}

\begin{exc}
\figref{comparison-tree-2}の比較木によって正しく整列できない$1,2,3$の置換として、別の例を見つけよ。
\end{exc}

\begin{exc}\exclabel{log-factorial}
$\log #n#! = #n#\log #n#-O(#n#)$を示せ。
\end{exc}

\begin{exc}
$k$個の葉を持つ二分木の高さは$\log k$以上であることを示せ。
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
$k$個の葉を持つ二分木の葉をランダムに選ぶとき、その葉の高さの期待値は$\log k$以上であることを示せ。
%  (Hint: Use induction along with the inequality $(k_1/k)\log k_1 +
%  (k_2/k)\log k_2) \ge  \log k-1$, when $k_1+k_2=k$.)
\end{exc}

% This was a stupid exercise, since IEEE 754 floating point numbers 
% are correctly ordered when they are treated as signed magnitude integers
% [i.e., a sign bit followed by an integer ]
%\begin{exc}
%  Simple floating point numbers are numbers of the form $x\cdot10^{y}$,
%  where $0\le x\le 1$ and $y$ is an integer and each of $x$ and $y$ can
%  be represented by at most $k$ bits.  Describe a version of radix-sort
%  that can be used to sort simple floating point numbers.
%
%  Extend your version of radix sort so that it can handle signed values
%  of both $x$ and $y$ and implement a version of radix-sort that sorts
%  arrays of type #float#.
%\end{exc}

\begin{exc}
この章で説明した#radixSort(a,k)#は、入力配列#a#が非負整数だけからなるときに動作する。
入力配列が負の整数を含むときにも正しく動作するように実装を修正せよ。
\end{exc}

