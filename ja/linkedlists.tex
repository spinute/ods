\chapter{連結リスト}
\chaplabel{linkedlists}

\ejindex{linked list}{れんけつりすと@連結リスト}%
この章でも#List#インターフェースの実装を扱う。
ただし今度は配列ではなくポインタを使う方法である。
この章のデータ構造は、リストの要素を収めたノードの集まりである。
参照（ポインタ）を使ってノードを繋げ、列を作る。
まずは単方向連結リストを紹介する。
これを使うと#Stack#と（FIFO）#Queue#の操作を定数時間で実行できる。
次に双方向連結リストを紹介する。
これを使うと#Deque#の操作を定数時間で実行できる。

#List#インターフェースの実装に連結リストを使うことには、配列を使う場合と比較して、次のような短所と長所がある。
連結リストを使う主な短所は、#get(i)#や#set(i,x)#がすべての要素に対して定数時間ではなくなることだ。
配列とは違って、#i#番目の要素を読み書きする際に、そこまでリストをひとつずつ辿らなければならないのである。
連結リストを使う主な長所は、動的な操作がしやすいことだ。
リストのノードへの参照#u#があれば、#u#の削除や#u#の隣へのノードの挿入が定数時間でできる。
このとき#u#はリストの中のどのノードであってもよい
\footnote{訳注：これも第2章におけるbacking arrayを用いた#List#インターフェースの実装とは対照的である。第2章では、削除と挿入をどれだけ高速に実行できるかは、どのデータ構造も添字#i#に依存していた。}。

\section{SLList：単方向連結リスト}
\seclabel{sllist}

\index{SLList@#SLList#}%
\ejindex{linked list!singly-}{れんけつりすと@連結リスト!たんほうこう@単方向}%
\ejindex{singly-linked list}{たんほうこうれんけつりすと@単方向連結リスト}%

#SLList#（singly-linked list、単方向連結リスト）は、#Node#（ノード）からなる列である。
各ノード#u#は、データ#u.x#と参照#u.next#を保持している。
参照は、列における次のノードを指している。
列の末尾のノード#w#においては$#w.next# = #null#$である。

% TODO: Remove constructors from SLList.Node
\javaimport{ods/SLList.Node}
\cppimport{ods/SLList.Node}

効率のため、#SLList#では、列の先頭と末尾のノードへの参照を保持する変数#head#および#tail#を利用する。
また、列の長さを表す変数#n#も利用する。
\codeimport{ods/SLList.head.tail.n}
#SLList#における#Stack#操作と#Queue#操作を\figref{sllist}に示す。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/sllist}
  \end{center}
  \caption{#SLList#における、#Queue#操作（#add(x)#・#remove()#）と、#Stack#操作（#push(x)#・#pop()#）}
  \figlabel{sllist}
\end{figure}

#SLList#を使うと#Stack#の#push(x)#と#pop()#を効率的に実装できる。
列の先頭に対して追加もしくは削除をすればよい。
#push(x)#は新しいノード#u#を作り、そのデータには値#x#を、参照#u.next#にはそれまでの先頭を設定する。そして#u#を新しい先頭にする。
最後に、#SLList#の要素が1つ増えたので、#n#を1だけ大きくする。

\codeimport{ods/SLList.push(x)}

#pop()#では、#SLList#が空でないことを確認してから$#head=head.next#$として先頭を削除し、#n#を1だけ小さくする。
最後の要素を削除する場合は特別で、#tail#を#null#に設定する。

\codeimport{ods/SLList.pop()}

#push(x)#と#pop()#の実行時間は、いずれも明らかに$O(1)$である。

\subsection{キュー操作}

#SLList#を使って、定数時間で#add(x)#と#remove()#を実行できるFIFOキュー操作も実装できる。
削除についてはリストの先頭から行うので#pop()#と同じである。

\codeimport{ods/SLList.remove()}

一方、要素の追加はリストの末尾に対して行う。
新たに加えるノードを#u#とすると、ほとんどの場合は$#tail.next#=#u#$とすればよい。
しかし$#n#=0$の場合は特別で、$#tail#=#head#=#null#$とする
\footnote{訳注：#tail#が#null#の場合に#tail.next#にアクセスするとエラーとなるので別の対応が必要となる。}。
この場合、#tail#も#head#も#u#になる。

\codeimport{ods/SLList.add(x)}

#add(x)#と#remove()#はいずれも明らかに定数時間で実行できる。

\subsection{要約}

次の定理は#SLList#の性能を整理したものである。

\begin{thm}\thmlabel{sllist}
  #SLList#は、#Stack#と（FIFO）#Queue#インターフェースを実装する。
  #push(x)#、#pop()#、#add(x)#、#remove()#の実行時間はいずれも$O(1)$である。
\end{thm}

#SLList#で#Deque#の操作もほぼすべて実装できる。
足りないのは#SLList#の末尾を削除する操作だ。
#SLList#の末尾を削除するのは難しい。これは、新しい末尾を現在の末尾の1つ前のノードに設定しなければならないためである。
末尾の1つ前のノード#w#では、$#w.next#=#tail#$となる。
困ったことに、このような#w#を見つけるには、#SLList#の各ノードを#head#から順に$#n#-2$回辿っていかなければならない。

\section{#DLList#: 双方向連結リスト}
\seclabel{dllist}

\index{DLList@#DLList#}%
\ejindex{doubly-linked list}{そうほうこうれんけつりすと@双方向連結リスト}%
\ejindex{linked list!doubly-}{れんけつりすと@連結リスト!そうほうこう@双方向}%

#DLList#（doubly-linked list、双方向連結リスト）は、#SLList#に似たノードの列である。
違いは、ノード#u#が直後のノード#u.next#への参照だけでなく、直前のノード#u.prev#への参照も持っている点だ。

\javaimport{ods/DLList.Node}
\cppimport{ods/DLList.Node}

#SLList#の実装では特別扱いが必要なケースがいくつかあった。
例えば、#SLList#の最後のノードを削除するときや、空の#SLList#にノードを追加するときは、#head#と#tail#をふつうと違うやり方で更新する必要があった。
#DLList#では、このような特別なケースがさらに増える。
そこで、#DLList#における特別なケースをシンプルに書くために、ダミーノードを使う。
\ejindex{dummy node}{だみーのーど@ダミーノード}%
ダミーノードとは、データを含まず、ただ場所を占めるだけの空のノードであり、#DLList#で特別扱いが必要なノードをなくすための仕掛けである。
%こうするとノードを特別扱いする必要がなくなるのだ。
具体的には、すべてのノードには#next#と#prev#に加えて#dummy#を持たせる。
この#dummy#は、リストの最後のノードの直後にあり、かつ最初のノードの直前にあると見なす。
これにより、双方向連結リストでは、\figref{dllist}に示すようにノードが循環する。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dllist2}
  \end{center}
  \caption{a,b,c,d,eからなる#DLList#}
  \figlabel{dllist}
\end{figure}

%TODO: Remove constructors from class Node

\codeimport{ods/DLList.n.dummy.DLList()}

#DLList#では、添え字を指定して簡単にノードを見つけられる。
先頭（#dummy.next#）から順方向に列を辿るか、末尾（#dummy.prev#）から逆方向に列を辿ればよい。
% XXX: O記法の中の1は必要か？
こうすれば、#i#番めのノードを見つけるのにかかる時間は$O(1+\min\{#i#,#n#-#i#\})$である。

\codeimport{ods/DLList.getNode(i)}

#get(i)#と#set(i,x)#も簡単である。
#i#番めのノードを見つけ、その値を読み書きすればよい。

\codeimport{ods/DLList.get(i).set(i,x)}

これらの操作の実行時間は、#i#番めのノードを見つける時間に大きく左右されるので、$O(1+\min\{#i#,#n#-#i#\})$である。

\subsection{追加と削除}

#DLList#においてノード#w#の直前にノード#u#を追加したいとき、ノード#w#の参照がわかっているなら、$#u.next#=#w#$および$#u.prev#=#w.prev#$とし、#u.prev.next#とお#u.next.prev#を適切に調整すればよい（\figref{dllist-addbefore}参照）。
ダミーノードがあるので、#w.prev#や#w.next#がない場合を特別扱いせずにすむ。

\codeimport{ods/DLList.addBefore(w,x)}

\begin{figure}
   \begin{center}
      \includegraphics[scale=0.90909]{figs/dllist-addbefore}
   \end{center}
   \caption{#DLList#において、#u#をノード#w#の直前に挿入する。}
   \figlabel{dllist-addbefore}
\end{figure}

#add(i,x)#操作の実装は自明だ。
#DLList#の#i#番めのノードを見つけ、データ#x#を持つ新しいノード#u#をその直前に挿入すればよい。

\codeimport{ods/DLList.add(i,x)}

#add(i,x)#の処理のうち、実行時間が定数でないのは、（#getNode(i)#を使って）#i#番めのノードを見つける処理だけだ。
よって、#add(i,x)#の実行時間は$O(1+\min\{#i#, #n#-#i#\})$である。

#DLList#からノード#w#を削除するのは簡単である。
#w.next#と#w.prev#のポインタを#w#をスキップするように調整すればよい。
ここでもダミーノードのおかげで複雑な場合分けの必要がなくなっている。

\codeimport{ods/DLList.remove(w)}

ここまでくると#remove(i)#も自明だ。
#i#番めのノードを見つけ、これを削除すればよい。

\codeimport{ods/DLList.remove(i)}

#remove(i)#の実行時間は、#getNode(i)#によって#i#番めのノードを見つける処理に左右されるので、$O(1+\min\{#i#, #n#-#i#\})$である。

\subsection{要約}

次の定理は#DLList#の性能をまとめたものである。

\begin{thm}\thmlabel{dllist}
  #DLList#は、#List#インターフェースを実装する。
  #get(i)#、#set(i,x)#、#add(i,x)#、#remove(i)#の実行時間はいずれも$O(1+\min\{#i#,#n#-#i#\})$である。
\end{thm}

#DLList#の操作の実行時間は、#getNode(i)#のコストを無視すると、いずれも定数時間である。
つまり、#DLList#の操作において時間のかかる部分は、目的のノードを見つける処理だけだ。
目的のノードさえ見つかれば、追加、削除、データの読み書きはいずれも定数時間で実行できる。

これは、\chapref{arrays}で説明した配列を使った#List#の実装とは対照的である。
\chapref{arrays}では、目的のノードは定数時間で見つかるが、要素を追加したり削除したりするために配列内の要素をシフトする必要があり、結果として各処理は非定数時間であった。

このことからわかるように、連結リストは何か別の方法でノードの参照が得られるようなアプリケーションに適している。
\javaonly{
Java Collections Frameworkにおける#LinkedHashSet#は、その好例である。
このデータ構造では、集合の要素は双方向連結リストに格納され、双方向連結リストのノードはハッシュテーブルに格納されている
（ハッシュテーブルについては\chapref{hashing}で説明する）。
#LinkedHashSet#から要素を削除するときは、ハッシュテーブルから対象となるノードを定数時間で見つけ、リストからもそのノードは削除される（これも定数時間で実行できる）。}
\cpponly{
例えば、連結リストのノードへの参照を#USet#に格納しておくという手法がある。
そうすれば、連結リストから#x#を削除したい場合には#x#を含むノードを#USet#から素早く探し出し、当該のノードを連結リストから定数時間で削除できる。}

\section{#SEList#：空間効率のよい連結リスト}
\seclabel{selist}

\index{SEList@#SEList#}%
\ejindex{linked list!space-efficient}{れんけつりすと@連結リスト!くうかんこうりつのよい@空間効率の良い}%
連結リストの欠点のひとつは、リストの中央付近の要素へのアクセスに時間がかかる点を除けば、メモリ使用量が多いことである。
#DLList#のノードは、いずれも前後2つのノードへの参照を持つ。
#Node#のフィールドのうち2つがリストを維持するために占められ、データを入れるのに使われるのが残りの1つだけなのである。

#SEList#（space-efficient list）は、この無駄な領域をシンプルなアイデアで削減するデータ構造だ。
#DLList#のように要素を1個ずつノードに入れるのではなく、複数の要素を含むブロック（配列）をデータとしてノードに入れる。
もう少し正確に説明しよう。
#SEList#には\emph{ブロックサイズ}を表す変数#b#がある。
#SEList#の個々のノードは、#b+1#個の要素を収容できる配列をデータとして持つのである。

あとで詳しく説明するが、個々のブロックに対して#Deque#の操作を実行できると都合がよい。
そこで、#BDeque#（bounded deque）というデータ構造を使うことにする。
\index{BDeque@#BDeque#}%
\ejindex{bounded deque}{せいげんつきDeque@制限付きDeque}%
\ejindex{deque!bounded}{Deque!せいげんつき@制限付き}%
#BDeque#は、\secref{arraydeque}で説明した#ArrayDeque#に似たデータ構造だが、少しだけ異なる点がある。
具体的には、新しい#BDeque#を作るときに用意する配列#a#の大きさは#b+1#であり、その後は拡大も縮小もされない。
#BDeque#の重要な特徴は、先頭や末尾に対する要素の追加や削除を定数時間で実行できることだ。
これは要素を他のブロックから移動するうえで都合がよい。

\javaimport{ods/SEList.BDeque}
\cppimport{ods/SEList.BDeque}

\notpcode{
#SEList#はブロックの双方向連結リストである。
} % notpcode
\javaimport{ods/SEList.Node}
\cppimport{ods/SEList.Node}
\javaimport{ods/SEList.n.dummy}
\cppimport{ods/SEList.n.dummy}

\pcodeonly{
#SEList#はブロックの双方向連結リストである。
ポインタ#next#と#prev#に加えて、#SEList#のノード#u#は#BDeque#である#u.d#を含む。
}

\subsection{必要なメモリ量}

#SEList#では、ブロックに含められる要素数に次のような強い制限がある。
すなわち、末尾以外のブロックはすべて$#b#-1$個以上$#b#+1$個以下の要素を含む。
つまり、#SEList#が#n#要素を含むなら、ブロック数は次の値以下である
\footnote{訳注：1が足されているのは$n=0$のような特殊な場合への対応と考えられる。}。
\[
    #n#/(#b#-1) + 1 = O(#n#/#b#)
\]
末尾以外の各ブロックの#BDeque#は$#b#-1$個以上の要素を含むので、各ブロック内の無駄な領域は高々定数である。
ブロックが使う余分なメモリも定数である。
よって、#SEList#の無駄な領域は$O(#b#+#n#/#b#)$である\footnote{訳注：最初の項#b#は、末尾のブロック内の無駄な領域を表す。}。
#b#を$\sqrt{#n#}$の定数倍にすれば、#SEList#の無駄な領域を\secref{rootishspaceusage}で導出した下界に等しくすることができる。

\subsection{要素を検索}

#SEList#の最初の課題は、リストの#i#番めの要素を見つけることである。
要素の位置は次の2つから決まる。
\begin{enumerate}
  \item i番めの要素を含むブロックをデータとして持つノード#u#
  \item そのブロックの中の要素の添字#j#
\end{enumerate}

\javaimport{ods/SEList.Location}
\cppimport{ods/SEList.Location}

ある要素を含むブロックを見つけるには、#DLList#のときと同じ方法を使う。
つまり、目的のノードを先頭から順方向に、もしくは末尾から逆方向に探す。
唯一の違いは、ノードからノードに移るたびにブロックをまるごとスキップできる点だ。

\pcodeimport{ods/SEList.get_location(i)}
\javaimport{ods/SEList.getLocation(i)}
\cppimport{ods/SEList.getLocation(i,ell)}

どのブロックにも少なくとも$#b#-1$個の要素が入っている（そうなっていないブロックは高々1つである）ことを考えれば、
毎回のステップで、探している要素に最低でも$#b#-1$個ずつは近づいていく。
よって、順方向に探索するときは、目的のノードに$O(1+#i#/#b#)$ステップで到達する。
一方、逆方向では$O(1+(#n#-#i#)/#b#)$ステップである。
この2つの値の小さいほうが、このアルゴリズムの実行時間を決める。
つまり、#i#番めの要素を特定するのに要する時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

#i#番めの要素を含むブロックさえ特定できれば、あとは目的のブロックの中でノードを取得するなり設定するなりして#get(i)#もしくは#set(i,x)#を実行できる。

\codeimport{ods/SEList.get(i).set(i,x)}

#get(i)#と#set(i,x)#の実行時間は、#i#番めの要素を含むブロックを探す時間に左右されるので、$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{要素の追加}

#SEList#への要素の追加はもう少し複雑だ。
一般的な場合を考える前に、より簡単な操作である末尾への要素の追加#add(x)#を考えよう。
末尾のブロックが一杯（あるいはそもそもブロックが1つもない）ときは、新しいブロックを割り当ててリストの末尾に追加する。
すると、末尾のブロックが一杯でないことが保証されるので、#x#をその末尾のブロックに追加できる。

\cppimport{ods/SEList.add(x)}
\javaimport{ods/SEList.add(x)}
\pcodeimport{ods/SEList.append(x)}

#add(i,x)#でリストの中に要素を追加しようとすると話が複雑になる。
まず、リストにおける#i#番めの要素が入るはずのノード#u#を特定する。
ここで問題になるのは、ノード#u#のブロックがすでに$#b#+1$個の要素を含んでおり、#x#を入れる隙間がない場合である。

$#u#_0,#u#_1,#u#_2,\ldots$がそれぞれ$#u#, #u.next#, #u.next.next#, \ldots$を表すとする。
#x#を入れる余地があるノードがないか、$#u#_0,#u#_1,#u#_2,\ldots$を探索する。
この探索の過程で3つの場合が考えられる（\figref{selist-add}参照）。

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{@{}l@{}}
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-a}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-b}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#に対する要素#x#の追加で起こりえる3つの状況（この#SEList#ではブロックの大きさ#b#は3である）}
  \figlabel{selist-add}
\end{figure}


\begin{enumerate}
\item すぐ（$r+1\le #b#$ステップ以内）に、一杯でないブロックを持つノード$#u#_r$が見つかる。
この場合は、$r$回のシフトによって要素を次のブロックに移し、$#u#_r$の空いたスペースを$#u#_0$に持ってくる。
すると、#x#を$#u#_0$のブロックに挿入できるようになる。

\item すぐ（$r+1\le #b#$ステップ以内）に、ブロックのリストの末尾に到達する。
この場合には、新しい空のブロックをリストの末尾に追加し、最初のケースと同様の処理を行う。

\item #b#ステップ探しても空きがあるブロックが見つからない。
この場合、$#u#_0,\ldots,#u#_{#b#-1}$はいずれも$#b#+1$個の要素を含むブロックの列である。
新しいブロック$#u#_{#b#}$をこの列の直後に追加し、もともとあった$#b#(#b#+1)$個の要素を、$#u#_0,\ldots,#u#_{#b#}$がいずれも#b#個の要素を含むように分配する。
すると、$#u#_0$のブロックは#b#個の要素しか含まないので、ここに#x#を挿入できる。
\end{enumerate}

\codeimport{ods/SEList.add(i,x)}

#add(i,x)#の実行時間は、上の3つの場合のどれが起きるかによって決まる。
% TALK これ while(u != l.u) の部分の擬似コード間違ってない？ちゃんとやったら最初のふたつもO(#b^2#)の気がする
最初の2つの場合では、最大#b#ブロックにわたって要素を探してシフトするので、実行時間は$O(#b#)$である。
3つめの場合では、#spread(u)#を呼び出して$#b#(#b#+1)$個の要素を動かすので、実行時間は$O(#b#^2)$である。
3つめの場合のコストを無視すれば、#i#番めの位置に要素#x#を挿入するときの実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である
% （この解析の妥当性はあとで償却法で説明する。）
(3つめの場合のコストはあとで償却法で説明する)。

\subsection{要素の削除}

#SEList#から要素を削除する操作は、要素を追加する操作に似ている。
すなわち、まず#i#番めの要素を含むノード#u#を特定し、
そのうえで#u#から要素を削除した結果として#u#のブロックの要素数が$#b#-1$より小さくなってしまう場合への対策が必要だ。

やはり$#u#, #u.next#, #u.next.next#, \ldots$を$#u#_0,#u#_1,#u#_2,\ldots$で表そう。
$#u#_0$のブロックの要素数を$#b#-1$以上にするため、$#u#_0,#u#_1,#u#_2,\ldots$を順番に調べていって、要素をもってくるノードを見つける。% #u#_0を調べる必要はないのでは？ -kshikano
ここでも考えられる可能性は3つある（\figref{selist-remove}参照）。

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics[scale=0.90909]{figs/selist-remove-a}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-b}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#に対する要素#x#の削除で起こりえる3つの儒教（この#SEList#ではブロックの大きさ#b#は3である）}
  \figlabel{selist-remove}
\end{figure}

\begin{enumerate}
\item すぐ（$r+1\le #b#$ステップ以内）に、$#b#-1$より多くの要素を含むノードが見つかる。
この場合は、$r$回のシフトで要素をあるブロックから後方のブロックに送り、$#u#_r$の余った要素を$#u#_0$に持ってくる。
すると、$#u#_0$のブロックから目的の要素を削除できるようになる。

\item すぐ（$r+1\le #b#$ステップ以内）に、リストの末尾に到達する。
この場合、$#u#_r$は末尾のノードであり、そのブロックには$#b#-1$個以上の要素を含むという制約がない。
そのため、1つめの場合と同様に$#u#_r$から要素を借りてきて$#u#_0$に足してよい。
その結果として$#u#_r$のブロックが空になったら削除する。

\item #b#ステップの間に$#b#-1$個より多くの要素を含むブロックが見つからない。
この場合、$#u#_0,\ldots,#u#_{#b#-1}$はいずれも要素数$#b#-1$のブロックの列である。
そこで\emph{#gather#}を呼び、$#b#(#b#-1)$要素を$#u#_0,\ldots,#u#_{#b#-2}$に集める。
これらの$#b#-1$個のブロックは、いずれもちょうど#b#個の要素を含むようになる。
空になった$#u#_{#b#-1}$を削除すれば、
$#u#_0$のブロックが#b#要素を含むようになるので、ここから適当な要素を削除すればよい。
\end{enumerate}

\codeimport{ods/SEList.remove(i)}

3つめの場合における#gather(u)#を無視すれば、#add(i,x)#と同様に、#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{spreadとgatherの償却解析}

続いて、#add(i,x)#と#remove(i)#で実行される可能性がある#gather(u)#と#spread(u)#のコストを考える。
はじめにコードを示す。

\codeimport{ods/SEList.spread(u)}
\codeimport{ods/SEList.gather(u)}

いずれの実行時間でも、支配的なのは二段階ネストしたループである。
内側と外側いずれのループも最大$#b#+1$回実行されるので、これらの操作の実行時間はどちらも$O((#b#+1)^2)=O(#b#^2)$である。
しかし次の補題により、これらのメソッドは#add(i,x)#および#remove(i)#の呼び出し#b#回につき多くとも1回しか呼ばれないことがわかる。

\begin{lem}\lemlabel{selist-amortized}
  空の#SEList#が作られ、$m\ge 1$回の#add(i,x)#および#remove(i)#が実行される
  このとき、#spread()#および#gather()#に要する時間の合計は$O(#b#m)$である。
\end{lem}

\begin{proof}
  ここでは償却解析のためのポテンシャル法を使う。
  \ejindex{potential method}{ぽてんしゃるほう@ポテンシャル法}%
  ノード#u#のブロックの要素数が#b#でないとき、#u#は\emph{不安定（unstable）}であるという。
  （すなわち、#u#は末尾のノードか、要素数が$#b#-1$または$#b#+1$である。）
  ブロックの要素数がちょうど#b#であるノードは\emph{安定（stable）}であるという。
  #SEList#の\emph{ポテンシャル}を不安定なノードの数で定義する。
  ここでは#add(i,x)#と#spread(u)#の呼び出し回数の関係だけを議論する。
  #remove(i)#と#gather(u)#の解析も同様である。

  #add(i,x)#の1つめの場合分けでは、
  ブロックの大きさが変化するノードは$#u#_r$だけである。
  よって高々1つのノードだけが安定から不安定になる。
  2つめの場合わけでは新しいノードが作られ、そのノードは不安定である。
  一方、他のノードの大きさは変わらず、不安定なノードの数は1つだけ増える。
  以上より、1つめ、2つめいずれの場合でも、#SEList#のポテンシャルの増加は高々1である。

  最後に3つめの場合分けでは、$#u#_0,\ldots,#u#_{#b#-1}$はいずれも不安定である。
  $#spread(#u_0#)#$が呼ばれると、これらの#b#個の不安定なノードは$#b#+1$個の安定なノードに置き換えられる。
  そして#x#が$#u#_0$のブロックに追加され、$#u#_0$は不安定になる。
  ポテンシャルの減少は、合計で$#b#-1$である。

  まとめると、ポテンシャルは0からはじまる（リストに1つもノードがない状態）。
  1つめと2つめの場合分けでは、ポテンシャルは高々1増える。
  3つめの場合分けでは、ポテンシャルは$#b#-1$減る。
  不安定なノードの数を表すポテンシャルが0より小さくなることはない。
  つまり、3つめの場合分けのたびに、少なくとも$#b#-1$回、1つめの場合分けと2つめの場合分けが起きる。
  以上より、#spread(u)#が呼ばれるたびに、少なくとも#b#回は#add(i,x)#が呼ばれることが示された。
\end{proof}

\subsection{要約}

次の定理は#SEList#の性能をまとめたものだ。

\begin{thm}\thmlabel{selist}
  #SEList#は、#List#インターフェースを実装する。
  #spread(u)#と#gather(u)#のコストを無視すると、#b#個のブロックを持つ#SEList#の操作について次が成り立つ。
  \begin{itemize}
    \item #get(i)#と#set(i,x)#の実行時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。
    \item #add(i,x)#と#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。
  \end{itemize}
  さらに、空の#SEList#からはじめて、#add(i,x)#および#remove(i)#からなる$m$個の操作の列における#spread(u)#と#gather(u)#の実行時間は、合計で$O(#b#m)$である。

  要素数#n#の#SEList#の領域使用量は、ワード単位で測ると$#n# +O(#b# + #n#/#b#)$である\footnote{メモリの計測方法については\secref{model}の説明を参照。}。
\end{thm}

#SEList#により、#ArrayList#か#DLList#かのトレードオフを調整できる。
つまり、2つのデータ構造のどちらに寄せるかを、ブロックの大きさ#b#によって調整できるのである。
一方の極端な状況は$#b#=2$の場合であり、このとき#SEList#のノードは最大で3つの値を持ち、これは#DLList#と同じである。
もう一方の極端な状況は$#b#>#n#$の場合であり、このときすべての要素が1つの配列に格納され、これは#ArrayList#のようなものである。
これらの両極端な状況の間で、リストに対する要素の追加と削除にかかる時間と、特定の要素を見つけるのにかかる時間とのトレードオフを考えることになる。

\section{ディスカッションと練習問題}

単方向連結リストも双方向連結リストも40年以上前からプログラムで使われており、研究され尽くされた技法である。
例えば、Knuthの\cite[Sections~2.2.3--2.2.5]{k97v1}で議論されている。
#SEList#もデータ構造における有名な練習問題である。
#SEList#は\emph{unrolled linked list}\cite{sra94}と呼ばれることもある。
\ejindex{unrolled linked list|seealso{#SEList#}}{あんろーるされたれんけつりすと@アンロールされた連結リスト|seealso{#SEList#}}%
\ejindex{linked list!unrolled|seealso{#SEList#}}{れんけつりすと@連結リスト!あんろーるされた@アンロールされた|seealso{#SEList#}}%

双方向連結リストの領域使用量を減らすための別な手法として、XOR-listsと呼ばれるものもある。
\ejindex{XOR-list}{XORリスト}%
XOR-listでは、各ノード#u#はポインタの代わりに#u.nextprev#だけを持つ。 % TODO: pointerの値のXORをとったものはポインタと呼んでいいのか？ -- 日本語での読みやすさも考慮して、「ポインタの代わりとなる値」としてみます -kshikano
このポインタの代わりとなる値は、#u.prev#と#u.next#のXOR（排他的論理和）を取ったものである。
リストでは、#dummy#を指すポインタと#dummy.next#を指すポインタの2つが必要だ
（#dummy.next#はリストが空なら#dummy#を、そうでないなら先頭のノードを指す）。
この技法では、#u#と#u.prev#があれば、#u.next#を次の関係式から計算できることを利用している（\verb+^+は2つの引数の排他的論理和とする）。
\[
   #u.next# = #u.prev# \verb+^+ #u.nextprev#
\]
この技法の欠点は、コードが少し複雑になること、JavaやPythonなどのガーベッジコレクションのある言語では使えないことである。 % TODO: Rubyは？
XOR-listについてのさらに踏み込んだ議論は、Sinhaの雑誌記事\cite{s04}を参照してほしい。

\begin{exc}
  #SLList#においてダミーノードを使って#push(x)#・#pop()#・#add(x)#・#remove()#の全ての特殊なケースを避けることができないのは何故か説明せよ。
\end{exc}

\begin{exc}
  #SLList#のメソッド#secondLast()#を設計・実装せよ。
  これは#SLList#の末尾の一つ前の要素を返すものだ。
  この実装の際にリストの要素数#n#を使わずに実装してみよ。
\end{exc}

\begin{exc}
  #SLList#の#get(i)#・#set(i,x)#・#add(i,x)#・#remove(i)#を実装せよ。
  いずれの操作の実行時間も$O(1+#i#)$であること。
\end{exc}

\begin{exc}
  #SLList#の#reverse()#操作を設計・実装せよ。
  これは#SLList#の要素の順番を逆にする操作である。
  この操作の実行時間は$O(#n#)$でなければならず、再帰は使ってはならない。
  また他のデータ構造を補助的に使ったり、新しいノードを作ってもいけない。
\end{exc}

\begin{exc}
  #SLList#および#DLList#の#checkSize()#を設計・実装せよ。
  これはリストを辿り、#n#の値がリストに入っている要素の数と一致するかを確認する操作だ。
  このメソッドはなにも返さないが、もし要素数が#n#と一致しなければ例外を投げる。
\end{exc}

\begin{exc}
  #addBefore(w)#を再実装せよ。
  これはノード#u#を作り、これをノード#w#の直前に追加する操作だ。
  この章を確認しながら実装してはならない。
  もしこの本のコードと完全に一致しなくともあなたの書くコードは正しいかもしれない。
  そのコードをテストし、正しく動くかどうかを確認せよ。
\end{exc}

続くいくつかの問題は#DLList#の操作に関連するものだ。
これらの問題では、新しいノードや一時的な配列を割当ててはいけない。
これらの問題はいずれもノードの#prev#・#next#を書き換えるだけで解くことができる。

\begin{exc}
  #DLList#の操作#isPalindrome()#を実装せよ。
  これはリストが\emph{回文}であるときtrueを返す。
  \ejindex{palindrome}{かいぶん@回文}%
  すなわち、$i\in\{0,\ldots,#n#-1\}$のいずれの場合も#i#番目の要素が$#n#-i-1$番目の要素と等しいかどうかを確認する操作である。
  実行時間は$O(#n#)$である必要がある。
\end{exc}

\begin{exc}
  #rotate(r)#を実装せよ。
  これは#DLList#の要素を回転する操作で、#i#番目の要素を$(#i#+#r#)\bmod #n#$番目の位置に移動するものだ。
  実行時間は$O(1+\min\{#r#,#n#-#r#\})$である必要があり、リスト内のノードを修正してはならない。
\end{exc}


\begin{exc}\exclabel{linkedlist-truncate}
  #truncate(i)#を実装せよ。
  これは#DLList#を#i#番目で切り詰める操作のだ。
  この操作を実行すると、リストの要素数は#i#になり、$0,\ldots,#i#-1$番目の要素だけが残る。
  返り値も別の#DLList#で、これは$#i#,\ldots,#n#-1$番目の要素を含むものである。
  この操作の実行時間は$O(\min\{#i#,#n#-#i#\})$である。
\end{exc}

\begin{exc}
  #DLList#の操作#absorb(l2)#を実装せよ。
  これは別の#DLList# #l2#を引数に取り、#l2#を空にし、その中身を自分の要素として追加する。
  例えば#l1#が$a,b,c$を含み、#l2#が$d,e,f$を含むとき、#l1.absorb(l2)#を実行すると#l1#は$a,b,c,d,e,f$を含み、#l2#は空になる。
\end{exc}

\begin{exc}
  #deal()#を実装せよ。
  これは#DLList#から偶数番目の要素を削除し、それらの要素を含む#DLList#を返す操作だ。
  例えば#l1#が$a,b,c,d,e,f$を含むとき、#l1.deal()#を呼ぶと、#l1#の要素は$a,c,e$になり、$b,d,f$を含むリストが返される。
\end{exc}

\begin{exc}
  #reverse()#を実装せよ。
  これは#DLList#の要素の順序を逆転する操作だ。
\end{exc}

\begin{exc}\exclabel{dllist-sort}
  \ejindex{merge-sort}{まーじそーと@マージソート}%
  この問題は#DLList#を整列するマージソートというアルゴリズムを実装してみるものだ。
  マージソートは\secref{merge-sort}扱う。
  \javaonly{
  要素の比較を#compareTo(x)#メソッドで行えば、#Comparable#インターフェースの実装である要素からなる任意の#DLList#を整列する際に実装を使いまわせる。}
  \begin{enumerate}
    \item #DLList#の#takeFirst(l2)#を実装せよ。
	この操作は#l2#の先頭ノードを取り出しレシーバに追加するものだ。
	これは新しいノードを作らないことを除けば、#add(size(),l2.remove(0))#と等価である。
    \item #DLList#の静的メソッド#merge(l1, l2)#を実装せよ。
	これはふたつの整列済みのリスト#l1#・#l2#を統合し、その結果を含む新たな整列済みリストを返す。
	この操作をすると#l1#・#l2#は空になる。
	例えば#l1#の要素は$a,c,d$、#l2#の要素は$b,e,f$であるとき、このメソッドは$a,b,c,d,e,f$を含むリストを返す。
    \item #DLList#の#sort()#メソッドを実装せよ。
	これはマージソートを使ってリストの全ての要素を整列するものである。
	この再帰的なアルゴリズムは次のように動作する。
       \begin{enumerate}
          \item リストの要素数が0または1ならなにもしない。
          \item そうでないなら#truncate(size()/2)#によってリストをほぼ等しい大きさのふたつのリスト#l1#と#l2#に分割する。
          \item 再帰的に#l1#を整列する。
          \item 再帰的に#l2#を整列する。
          \item 最後に#l1#と#l2#を統合して一つの整列済みリストとする。
       \end{enumerate}
  \end{enumerate}
\end{exc}

つづく数問は発展的なもので、要素が追加・削除される際に#Stack#・#Queue#の最小値がどうなるかについての理解を要求するものである。

\begin{exc}
  \index{MinStack@#MinStack#}%
  #MinStack#を設計・実装せよ。
  これは比較可能な要素を持ち、
  スタックの操作#push(x)#・#pop()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  #min()#はデータ構造に入っている要素のうち最小の値を返す。
  全ての操作の実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinQueue@#MinQueue#}%
  #MinQueue#を設計・実装せよ。
  これは比較可能な要素を持ち、
  キューの操作#add(x)#・#remove()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinDeque@#MinDeque#}%
  #MinDeque#を設計・実装せよ。
  これは比較可能な要素を持ち、
  双方向キューの操作#addFirst(x)#・addLast(x)・#removeFirst()#・#removeLast()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

次の問題は領域効率のよい#SLList#の解析・実装の理解度を測るためのものである。

\begin{exc}
  #SEList#が#Stack#のように使われるとき、つまり#SEList#は$#push(x)#\equiv #add(size(),x)#$と$#pop()#\equiv #remove(size()-1)#$によってのみ更新されるとき、これらの操作の償却実行時間はいずれも#b#の値に依らない定数であることを証明せよ。
\end{exc}

\begin{exc}
  #Deque#の操作をすべてサポートし、いずれの償却実行時間も#b#に依らない定数である#SEList#を設計・実装せよ。
\end{exc}

\begin{exc}
  ビット単位の排他的論理和\verb+^+によってふたつの#int#型の値を入れ替える方法を説明せよ。
  ただし、このときにみっつめの変数を使ってはならないものとする。
\end{exc}





