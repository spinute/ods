\chapter{連結リスト}
\chaplabel{linkedlists}

\ejindex{linked list}{れんけつりすと@連結リスト}%
この章でも#List#インターフェースの実装を扱う。
ただし今度は配列ではなくポインタを使う方法である。
この章のデータ構造は、リストの要素を収めたノードの集まりである。
参照（ポインタ）を使ってノードを繋げ列を作る。
まずは単方向連結リストを紹介する。
これを使うと#Stack#・(FIFO)#Queue#の操作を定数時間で実行できる。
次に双方向連結リストを紹介する。
これを使うと#Deque#の操作を定数時間で実行できる。

#List#インターフェースを実装するために、連結リスト・配列のいずれを使うのがよいかは場合による。
連結リストの短所はどんな要素の#get(i)#・#set(i,x)#も定数時間で行えるわけではないことだ。
配列とは違って#i#番目の要素を読み書きする際にリストをひとつずつ辿らなければならないのである。
連結リストの長所は動的な操作がしやすいことだ。
ノードの参照#u#があれば、#u#を削除したり、#u#の隣にノードを挿入したりするのにかかる時間は定数である。
このとき#u#がリストの中のどのノードであってもよいのだ
\footnote{訳注：第2章で紹介されたbacking arrayを用いたデータ構造とは対照的である。削除と挿入をどれだけ高速に実行できるかは、どのデータ構造も添字#i#に依存していた。}。

\section{SLList：単方向連結リスト}
\seclabel{sllist}

\index{SLList@#SLList#}%
\ejindex{linked list!singly-}{れんけつりすと@連結リスト!たんほうこう@単方向}%
\ejindex{singly-linked list}{たんほうこうれんけつりすと@単方向連結リスト}%

#SLList#（singly-linked list、単方向連結リスト）は#Node#の列である。
各ノード#u#はデータ#u.x#と参照#u.next#を保持している。
参照は列における次のノードを指している。
列の末尾のノード#w#においては$#w.next# = #null#$である。

% TODO: Remove constructors from SLList.Node
\javaimport{ods/SLList.Node}
\cppimport{ods/SLList.Node}

効率のため#SLList#は変数#head#・#tail#で列の先頭・末尾のノードへの参照を保持している。
また#n#は列の長さを表している。
\codeimport{ods/SLList.head.tail.n}
#SLList#における#Stack#・#Queue#操作を\figref{sllist}に示した。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/sllist}
  \end{center}
  \caption{#SLList#における、#Queue#操作（#add(x)#・#remove()#）と、#Stack#操作（#push(x)#・#pop()#）}
  \figlabel{sllist}
\end{figure}

#SLList#を使って#Stack#の#push(x)#・#pop()#を効率的に実装できる。
列の先頭に追加・削除すればよいのである。
#push(x)#は新しいノード#u#を作り、データ値に#x#を設定し、#u.next#を古い先頭とし、#u#を新しい先頭にする。
最後に#SLList#の要素がひとつ増えたので、#n#を1だけ大きくする。

\codeimport{ods/SLList.push(x)}

#pop()#では、#SLList#が空でないことを確認し、$#head=head.next#$として先頭を削除し、#n#を1だけ小さくする。
最後の要素が削除される場合は特別で、#tail#を#null#に設定する必要がある。

\codeimport{ods/SLList.pop()}

明らかに#push(x)#・#pop()#の実行時間はいずれも$O(1)$である。

\subsection{キュー操作}

#SLList#を使ってFIFOキューの操作#add(x)#・#remove()#を定数時間で実行することもできる。
削除はリストの先頭から行われるので、#pop()#と同じである。

\codeimport{ods/SLList.remove()}

一方で要素の追加はリストの末尾に対して行う。
#u#を新たに加えるノードとすると、ほとんどの場合は$#tail.next#=#u#$とすればよい。
しかし$#n#=0$の場合は特別で、$#tail#=#head#=#null#$となっている
\footnote{訳注：つまり#tail#が#null#で#tail.next#にアクセスするとエラーとなるため、別の対応が必要となる。}。
この場合、#tail#も#head#も#u#になる。

\codeimport{ods/SLList.add(x)}

明らかに#add(x)#・#remove()#はいずれも定数時間で実行できる。

\subsection{要約}

次の定理は#SLList#の性能を整理したものである。

\begin{thm}\thmlabel{sllist}
  #SLList#は#Stack#・(FIFO) #Queue#インターフェースの実装である。
  #push(x)#・#pop()#・#add(x)#・#remove()#の実行時間はいずれも$O(1)$である。
\end{thm}

#SLList#は#Deque#の操作をほぼすべて実装している。
足りないのは#SLList#の末尾を削除する操作だ。
#SLList#の末尾を削除するのは難しいが、これは新しい末尾を現在の末尾のひとつ前のノードに設定しなければならないためである。
末尾のひとつ前のノード#w#とは$#w.next#=#tail#$であるもののことだ。
困ったことに#w#を見つけるには#SLList#を#head#から順に$#n#-2$個のノードだけ辿っていかなければならないのである。

\section{#DLList#: 双方向連結リスト}
\seclabel{dllist}

\index{DLList@#DLList#}%
\ejindex{doubly-linked list}{そうほうこうれんけつりすと@双方向連結リスト}%
\ejindex{linked list!doubly-}{れんけつりすと@連結リスト!そうほうこう@双方向}%

#DLList#（doubly-linked list、双方向連結リスト）は#SLList#に似ている。
違いはノード#u#が直後のノード#u.next#への参照と直前のノード#u.prev#への参照との両方を持っている点だ。

\javaimport{ods/DLList.Node}
\cppimport{ods/DLList.Node}

#SLList#を実装には特別扱いしなければならない場合がいくつかあった。
例えば#SLList#の最後のノードを削除するときや、空の#SLList#にノードを追加するときには#head#・#tail#をふつうと違うやり方で更新する必要があった。
#DLList#ではこういう特別な場合がより多い。
ダミーノードを使うと#DLList#のこれら特別な場合をシンプルに書ける。
\ejindex{dummy node}{だみーのーど@ダミーノード}%
ダミーノードはデータを含まずただ場所だけを占める空のノードで、これを置くと特別扱いする必要のあるノードがなくなるのだ。
%こうするとノードを特別扱いする必要がなくなるのだ。
すべてのノードには#next#と#prev#がある。
#dummy#はリストの最後のノードの直後にあり、かつ最初のノードの直前にあると見なす。
こうすると双方向連結リストのノードは\figref{dllist}に示すように循環する。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dllist2}
  \end{center}
  \caption{a,b,c,d,eからなる#DLList#}
  \figlabel{dllist}
\end{figure}

%TODO: Remove constructors from class Node

\codeimport{ods/DLList.n.dummy.DLList()}

#DLList#から添え字を指定してノードを見つけるのは簡単だ。
先頭（#dummy.next#）から順方向に列を辿るか、末尾（#dummy.prev#）から逆方向に列を辿ればよい。
% XXX: O記法の中の1は必要か？
こうして#i#番目のノードを見つけるのにかかる時間は$O(1+\min\{#i#,#n#-#i#\})$である。

\codeimport{ods/DLList.getNode(i)}

#get(i)#・#set(i,x)#もまた簡単である。
#i#番目の頂点を見つけ、その値を読み書きすればよい。

\codeimport{ods/DLList.get(i).set(i,x)}

これらの操作の実行時間のうち支配的なのは#i#番目のノードを見つける時間なので、実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。

\subsection{追加と削除}

#DLList#におけるノード#w#の参照を持っていて、ノード#u#を#w#の直前に追加したいときは、$#u.next#=#w#$、$#u.prev#=#w.prev#$とし、#u.prev.next#・#u.next.prev#を適切に調整すればよい。（\figref{dllist-addbefore}を参照せよ。）
ダミーノードがあるので#w.prev#や#w.next#がない場合を特別扱いせずにすむ。

\codeimport{ods/DLList.addBefore(w,x)}

\begin{figure}
   \begin{center}
      \includegraphics[scale=0.90909]{figs/dllist-addbefore}
   \end{center}
   \caption{#DLList#において、#u#をノード#w#の直前に挿入する。}
   \figlabel{dllist-addbefore}
\end{figure}

#add(i,x)#操作の実装は自明だ。
#DLList#の#i#番目のノードを見つけ、データ#x#を持つ新しいノード#u#をその直前に挿入すればよい。

\codeimport{ods/DLList.add(i,x)}

#add(i,x)#の処理のうち実行時間が定数でないのは（#getNode(i)#を使って）#i#番目のノードを見つける処理だけだ。
よって#add(i,x)#の実行時間は$O(1+\min\{#i#, #n#-#i#\})$である。

#DLList#からノード#w#を削除するのは簡単である。
#w.next#・#w.prev#のポインタを#w#をスキップするように調整すればよいのだ。
ここでもまたダミーノードのおかげで複雑な場合分けの必要がなくなっている。

\codeimport{ods/DLList.remove(w)}

ここまでくると#remove(i)#も自明だ。
#i#番目のノードを見つけ、これを削除すればよい。

\codeimport{ods/DLList.remove(i)}

#getNode(i)#によって#i#番目のノードを見つける処理が支配的なので、#remove(i)#の実行時間は$O(1+\min\{#i#, #n#-#i#\})$である。

\subsection{要約}

次の定理は#DLList#の性能をまとめたものである。

\begin{thm}\thmlabel{dllist}
  #DLList#は#List#インターフェースを実装する。
  #get(i)#・#set(i,x)#・#add(i,x)#・#remove(i)#の実行時間はいずれも$O(1+\min\{#i#,#n#-#i#\})$である。
\end{thm}

もし#getNode(i)#のコストを無視すると、#DLList#の操作の実行時間はいずれも定数であることは注目に値する。
つまり#DLList#の操作における時間のかかる部分は、興味のあるノードを見つける処理だけなのである。
興味のあるノードさえ見つかれば、追加・削除・データの読み書きはいずれも定数時間で実行できる。

これは\chapref{arrays}で説明した配列を使った#List#の実装とは対照的である。
そのときは興味のあるノードは定数時間で見つかるのだが、要素を追加したり削除したりするために、配列内の要素をシフトする必要があり、その結果として各処理は非定数時間であった。

このことから連結リストは何か別の方法でノードの参照が得られるアプリケーションに適している。
\javaonly{
Java Collections Frameworkにおける#LinkedHashSet#はこの例である。
このデータ構造では、集合の要素は双方向連結リストに格納され、双方向連結リストのノードはハッシュテーブルに格納されている。
（ハッシュテーブルについては、\chapref{hashing}で説明する。）
#LinkedHashSet#から要素を削除するときは、ハッシュテーブルから対象となるノードを定数時間で見つけ、リストからもそのノードは削除される。（これも定数時間で実行できる。）}
\cpponly{
例えば連結リストのノードを#USet#に格納しておく。
すると、要素#x#のノードは#USet#から効率よく見つけられ、このノードはリストから定数時間で削除できる。}

\section{#SEList#：空間効率のよい連結リスト}
\seclabel{selist}

\index{SEList@#SEList#}%
\ejindex{linked list!space-efficient}{れんけつりすと@連結リスト!くうかんこうりつのよい@空間効率の良い}%
連結リストの欠点はそのメモリ使用量である。（リストの真ん中に近い要素へのアクセスに時間がかかるのも欠点だが。）
#DLList#のノードはみな前後合わせてふたつの参照を持つ。
#Node#のフィールドのうちふたつはリストを維持するために占められ、残りのひとつだけがデータを入れるのに使われるのである。

#SEList#(space-efficient list)はシンプルなアイデアでこの無駄な領域を削減する。
#DLList#のように一個ずつ要素を入れるのではなく、複数の要素を含むブロック（配列）をデータとして入れるのである。
もう少し正確に説明する。
#SEList#のパラメータとして\emph{ブロックサイズ}#b#がある。
#SEList#の個々のノードは#b+1#個の要素を収容できる配列をデータとして持つ。

後で詳しく説明するが、個々のブロックには#Deque#の操作を実行できると便利だ。
このために#BDeque# (bounded deque)というデータ構造を使うことにする。
\index{BDeque@#BDeque#}%
\ejindex{bounded deque}{せいげんつきDeque@制限付きDeque}%
\ejindex{deque!bounded}{Deque!せいげんつき@制限付き}%
これは\secref{arraydeque}で説明した#ArrayDeque#みたいなものだ。
#BDeque#は#ArrayDeque#と少しだけ違う。
新しい#BDeque#を作るときに用意する配列#a#の大きさは#b+1#であり、その後拡大も縮小もされない。
#BDeque#の重要な特徴は先頭・末尾の要素を追加・削除する操作を定数時間で実行できることだ。
これは要素を他のブロックから移動するのに役立つ。

\javaimport{ods/SEList.BDeque}
\cppimport{ods/SEList.BDeque}

\notpcode{
#SEList#はブロックの双方向連結リストである。
} % notpcode
\javaimport{ods/SEList.Node}
\cppimport{ods/SEList.Node}
\javaimport{ods/SEList.n.dummy}
\cppimport{ods/SEList.n.dummy}

\pcodeonly{
#SEList#はブロックの双方向連結リストである。
ポインタ#next#・#prev#に加えて、#SEList#のノード#u#は#BDeque#である#u.d#を含む。
}

\subsection{必要なメモリ量}

#SEList#はブロックに含む要素数に次のような強い制限がある。
末尾でないブロックはみな$#b#-1$個以上$#b#+1$個以下の要素を含む。
これはつまり#SEList#が#n#要素を含むならブロック数は次の値以下である
\footnote{訳注：1が足されているのはn=0のような特殊な場合への対応かと思われる。}。
\[
    #n#/(#b#-1) + 1 = O(#n#/#b#)
\]
末尾以外の各ブロックの#BDeque#は$#b#-1$個以上の要素を含むので各ブロック内の無駄な領域は高々定数である。
ブロックが使う余分なメモリも定数である。
よって#SEList#の無駄な領域は$O(#b#+#n#/#b#)$である\footnote{訳注：最初の項#b#は末尾のブロック内の無駄な領域から来ている。}。
#b#を$\sqrt{#n#}$の定数倍にすれば、#SEList#の無駄な領域を\secref{rootishspaceusage}で導出した下界に等しくすることができる。

\subsection{要素を検索}

#SEList#の最初の課題はリストの#i#番目の要素を見つけることである。
要素の位置は次のふたつから決まる。
\begin{enumerate}
  \item i番目の要素を含むブロックをデータとして持つノード#u#
  \item そのブロックの中の要素の添字#j#
\end{enumerate}

\javaimport{ods/SEList.Location}
\cppimport{ods/SEList.Location}

ある要素を含むブロックを見つけるために#DLList#のときと同じ方法を使う。
目的のノードを、先頭から順方向にあるいは末尾から逆方向に探すのだ。
唯一の違いはノードからノードに移る度にブロックをまるごとスキップすることになる点である。

\pcodeimport{ods/SEList.get_location(i)}
\javaimport{ods/SEList.getLocation(i)}
\cppimport{ods/SEList.getLocation(i,ell)}


ひとつ以下のブロックを除いて、すべてのブロックの要素数は$#b#-1$個以上であることを思い出してほしい。
そのため全てのステップで探している要素に最低$#b#-1$個の要素ずつ近づいていく。
よって、順方向に探索するときは目的のノードに$O(1+#i#/#b#)$ステップで到達する。
一方逆方向では$O(1+(#n#-#i#)/#b#)$ステップである。
このふたつの値の小さい方がこのアルゴリズムの実行時間を決める。
つまり、#i#番目の要素を特定するのに要する時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

#i#番目の要素を含むブロックを特定できたので、#get(i)#・#set(i,x)#はあとは目的のブロックの中の添え字を計算すればよい。

\codeimport{ods/SEList.get(i).set(i,x)}

これらの操作の実行時間のうち#i#番目の要素を含むブロックを探す時間が支配的なので、#get(i)#と#set(i,x)#の実行時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{要素の追加}

#SEList#への要素の追加はもう少し複雑だ。
一般的な場合を考える前に、より簡単な操作、末尾に要素を追加する操作#add(x)#を考えよう。
末尾のブロックが一杯（あるいはそもそもブロックがひとつも無い）ときは、新しいブロックを割当ててリストの末尾に追加する。
すると末尾のブロックは一杯でないことが保証されるので、#x#をその末尾のブロックに追加できる。

\cppimport{ods/SEList.add(x)}
\javaimport{ods/SEList.add(x)}
\pcodeimport{ods/SEList.append(x)}

#add(i,x)#でリストの中に要素を追加するのはより複雑だ。
まず#i#番目の要素を入れるべきノード#u#を特定する。
ここで問題になるのは、#u#のブロックが既に$#b#+1$個の要素を含んでおり、#x#を入れる隙間が無い場合である。

$#u#_0,#u#_1,#u#_2,\ldots$がそれぞれ#u#, #u.next#, #u.next.next#...を表すとする。
$#u#_0$に#x#を入れるスペースを提供してくれるブロックを求めて、$#u#_0,#u#_1,#u#_2,\ldots$を探索する。
この探索の過程で3つの場合が考えられる。（\figref{selist-add}を参照せよ。）

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{@{}l@{}}
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-a}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-b}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#において、要素#x#を追加する際に起きる3つの場合。（この#SEList#ではブロックの大きさ#b#は3である。）}
  \figlabel{selist-add}
\end{figure}


\begin{enumerate}
\item すぐに（$r+1\le #b#$ステップ以内に）一杯でないブロックを持つノード$#u#_r$が見つかる。
この場合、$r$回のシフトによって要素を次のブロックに移し、$#u#_r$の空いたスペースを$#u#_0$に持ってくる。
すると#x#を$#u#_0$のブロックに挿入できるようになる。

\item すぐに（$r+1\le #b#$ステップ以内に）ブロックのリストの末尾に到達する。
この場合、新しい空のブロックをリストの末尾に追加し、最初のケースと同様の処理を行う。

\item #b#ステップ探しても空きがあるブロックが見つからない。
この場合、 $#u#_0,\ldots,#u#_{#b#-1}$はいずれも$#b#+1$個の要素を含むブロックの列である。
新しいブロック$#u#_{#b#}$をこの列の直後に追加し、元々あった$#b#(#b#+1)$個の要素を、$#u#_0,\ldots,#u#_{#b#}$がいずれも#b#個の要素を含むように分配する。
すると$#u#_0$のブロックは#b#個の要素しか含まないため、ここに#x#を挿入できる。
\end{enumerate}

\codeimport{ods/SEList.add(i,x)}

#add(i,x)#の実行時間は上の3つの場合のどれが起きるかによって決まる。
% TALK これ while(u != l.u) の部分の擬似コード間違ってない？ちゃんとやったら最初のふたつもO(#b^2#)の気がする
最初のふたつの場合は最大#b#ブロックにわたって要素を探しシフトするので、実行時間は$O(#b#)$である。
3つめの場合では、#spread(u)#を呼び出し$#b#(#b#+1)$個の要素を動かすので、実行時間は$O(#b#^2)$である。
3つめの場合のコストを無視すれば、#i#番目の位置に要素#x#を挿入するときの実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。
% （この解析の妥当性はあとで償却法で説明する。）
(３つめの場合のコストはあとで償却法で説明する。)

\subsection{要素の削除}

#SEList#から要素を削除する操作は要素を追加する操作に似ている。
まずは#i#番目の要素を含むノード#u#を特定する。
そして#u#から要素を削除すると#u#のブロックの要素数が$#b#-1$より小さくなってしまう場合の対策が必要だ。

ここでもまた$#u#_0,#u#_1,#u#_2,\ldots$は#u#, #u.next#, #u.next.next#, ...を表すとする
$#u#_0,#u#_1,#u#_2,\ldots$を順に$#u#_0$のブロックの要素数を$#b#-1$以上にするために要素をもらえるノードを探す。
ここでも考えられる3つの可能性がある。（\figref{selist-remove}を参照せよ。）

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics[scale=0.90909]{figs/selist-remove-a}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-b}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#において、要素#x#を削除する際に起きる3つの場合。（この#SEList#ではブロックの大きさ#b#は3である。）}
  \figlabel{selist-remove}
\end{figure}


\begin{enumerate}
\item すぐに（$r+1\le #b#$ステップ以内に）$#b#-1$より多くの要素を含むノードが見つかる。
この場合、$r$回のシフトで要素をあるブロックから後方のブロックに送り、$#u#_r$の余剰の要素を$#u#_0$に持ってくる。
すると$#u#_0$のブロックから目的の要素を削除できるようになる。

\item すぐに（$r+1\le #b#$ステップ以内に）リストの末尾に到達する。
この場合、$#u#_r$は末尾のノードなので$#u#_r$のブロックには$#b#-1$個以上の要素を含むという制約がない。
そのためひとつめの場合と同様に$#u#_r$から要素を借りてきて$#u#_0$に足してよい。
この結果$#u#_r$のブロックが空になったら削除する。

\item #b#ステップの間に$#b#-1$個より多くの要素を含むブロックが見つからない。
この場合$#u#_0,\ldots,#u#_{#b#-1}$はいずれも要素数$#b#-1$のブロックの列である。
\emph{#gather#}を呼び、$#b#(#b#-1)$要素を$#u#_0,\ldots,#u#_{#b#-2}$に集める。
これらの$#b#-1$個のブロックはいずれもちょうど#b#要素を含むようになる。
そして空になった$#u#_{#b#-1}$を削除する。
すると、$#u#_0$のブロックは#b#要素を含むようになったので、ここから適当な要素を削除できる。
\end{enumerate}

\codeimport{ods/SEList.remove(i)}

#add(i,x)#と同様に、3つめの場合での#gather(u)#を無視すれば、#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{spreadとgatherの償却解析}

続いて、#add(i,x)#・#remove(i)#で実行されるかもしれない#gather(u)#・#spread(u)#のコストを考える。
はじめにコードを示す。

\codeimport{ods/SEList.spread(u)}
\codeimport{ods/SEList.gather(u)}

いずれの実行時間においても支配的なのは二段階ネストしたループである。
内側・外側いずれのループも最大$#b#+1$回実行されるのでいずれの操作の実行時間も$O((#b#+1)^2)=O(#b#^2)$である。
しかし、次の補題によってこれらのメソッドは、#add(i,x)#・#remove(i)#の呼び出し#b#回につき多くとも1回しか呼ばれないことがわかる。

\begin{lem}\lemlabel{selist-amortized}
  空の#SEList#が作られ、$m\ge 1$回#add(i,x)#・#remove(i)#が実行される
  このとき#spread()#・#gather()#に要する時間の合計は$O(#b#m)$である。
\end{lem}

\begin{proof}
  ここでは償却解析のためのポテンシャル法を使う。
  \ejindex{potential method}{ぽてんしゃるほう@ポテンシャル法}%
  ノード#u#のブロックの要素数が#b#でないとき、#u#は\emph{不安定 (unstable)}であるという。
  （すなわち、#u#は末尾のノードか、要素数が$#b#-1$または$#b#+1$である。）
  ブロックの要素数がちょうど#b#であるノードは\emph{安定 (stable)}であるという。
  #SEList#の\emph{ポテンシャル}を不安定なノードの数で定義する。
  ここでは#add(i,x)#と#spread(u)#の呼び出し回数の関係だけを議論する。
  しかし#remove(i)#・#gather(u)#の解析も同様である。

  #add(i,x)#のひとつめの場合分けでは、
  ブロックの大きさが変化するノードは$#u#_r$ひとつだけである。
  よって高々一つのノードだけが安定から不安定になる。
  ふたつめの場合わけでは新しいノードが作られ、そのノードは不安定である。
  一方、他のノードの大きさは変わらず不安定なノードの数はひとつだけ増える。
  以上よりひとつめふたつめいずれの場合でも、#SEList#のポテンシャルの増加は高々1である。

  最後に３つめの場合わけでは$#u#_0,\ldots,#u#_{#b#-1}$はいずれも不安定である。
  $#spread(#u_0#)#$が呼ばれると、これらの#b#個の不安定なノードは$#b#+1$個の安定なノードに置き換えられる。
  そして#x#が$#u#_0$のブロックに追加され、$#u#_0$は不安定になる。
  合わせてポテンシャルは$#b#-1$減少する。

  まとめると、ポテンシャルは0からはじまる。（リストに一つもノードがない状態である。）
  ケース１・２では、ポテンシャルは高々1増える。
  ケース３ではポテンシャルは$#b#-1$減る。
  不安定なノードの数であるポテンシャルは、0より小さくなることはない。
  つまり、ケース３が起きるたびに、少なくとも$#b#-1$回のケース１・２が起きる。
  以上より#spread(u)#が呼ばれる毎に、少なくとも#b#回#add(i,x)#が呼ばれていることが示された。
\end{proof}

\subsection{要約}

次の定理は#SEList#の性能をまとめたものだ。

\begin{thm}\thmlabel{selist}
  #SEList#は#List#インターフェースを実装する。
  #spread(u)#・#gather(u)#のコストを無視すると#b#個のブロックを持つ#SEList#の操作について次が成り立つ。
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。
  \end{itemize}
  さらに、空の#SEList#からはじめて、#add(i,x)#・#remove(i)#からなる$m$個の操作の列における、#spread(u)#・#gather(u)#の実行時間は合わせて$O(#b#m)$である。

  要素数#n#の#SEList#における（ワード単位で測った）\footnote{\secref{model}で説明したメモリの図り方の議論を思い出すこと。}領域使用量は$#n# +O(#b# + #n#/#b#)$である。
\end{thm}

#SEList#により#ArrayList#と#DLList#の間のトレードオフを調整できる。
ブロックの大きさ#b#によって、ふたつのデータ構造のどちらに寄せるかを調整できるのである。
極端な場合として$#b#=2$のとき、#SEList#のノードは最大3つの値を持ち、これは#DLList#と同じである。
もう一方の極端な場合として$#b#>#n#$のとき、すべての要素は一つの配列に格納され、これは#ArrayList#みたいなものだ。
これらの間の調整は、リストへの要素の追加・削除の時間と、特定の要素を見つける時間のトレードオフでもある。

\section{ディスカッションと練習問題}

単方向連結リストも双方向連結リストも40年以上前からプログラムで使われており、研究され尽くしているテクニックである。
例えばKnuthの\cite[Sections~2.2.3--2.2.5]{k97v1}で議論されている。
#SEList#もデータ構造の有名な練習問題である。
#SEList#は\emph{unrolled linked list}\cite{sra94}と呼ばれることもある。
\ejindex{unrolled linked list|seealso{#SEList#}}{あんろーるされたれんけつりすと@アンロールされた連結リスト|seealso{#SEList#}}%
\ejindex{linked list!unrolled|seealso{#SEList#}}{れんけつりすと@連結リスト!あんろーるされた@アンロールされた|seealso{#SEList#}}%

双方向連結リストの領域使用量を減らすための別の手法としてXOR-listsと呼ばれるものもある。
\ejindex{XOR-list}{XORリスト}%
XOR-listでは各ノード#u#はひとつだけのポインタ#u.nextprev#を持つ。 % TODO: pointerの値のXORをとったものはポインタと呼んでいいのか？
このポインタは#u.prev#と#u.next#のXORを取ったものである。
リストは、#dummy#を指すポインタと#dummy.next#を指すポインタの二つを持つ必要がある。
（#dummy.next#はリストが空なら#dummy#を、そうでないなら先頭のノードを指す。）
このテクニックは#u#と#u.prev#があれば#u.next#を次の関係式から計算できることを利用している。
\[
   #u.next# = #u.prev# \verb+^+ #u.nextprev#
\]
（ここで\verb+^+はふたつの引数の排他的論理和を計算する。)
このテクニックはコードを少し複雑すること、JavaやPythonなどガーベッジコレクションのある言語では使えないことは欠点である。 % TODO: Rubyは？
XOR-listのもっと踏み込んだ議論はSinhaの雑誌記事\cite{s04}を参照してほしい。

\begin{exc}
  #SLList#においてダミーノードを使って#push(x)#・#pop()#・#add(x)#・#remove()#の全ての特殊なケースを避けることができないのは何故か説明せよ。
\end{exc}

\begin{exc}
  #SLList#のメソッド#secondLast()#を設計・実装せよ。
  これは#SLList#の末尾の一つ前の要素を返すものだ。
  この実装の際にリストの要素数#n#を使わずに実装してみよ。
\end{exc}

\begin{exc}
  #SLList#の#get(i)#・#set(i,x)#・#add(i,x)#・#remove(i)#を実装せよ。
  いずれの操作の実行時間も$O(1+#i#)$であること。
\end{exc}

\begin{exc}
  #SLList#の#reverse()#操作を設計・実装せよ。
  これは#SLList#の要素の順番を逆にする操作である。
  この操作の実行時間は$O(#n#)$でなければならず、再帰は使ってはならない。
  また他のデータ構造を補助的に使ったり、新しいノードを作ってもいけない。
\end{exc}

\begin{exc}
  #SLList#および#DLList#の#checkSize()#を設計・実装せよ。
  これはリストを辿り、#n#の値がリストに入っている要素の数と一致するかを確認する操作だ。
  このメソッドはなにも返さないが、もし要素数が#n#と一致しなければ例外を投げる。
\end{exc}

\begin{exc}
  #addBefore(w)#を再実装せよ。
  これはノード#u#を作り、これをノード#w#の直前に追加する操作だ。
  この章を確認しながら実装してはならない。
  もしこの本のコードと完全に一致しなくともあなたの書くコードは正しいかもしれない。
  そのコードをテストし、正しく動くかどうかを確認せよ。
\end{exc}

続くいくつかの問題は#DLList#の操作に関連するものだ。
これらの問題では、新しいノードや一時的な配列を割当ててはいけない。
これらの問題はいずれもノードの#prev#・#next#を書き換えるだけで解くことができる。

\begin{exc}
  #DLList#の操作#isPalindrome()#を実装せよ。
  これはリストが\emph{回文}であるときtrueを返す。
  \ejindex{palindrome}{かいぶん@回文}%
  すなわち、$i\in\{0,\ldots,#n#-1\}$のいずれの場合も#i#番目の要素が$#n#-i-1$番目の要素と等しいかどうかを確認する操作である。
  実行時間は$O(#n#)$である必要がある。
\end{exc}

\begin{exc}
  #rotate(r)#を実装せよ。
  これは#DLList#の要素を回転する操作で、#i#番目の要素を$(#i#+#r#)\bmod #n#$番目の位置に移動するものだ。
  実行時間は$O(1+\min\{#r#,#n#-#r#\})$である必要があり、リスト内のノードを修正してはならない。
\end{exc}


\begin{exc}\exclabel{linkedlist-truncate}
  #truncate(i)#を実装せよ。
  これは#DLList#を#i#番目で切り詰める操作のだ。
  この操作を実行すると、リストの要素数は#i#になり、$0,\ldots,#i#-1$番目の要素だけが残る。
  返り値も別の#DLList#で、これは$#i#,\ldots,#n#-1$番目の要素を含むものである。
  この操作の実行時間は$O(\min\{#i#,#n#-#i#\})$である。
\end{exc}

\begin{exc}
  #DLList#の操作#absorb(l2)#を実装せよ。
  これは別の#DLList# #l2#を引数に取り、#l2#を空にし、その中身を自分の要素として追加する。
  例えば#l1#が$a,b,c$を含み、#l2#が$d,e,f$を含むとき、#l1.absorb(l2)#を実行すると#l1#は$a,b,c,d,e,f$を含み、#l2#は空になる。
\end{exc}

\begin{exc}
  #deal()#を実装せよ。
  これは#DLList#から偶数番目の要素を削除し、それらの要素を含む#DLList#を返す操作だ。
  例えば#l1#が$a,b,c,d,e,f$を含むとき、#l1.deal()#を呼ぶと、#l1#の要素は$a,c,e$になり、$b,d,f$を含むリストが返される。
\end{exc}

\begin{exc}
  #reverse()#を実装せよ。
  これは#DLList#の要素の順序を逆転する操作だ。
\end{exc}

\begin{exc}\exclabel{dllist-sort}
  \ejindex{merge-sort}{まーじそーと@マージソート}%
  この問題は#DLList#を整列するマージソートというアルゴリズムを実装してみるものだ。
  マージソートは\secref{merge-sort}扱う。
  \javaonly{
  要素の比較を#compareTo(x)#メソッドで行えば、#Comparable#インターフェースの実装である要素からなる任意の#DLList#を整列する際に実装を使いまわせる。}
  \begin{enumerate}
    \item #DLList#の#takeFirst(l2)#を実装せよ。
	この操作は#l2#の先頭ノードを取り出しレシーバに追加するものだ。
	これは新しいノードを作らないことを除けば、#add(size(),l2.remove(0))#と等価である。
    \item #DLList#の静的メソッド#merge(l1, l2)#を実装せよ。
	これはふたつの整列済みのリスト#l1#・#l2#を統合し、その結果を含む新たな整列済みリストを返す。
	この操作をすると#l1#・#l2#は空になる。
	例えば#l1#の要素は$a,c,d$、#l2#の要素は$b,e,f$であるとき、このメソッドは$a,b,c,d,e,f$を含むリストを返す。
    \item #DLList#の#sort()#メソッドを実装せよ。
	これはマージソートを使ってリストの全ての要素を整列するものである。
	この再帰的なアルゴリズムは次のように動作する。
       \begin{enumerate}
          \item リストの要素数が0または1ならなにもしない。
          \item そうでないなら#truncate(size()/2)#によってリストをほぼ等しい大きさのふたつのリスト#l1#と#l2#に分割する。
          \item 再帰的に#l1#を整列する。
          \item 再帰的に#l2#を整列する。
          \item 最後に#l1#と#l2#を統合して一つの整列済みリストとする。
       \end{enumerate}
  \end{enumerate}
\end{exc}

つづく数問は発展的なもので、要素が追加・削除される際に#Stack#・#Queue#の最小値がどうなるかについての理解を要求するものである。

\begin{exc}
  \index{MinStack@#MinStack#}%
  #MinStack#を設計・実装せよ。
  これは比較可能な要素を持ち、
  スタックの操作#push(x)#・#pop()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  #min()#はデータ構造に入っている要素のうち最小の値を返す。
  全ての操作の実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinQueue@#MinQueue#}%
  #MinQueue#を設計・実装せよ。
  これは比較可能な要素を持ち、
  キューの操作#add(x)#・#remove()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinDeque@#MinDeque#}%
  #MinDeque#を設計・実装せよ。
  これは比較可能な要素を持ち、
  双方向キューの操作#addFirst(x)#・addLast(x)・#removeFirst()#・#removeLast()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

次の問題は領域効率のよい#SLList#の解析・実装の理解度を測るためのものである。

\begin{exc}
  #SEList#が#Stack#のように使われるとき、つまり#SEList#は$#push(x)#\equiv #add(size(),x)#$と$#pop()#\equiv #remove(size()-1)#$によってのみ更新されるとき、これらの操作の償却実行時間はいずれも#b#の値に依らない定数であることを証明せよ。
\end{exc}

\begin{exc}
  #Deque#の操作をすべてサポートし、いずれの償却実行時間も#b#に依らない定数である#SEList#を設計・実装せよ。
\end{exc}

\begin{exc}
  ビット単位の排他的論理和\verb+^+によってふたつの#int#型の値を入れ替える方法を説明せよ。
  ただし、このときにみっつめの変数を使ってはならないものとする。
\end{exc}





