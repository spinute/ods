\chapter{連結リスト}
\chaplabel{linkedlists}

\index{linked list}%
この章でも#List#インターフェースの実装を扱うが、次は配列ではなくポインタを使ったデータ構造の話をする。
この章のデータ構造は、リストの要素を含むノードから構成される。
ノード間は参照（ポインタ）によって繋げられ、列を作る。
%参照（ポインタ）を使ってノードを繋げて列を作る。
まずは単方向連結リストを紹介する。
これを使うと#Stack#・(FIFO)#Queue#の操作を定数時間で実行できる。
次に双方向連結リストを紹介する。
これを使うと#Deque#の操作を定数時間で実行できる。

連結リストを使って#List#インターフェースの実装するのは、配列を使う場合と比べて長所・短所がある。
どんな要素の#get(i)#・#set(i,x)#も定数時間で行えるわけではないのが主な短所だ。
配列と違い、#i#番目の要素までリストをひとつずつ辿らなければならないのである。
連結リストの主な長所は動的な操作がしやすいことだ。
ノードの参照#u#があれば、#u#を削除したり、#u#の隣にノードを挿入したりを定数時間で実行できる。
これが#u#がリストの中のどのノードであっても成り立つのだ。

\section{SLList：単方向連結リスト}
\seclabel{sllist}

\index{SLList@#SLList#}%
\index{linked list!singly-}%
\index{singly-linked list}%

#SLList#（singly-linked list、単方向連結リスト）は#Node#の列である。
各ノード#u#はデータ#u.x#と参照#u.next#を保持している。
参照は列における次のノードを指している。
列の末尾のノード#w#においては$#w.next# = #null#$である。

% TODO: Remove constructors from SLList.Node
\javaimport{ods/SLList.Node}
\cppimport{ods/SLList.Node}

効率のため#SLList#は変数#head#・#tail#で列の先頭・末尾のノードへの参照を保持している。
また#n#は列の長さを表している。
\codeimport{ods/SLList.head.tail.n}
#SLList#における#Stack#・#Queue#操作を\figref{sllist}に示した。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/sllist}
  \end{center}
  \caption{#SLList#における、#Queue#操作（#add(x)#・#remove()#）と、#Stack#操作（#push(x)#・#pop()#）}
  \figlabel{sllist}
\end{figure}

#SLList#を使って#Stack#の#push(x)#・#pop()#を効率的に実装できる。
列の先頭に追加・削除すればよいのである。
#push(x)#は新しいノード#u#を作り、データ値に#x#を設定し、#u.next#を古い先頭とし、#u#を新しい先頭にする。
最後に#SLList#の要素がひとつ増えたので、#n#を1だけ大きくする。

\codeimport{ods/SLList.push(x)}

#pop()#では、#SLList#が空でないことを確認し、$#head=head.next#$として先頭を削除し、#n#を1だけ小さくする。
最後の要素が削除される場合は特別で、#tail#を#null#に設定する必要がある。

\codeimport{ods/SLList.pop()}

明らかに#push(x)#・#pop()#の実行時間はいずれも$O(1)$である。

\subsection{キュー操作}

#SLList#を使ってFIFOキューの操作#add(x)#・#remove()#を定数時間で実行することもできる。
削除はリストの先頭から行われるので、#pop()#と同じである。

\codeimport{ods/SLList.remove()}

一方で要素の追加はリストの末尾に対して行う。
#u#を新たに加えるノードとすると、ほとんどの場合は$#tail.next#=#u#$とすればよい。
しかし$#n#=0$の場合は特別で、代わりに$#tail#=#head#=#null#$とする必要がある。
この場合、#tail#も#head#も#u#になる。

\codeimport{ods/SLList.add(x)}

明らかに#add(x)#・#remove()#はいずれも定数時間で実行できる。

\subsection{要約}

次の定理は#SLList#の性能を整理したものである。

\begin{thm}\thmlabel{sllist}
  #SLList#は#Stack#・(FIFO) #Queue#インターフェースの実装である。
  #push(x)#・#pop()#・#add(x)#・#remove()#の実行時間はいずれも$O(1)$である。
\end{thm}

#SLList#は#Deque#の操作をほぼすべて実装している。
足りないのは#SLList#の末尾を削除する操作だ。
#SLList#の末尾を削除するのは難しいが、これは新しい末尾を現在の末尾のひとつ前のノードに設定しなければならないためである。
末尾のひとつ前のノード#w#とは$#w.next#=#tail#$であるもののことだ。
困ったことに#w#を見つけるには#SLList#を#head#から順に$#n#-2$個のノードを辿っていかなければならないのである。

\section{#DLList#: 双方向連結リスト}
\seclabel{dllist}

\index{DLList@#DLList#}%
\index{doubly-linked list}%
\index{linked list!doubly-}%

#DLList#（doubly-linked list、双方向連結リスト）は#SLList#によく似ている。
違いがあるのは、#DLList#ではノード#u#が直後のノード#u.next#への参照と直前のノード#u.prev#への参照の両方を持っている点だ。

\javaimport{ods/DLList.Node}
\cppimport{ods/DLList.Node}

#SLList#を実装するときにはいくつか特別な処理があった。 %注意しなければならないことがいくつかあった。
例えば#SLList#の最後のノードを削除したり、空の#SLList#にノードを追加するときは#head#・#tail#を適切に更新するため特別な処理が必要であった。
#DLList#ではこういう特別な場合というのがかなり増える。
#DLList#におけるこれら全ての特別な場合を綺麗に扱う最善の方法はおそらくダミーノードを使うことだろう。
\index{dummy node}%
ダミーノードはデータを含まない空のノードで、これを置くことで特別なノードが生じなくなるのだ。 %ただ場所だけを占める。
%こうするとノードを特別扱いする必要がなくなるのだ。
すべてのノードには#next#と#prev#がある。
#dummy#はリストの最後のノードの直後にあり、最初のノードの直前にあると見なす。
こうすると双方向連結リストのノードは\figref{dllist}に示すようなサイクルになる。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dllist2}
  \end{center}
  \caption{a,b,c,d,eからなる#DLList#}
  \figlabel{dllist}
\end{figure}

%TODO: Remove constructors from class Node

\codeimport{ods/DLList.n.dummy.DLList()}

#DLList#で番号を指定してノードを見つけるのは簡単だ。
先頭（#dummy.next#）から順方向に列を辿るか、末尾（#dummy.prev#）から逆方向に列を辿ればよい。
こうして#i#番目のノードを見つけるのにかかる時間は$O(1+\min\{#i#,#n#-#i#\})$である。

\codeimport{ods/DLList.getNode(i)}

#get(i)#・#set(i,x)#もまた簡単である。
#i#番目の頂点を見つけ、その値を読み書きすればよい。

\codeimport{ods/DLList.get(i).set(i,x)}

これらの操作の実行時間のうち支配的なのは#i#番目のノードを見つける時間なので、実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。

\subsection{追加と削除}

#DLList#におけるノード#w#の参照を持っていて、ノード#u#を#w#の直前に追加したいときは、$#u.next#=#w#$、$#u.prev#=#w.prev#$とし、#u.prev.next#・#u.next.prev#を適切に調整すればよい。（\figref{dllist-addbefore}を参照せよ。）
ダミーノードがあるので#w.prev#・#w.next#がない場合を気にする必要はない。

\codeimport{ods/DLList.addBefore(w,x)}

\begin{figure}
   \begin{center}
      \includegraphics[scale=0.90909]{figs/dllist-addbefore}
   \end{center}
   \caption{#DLList#において、#u#をノード#w#の直前に挿入する。}
   \figlabel{dllist-addbefore}
\end{figure}

#add(i,x)#操作の実装は自明だ。
#DLList#の#i#番目のノードを見つけ、データ#x#を持つ新しいノード#u#をその直前に挿入すればよい。

\codeimport{ods/DLList.add(i,x)}

#add(i,x)#の実行時間のうち定数でないのは（#getNode(i)#を使って）#i#番目のノードを見つける処理だけだ。
よって
#add(i,x)#の実行時間は$O(1+\min\{#i#, #n#-#i#\})$である。

#DLList#からノード#w#を削除するのは簡単である。
#w.next#・#w.prev#のポインタを#w#をスキップするように調整すればよいのだ。
ここでもまたダミーノードのおかげで複雑な場合分けの必要がなくなっている。

\codeimport{ods/DLList.remove(w)}

ここまでくると#remove(i)#も自明だ。
#i#番目のノードを見つけ、これを削除すればよい。

\codeimport{ods/DLList.remove(i)}

#getNode(i)#によって#i#番目のノードを見つける処理が支配的なので、#remove(i)#の実行時間は$O(1+\min\{#i#, #n#-#i#\})$である。

\subsection{要約}

次の定理は#DLList#の性能をまとめたものである。

\begin{thm}\thmlabel{dllist}
  #DLList#は#List#インターフェースを実装する。
  #get(i)#・#set(i,x)#・#add(i,x)#・#remove(i)#の実行時間はいずれも$O(1+\min\{#i#,#n#-#i#\})$である。
\end{thm}

もし#getNode(i)#のコストを無視すると、#DLList#の操作の実行時間はいずれも定数であることは注目に値する。
つまり#DLList#の操作における時間のかかる部分は、興味のあるノードを見つける処理だけなのである。
興味のあるノードさえ見つかれば、追加・削除・データの読み書きはいずれも定数時間で実行できる。

これは\chapref{arrays}で説明した配列を使った#List#の実装とは対照的である。
そのときは興味のあるノードは定数時間で見つかるのだが、要素を追加したり削除したりするために、配列内の要素をシフトする必要があり、その結果として各処理は非定数時間であった。

このことから連結リストは何か別の方法でノードの参照が得られるアプリケーションに適している。
\javaonly{
Java Collections Frameworkにおける#LinkedHashSet#はこの例である。
このデータ構造では、集合の要素は双方向連結リストに格納され、双方向連結リストのノードはハッシュテーブルに格納されている。
（ハッシュテーブルについては、\chapref{hashing}で説明する。）
#LinkedHashSet#から要素を削除するときは、ハッシュテーブルから対象となるノードをを定数時間で見つけ、リストからもそのノードは削除される。（これも定数時間で実行できる。）
\cpponly{
例えば、連結リストのノードを#USet#に格納することができる。
そうすると、要素#x#を連結リストから削除するとき、#USet#から要素を効率良く見つけられる。そのため、リストから定数時間で削除できる。}

\section{#SEList#：空間効率のよい連結リスト}
\seclabel{selist}

\index{SEList@#SEList#}%
\index{linked list!space-efficient}%
連結リストの欠点はそのメモリ使用量である。（リストの真ん中に近い要素へのアクセスに時間がかかるのも欠点だが。）
#DLList#のノードはみな前後合わせてふたつの参照を持つ。
#Node#のフィールドのうちふたつはリストを維持するために占められ、残りのひとつだけがデータを入れるのに使われるのである。

#SEList#(space-efficient list)はシンプルなアイデアでこの無駄な領域を削減する。
#DLList#のように一個ずつ要素を入れるのではなく、複数の要素を含むブロック（配列）をデータとして入れるのである。
もう少し正確に説明する。
#SEList#のパラメータとして\emph{ブロックサイズ}#b#がある。
#SEList#の個々のノードは#b+1#要素を収容できる配列をデータとして持つ。

後で詳しく説明するが、個々のブロックには#Deque#の操作を実行できると便利だ。
このために#BDeque# (bounded deque)というデータ構造を使うことにする。
\index{BDeque@#BDeque#}%
\index{bounded deque}%
\index{deque!bounded}%
これは\secref{arraydeque}で説明した#ArrayDeque#みたいなものだ。
#BDeque#は#ArrayDeque#と少しだけ違う。
新しい#BDeque#を作るときに用意する配列#a#の大きさは#b+1#であり、その後拡大も縮小もされない。
#BDeque#の重要な特徴は先頭・末尾の要素を追加・削除する操作を定数時間で実行できることだ。
これは要素を他のブロックから移動するのに役立つ。

\javaimport{ods/SEList.BDeque}
\cppimport{ods/SEList.BDeque}

\notpcode{
#SEList#はブロックの双方向連結リストである。
} % notpcode
\javaimport{ods/SEList.Node}
\cppimport{ods/SEList.Node}
\javaimport{ods/SEList.n.dummy}
\cppimport{ods/SEList.n.dummy}

\pcodeonly{
#SEList#はブロックの双方向連結リストである。
ポインタ#next#・#prev#に加えて、#SEList#のノード#u#は#BDeque#である#u.d#を含む。
}

\subsection{必要なメモリ量}

#SEList#はブロックに含む要素数に次のような強い制限がある。
末尾でないブロックはみな$#b#-1$以上$#b#+1$以下の要素を含む。
これはつまり#SEList#が#n#要素を含むならブロック数は次の値以下である。
\[
    #n#/(#b#-1) + 1 = O(#n#/#b#)
\]
末尾以外の各ブロックの#BDeque#は$#b#+1$以下の要素を含むので各配列内の無駄な領域は高々定数である。
ブロックが使う余分なメモリも定数である。
よって#SEList#の無駄な領域は$O(#b#+#n#/#b#)$である。
#b#を$\sqrt{#n#}$の定数倍にすれば、#SEList#の無駄な領域を\secref{rootishspaceusage}で導出した下界に等しくすることができる。

\subsection{要素を検索}

#SEList#の最初の課題はリストの#i#番目の要素を見つけることである。
要素の位置は次のふたつから決まる。
\begin{enumerate}
  \item i番目の要素を含むブロックをデータとして持つノード#u#
  \item そのブロックの中の要素の添字#j#
\end{enumerate}

\javaimport{ods/SEList.Location}
\cppimport{ods/SEList.Location}

ある要素を含むブロックを見つけるために#DLList#のときと同じ方法を使う。
目的のノードを、先頭から順方向にあるいは末尾から逆方向に探すのだ。
唯一の違うのはノードからノードに移る度にブロックをまるごとスキップすることになる点である。

\pcodeimport{ods/SEList.get_location(i)}
\javaimport{ods/SEList.getLocation(i)}
\cppimport{ods/SEList.getLocation(i,ell)}


最大でひとつのブロックを除いて、すべてのブロックの要素数は$#b#-1$以上であることを思い出してほしい。
そのため全てのステップで探している要素に$#b#-1$以上ずつ近づいていく。
よって、順方向に探索するときは目的のノードに$O(1+#i#/#b#)$ステップで到達する。
一方逆方向では$O(1+(#n#-#i#)/#b#)$ステップである。
このふたつの値の#i#によって決まる小さい方がこのアルゴリズムの実行時間を決める。
つまり、#i#番目の要素を特定するのに要する時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

#i#番目の要素を含むブロックを特定できたので、#get(i)#・#set(i,x)#はあとは目的のブロックの中の添え字を計算すればよい。

\codeimport{ods/SEList.get(i).set(i,x)}

これらの操作の実行時間のうち#i#番目の要素を含むブロックを探す時間が支配的なので、実行時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{要素の追加}

#SEList#への要素の追加はもう少し複雑だ。
一般的な場合を考える前に、より簡単な末尾に要素を追加する操作#add(x)#を考えよう。
末尾のブロックが一杯（あるいはそもそもブロックがひとつも無い）ときは、新しいブロックを割当ててリストの末尾に追加する。
すると末尾のブロックは存在し、一杯でないので、#x#をそのブロックの末尾に追加できる。

\cppimport{ods/SEList.add(x)}
\javaimport{ods/SEList.add(x)}
\pcodeimport{ods/SEList.append(x)}

#add(i,x)#でリストの中に要素を追加するのはより複雑だ。
まず#i#番目の要素を入れるべきノード#u#を特定する。
ここで問題になるのは、#u#のブロックは既に$#b#+1$個の要素を含んでいるため#x#を入れる隙間が無い場合である。

$#u#_0,#u#_1,#u#_2,\ldots$がそれぞれ#u#, #u.next#, #u.next.next#...を表すとする。
$#u#_0,#u#_1,#u#_2,\ldots$を#x#を入れられるスペースを求めて探索する。
この探索の過程で3つの可能性が考えられる。（\figref{selist-add}を参照せよ。）

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{@{}l@{}}
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-a}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-b}\\[4ex]
      \includegraphics[width=\ScaleIfNeeded]{figs/selist-add-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#において、要素#x#を追加する際に起きる3つの場合。（この#SEList#ではブロックの大きさ#b#は3である。）}
  \figlabel{selist-add}
\end{figure}


\begin{enumerate}
\item すぐに（$r+1\le #b#$ステップ以内に）一杯でないブロックを持つノード$#u#_r$が見つかる。
この場合、$r$回のシフトによって要素を次のブロックに移し、$#u#_r$の空いたスペースを$#u#_0$に持ってくる。
すると#x#を$#u#_0$のブロックに挿入できるようになる。

\item すぐに（$r+1\le #b#$ステップ以内に）ブロックのリストの末尾に到達する。
この場合、新しい空のブロックをリストの末尾に追加し、最初のケースと同様の処理を行う。

\item #b#ステップ探してもから出ないブロックが見つからない。
この場合、 $#u#_0,\ldots,#u#_{#b#-1}$はいずれも$#b#+1$個の要素を含むブロックの列である。
新しいブロック$#u#_{#b#}$をこの列の直後に追加し、元々あった$#b#(#b#+1)$この要素を広げる\emph{#spread#を呼ぶ}。
$#u#_0,\ldots,#u#_{#b#}$はいずれも#b#個の要素を含むようになる。
すると$#u#_0$のブロックは#b#個の要素を含むため、ここに#x#を挿入できる。
\end{enumerate}

\codeimport{ods/SEList.add(i,x)}

#add(i,x)#の実行時間は上の3つの場合のどれが起きるかに依って決まる。
上のふたつの場合は最大#b#ブロックにわたって要素を探しシフトするので、実行時間は$O(#b#)$である。
3つめの場合では、#spread(u)#を呼び出し$#b#(#b#+1)$要素を動かすので、実行時間は$O(#b#^2)$である。
3つめの場合のコストを無視すれば#i#番目の位置に要素#x#を挿入するときの実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。
% （この解析の妥当性はあとで償却法で説明する。）
(３つめの場合のコストはあとで償却法で説明する。)

\subsection{要素の削除}

#SEList#から要素を削除する操作は要素を追加する操作に似ている。
まずは#i#番目の要素を含むノード#u#を特定する。
そして#u#から要素を削除すると#u#のブロックの要素数が$#b#-1$より小さくなってしまう場合の対策が必要だ。

ここでもまた$#u#_0,#u#_1,#u#_2,\ldots$は#u#, #u.next#, #u.next.next#, ...を表すとする
$#u#_0,#u#_1,#u#_2,\ldots$を順に$#u#_0$のブロックの要素数を$#b#-1$以上にするために要素をもらえるノードを探す。
ここでも考えられる3つの可能性がある。（\figref{selist-remove}を参照せよ。）

\begin{figure}
  \noindent
  \begin{center}
    \begin{tabular}{l}
      \includegraphics[scale=0.90909]{figs/selist-remove-a}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-b}\\[4ex]
      \includegraphics[scale=0.90909]{figs/selist-remove-c}\\
    \end{tabular}
  \end{center}
  \caption{#SEList#において、要素#x#を削除する際に起きる3つの場合。（この#SEList#ではブロックの大きさ#b#は3である。）}
  \figlabel{selist-remove}
\end{figure}


\begin{enumerate}
\item すぐに（$r+1\le #b#$ステップ以内に）$#b#-1$より多くの要素を含むノードが見つかる。
この場合、$r$回のシフトで要素をあるブロックから後方のブロックに送り、$#u#_r$の余剰の要素を$#u#_0$に持ってくる。
すると$#u#_0$のブロックから目的の要素を削除できるようになる。

\item すぐに（$r+1\le #b#$ステップ以内に）リストの末尾に到達する。
この場合、$#u#_r$は末尾のノードなので$#u#_r$のブロックには$#b#-1$個以上の要素を含むという制約がない。
そのためひとつめの場合と同様に$#u#_r$から要素を借りてきて$#u#_0$に足してよい。
この結果$#u#_r$のブロックが空になったら削除する。

\item #b#ステップの間に$#b#-1$個より多くの要素を含むブロックが見つからない。
この場合$#u#_0,\ldots,#u#_{#b#-1}$はいずれも要素数$#b#-1$のブロックの列である。
\emph{#gather#}を呼び、$#b#(#b#-1)$要素を$#u#_0,\ldots,#u#_{#b#-2}$に集める。
これらの$#b#-1$個のブロックはいずれもちょうど#b#要素を含むようになる。
そして空になった$#u#_{#b#-1}$を削除する。
すると、$#u#_0$のブロックは#b#要素を含むようになったので、ここから適当な要素を削除できる。
\end{enumerate}

\codeimport{ods/SEList.remove(i)}

#add(i,x)#と同様に、3つめの場合での#gather(u)#を無視すれば、#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。

\subsection{spreadとgatherの償却解析}

続いて、#add(i,x)#・#remove(i)#で実行されるかもしれない#gather(u)#・#spread(u)#のコストを考える。
はじめにコードを示す。

\codeimport{ods/SEList.spread(u)}
\codeimport{ods/SEList.gather(u)}

いずれの実行時間においても支配的なのは二段階ネストしたループである。
内側・外側いずれのループも最大$#b#+1$回実行されるのでいずれの操作の実行時間も$O((#b#+1)^2)=O(#b#^2)$である。
しかし、次の補題によってこれらのメソッドは、#add(i,x)#・#remove(i)#の呼び出し#b#回につき多くとも1回しか呼ばれないことがわかる。

\begin{lem}\lemlabel{selist-amortized}
  空の#SEList#が作られ、$m\ge 1$回#add(i,x)#・#remove(i)#が実行される
  このとき#spread()#・#gather()#に要する時間の合計は$O(#b#m)$である。
\end{lem}

\begin{proof}
  ここでは償却解析のためのポテンシャル法を使う。
  \index{potential method}%
  ノード#u#のブロックの要素数が#b#でないとき、#u#は\emph{不安定}であるという。
  （すなわち、#u#は末尾のノードか、要素数が$#b#-1$または$#b#+1$である。）
  ブロックの要素数がちょうど#b#であるノードは\emph{安定}であるという。
  #SEList#の\emph{ポテンシャル}を不安定なノードの数で定義する。
  ここでは#add(i,x)#と#spread(u)#の呼び出し回数の関係だけを議論する。
  しかし#remove(i)#・#gather(u)#の解析も同様である。

  #add(i,x)#のひとつめの場合分けでは、
  ブロックの大きさが変化するノードは$#u#_r$ひとつだけである。
  よって高々一つのノードだけが安定から不安定になる。
  ふたつめの場合わけでは新しいノードが作られ、そのノードは不安定である。
  一方、他のノードの大きさは変わらず不安定なノードの数はひとつだけ増える。
  以上よりひとつめふたつめいずれの場合でも、#SEList#のポテンシャルの増加は高々1である。

  最後に３つめの場合わけでは$#u#_0,\ldots,#u#_{#b#-1}$はいずれも不安定である。
  $#spread(#u_0#)#$が呼ばれると、これらの#b#個の不安定なノードは$#b#+1$個の安定なノードに置き換えられる。
  そして#x#が$#u#_0$のブロックに追加され、$#u#_0$は不安定になる。
  合わせてポテンシャルは$#b#-1$減少する。

  まとめると、ポテンシャルは0からはじまる。（リストに一つもノードがない状態である。）
  ケース１・２では、ポテンシャルは高々1増える。
  ケース３ではポテンシャルは$#b#-1$減る。
  不安定なノードの数であるポテンシャルは、0より小さくなることはない。
  つまり、ケース３が起きるたびに、少なくとも$#b#-1$回のケース１・２が起きる。
  以上より#spread(u)#が呼ばれる毎に、少なくとも#b#回#add(i,x)#が呼ばれていることが示された。
\end{proof}

\subsection{要約}

次の定理は#SEList#の性能をまとめたものだ。

\begin{thm}\thmlabel{selist}
  #SEList#は#List#インターフェースを実装する。
  #spread(u)#・#gather(u)#のコストを無視すると#b#個のブロックを持つ#SEList#の操作について次が成り立つ。
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1+\min\{#i#,#n#-#i#\}/#b#)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(#b#+\min\{#i#,#n#-#i#\}/#b#)$である。
  \end{itemize}
  さらに、空の#SEList#からはじめて、#add(i,x)#・#remove(i)#からなる$m$個の操作の列における、#spread(u)#・#gather(u)#の実行時間は合わせて$O(#b#m)$である。

  要素数#n#の#SEList#における（ワード単位で測った）\footnote{\secref{model}で説明したメモリの図り方の議論を思い出すこと。}領域使用量は$#n# +O(#b# + #n#/#b#)$である。
\end{thm}

#SEList#により#ArrayList#と#DLList#の間のトレードオフを調整できる。
ブロックの大きさ#b#によって、ふたつのデータ構造の濃さを調整できるである。
極端な場合として$#b#=2$のとき、#SEList#のノードは最大3つの値を持ち、これは#DLList#と同じである。
もう一方の極端な場合として$#b#>#n#$のとき、すべての要素は一つの配列に格納され、これは#ArrayList#みたいなものだ。
これらの間の調整は、リストへの要素の追加・削除の時間と、特定の要素を見つける時間のトレードオフでもある。

\section{ディスカッションと練習問題}

単方向連結リストも双方向連結リストも40年以上前からプログラムで使われており、研究され尽くしているテクニックである。
例えばKnuthの\cite[Sections~2.2.3--2.2.5]{k97v1}で議論されている。
#SEList#もデータ構造の有名な練習問題である。
#SEList#は\emph{unrolled linked list}\cite{sra94}と呼ばれることもある。
\index{unrolled linked list|seealso{#SEList#}}%
\index{linked list!unrolled|seealso{#SEList#}}%

双方向連結リストの領域使用量を減らすための別の手法としてXOR-listsと呼ばれるものもある。
\index{XOR-list}%
XOR-listでは各ノード#u#はひとつだけのポインタ#u.nextprev#を持つ。 % TODO: pointerの値のXORをとったものはポインタと呼んでいいのか？
このポインタは#u.prev#と#u.next#のXORを取ったものである。
リストは、#dummy#を指すポインタと#dummy.next#を指すポインタの二つを持つ必要がある。
（#dummy.next#はリストが空なら#dummy#を、そうでないなら先頭のノードを指す。）
このテクニックは#u#と#u.prev#があれば#u.next#を次の関係式から計算できることを利用している。
\[
   #u.next# = #u.prev# \verb+^+ #u.nextprev#
\]
（ここで\verb+^+はふたつの引数の排他的論理和を計算する。)
このテクニックはコードを少し複雑すること、JavaやPythonなどガーベッジコレクションのある言語では使えないことは欠点である。 % TODO: Rubyは？
XOR-listのもっと踏み込んだ議論はSinhaの雑誌記事\cite{s04}を参照してほしい。

\begin{exc}
  #SLList#においてダミーノードを使って#push(x)#・#pop()#・#add(x)#・#remove()#の全ての特殊なケースを避けることができないのは何故か説明せよ。
\end{exc}

\begin{exc}
  #SLList#のメソッド#secondLast()#を設定・実装せよ。
  これは#SLList#の末尾の一つ前の要素を返すものだ。
  この実装の際にリストの要素数#n#を使わずに実装してみよ。
\end{exc}

\begin{exc}
  #SLList#の#get(i)#・#set(i,x)#・#add(i,x)#・#remove(i)#を実装せよ。
  いずれの操作の実行時間も$O(1+#i#)$であること。
\end{exc}

\begin{exc}
  #SLList#の#reverse()#操作を設定・実装せよ。
  これは#SLList#の要素の順番を逆にする操作である。
  この操作の実行時間は$O(#n#)$でなければならず、再帰は使ってはならない。
  また他のデータ構造を補助的に使ったり、新しいノードを作ってもいけない。
\end{exc}

\begin{exc}
  #SLList#および#DLList#の#checkSize()#操作を設計・実装せよ。
  これはリストを辿り、#n#の値がリストに入っている要素の数と一致するかを確認するものだ。
  このメソッドはなにも返さないが、もし要素数が#n#と一致しなければ例外を投げる。
\end{exc}

\begin{exc}
  #addBefore(w)#を再実装せよ。
  これはノード#u#を作り、これをノード#w#の直前に追加するものだ。
  この章を確認しながら実装してはならない。
  もしこの本のコードと完全に一致しなくともあなたの書くコードは正しいかもしれない。
  そのコードをテストし、正しく動くかどうかを確認せよ。
\end{exc}

続くいくつかの問題は#DLList#の操作に関連するものだ。
これらの問題では、新しいノードや一時的な配列を割当ててはいけない。
これらの問題はいずれもノードの#prev#・#next#を書き換えるだけで解くことができる。

\begin{exc}
  #DLList#の操作#isPalindrome()#を実装せよ。
  これはリストが\emph{回文}であるときtrueを返す。
  \index{palindrome}%
  すなわち、$i\in\{0,\ldots,#n#-1\}$のいずれの場合も#i#番目の要素が$#n#-i-1$番目の要素と等しいかどうかを確認するものである。
  実行時間は$O(#n#)$である必要がある。
\end{exc}

\begin{exc}
  #rotate(r)#を実装せよ。
  これは#DLList#の要素を回転するもので、#i#番目の要素を$(#i#+#r#)\bmod #n#$番目の位置に移動するものだ。
  実行時間は$O(1+\min\{#r#,#n#-#r#\})$である必要があり、リスト内のノードを修正してはならない。
\end{exc}


\begin{exc}\exclabel{linkedlist-truncate}
  #truncate(i)#を実装せよ。
  これは#DLList#を#i#番目で切り詰めるものだ。
  この操作を実行すると、リストの要素数は#i#になり、$0,\ldots,#i#-1$番目の要素だけが残る。
  返り値も別の#DLList#で、これは$#i#,\ldots,#n#-1$番目の要素を含むものである。
  この操作の実行時間は$O(\min\{#i#,#n#-#i#\})$である。
\end{exc}

\begin{exc}
  #DLList#の操作#absort(l2)#を実装せよ。
  これは別の#DLList# #l2#を引数に取り、#l2#を空にし、その中身を自分の要素として追加する。
  例えば#l1#が$a,b,c$を含み、#l2#が$d,e,f$を含むとき、#l1.absorb(l2)#を実行すると#l1#は$a,b,c,d,e,f$を含み、#l2#は空になる。
\end{exc}

\begin{exc}
  #deal()#を実装せよ。
  これは#DLList#から偶数番目の要素を削除し、それらの要素を含む#DLList#を返すものだ。
  例えば#l1#が$a,b,c,d,e,f$を含むとき、#l1.deal()#を呼ぶと、#l1#の要素は$a,c,e$になり、$b,d,f$を含むリストが返される。
\end{exc}

\begin{exc}
  #reverse()#を実装せよ。
  これは#DLList#の要素の順序を逆転するものだ。
\end{exc}

\begin{exc}\exclabel{dllist-sort}
  \index{merge-sort}%
  この問題は#DLList#を整列するマージソートというアルゴリズムを実装してみるものだ。
  マージソートは\secref{merge-sort}扱う。
  \javaonly{
  要素の比較を#compareTo(x)#メソッドで行えば、#Comparable#インターフェースの実装である要素からなる任意の#DLList#を整列する際に実装を使いまわせる。}
  \begin{enumerate}
    \item #DLList#の#takeFirst(l2)#操作を実装せよ。
	この操作は#l2#の先頭ノードを取り出しレシーバに追加するものだ。
	これは新しいノードを作らないことを除けば、#add(size(),l2.remove(0))#と等価である。
    \item #DLList#の静的メソッド#merge(l1, l2)#を実装せよ。
	これはふたつの整列済みのリスト#l1#・#l2#を統合し、その結果を含む新たな整列済みリストを返す。
	この操作をすると#l1#・#l2#は空になる。
	例えば#l1#の要素は$a,c,d$、#l2#の要素は$b,e,f$であるとき、このメソッドは$a,b,c,d,e,f$を含むリストを返す。
    \item #DLList#の#sort()#メソッドを実装せよ。
	これはマージソートを使ってリストの全ての要素を整列するものである。
	この再帰的なアルゴリズムは次のように動作する。
       \begin{enumerate}
          \item リストの要素数が0または1ならなにもしない。
          \item そうでないなら#truncate(size()/2)#によってリストをほぼ等しい大きさのふたつのリスト#l1#と#l2#に分割する。
          \item 再帰的に#l1#を整列する。
          \item 再帰的に#l2#を整列する。
          \item 最後に#l1#と#l2#を統合して一つの整列済みリストとする。
       \end{enumerate}
  \end{enumerate}
\end{exc}

つづく数問は発展的なもので、要素が追加・削除される際に#Stack#・#Queue#の最小値がどうなるかについての理解を要求するものである。

\begin{exc}
  \index{MinStack@#MinStack#}%
  #MinStack#を設計・実装せよ。
  これは比較可能な要素を持ち、
  スタックの操作#push(x)#・#pop()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  #min()#はデータ構造に入っている要素のうち最小の値を返す。
  全ての操作の実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinQueue@#MinQueue#}%
  #MinQueue#を設計・実装せよ。
  これは比較可能な要素を持ち、
  キューの操作#add(x)#・#remove()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

\begin{exc}
  \index{MinDeque@#MinDeque#}%
  #MinDeque#を設計・実装せよ。
  これは比較可能な要素を持ち、
  双方向キューの操作#addFirst(x)#・addLast(x)・#removeFirst()#・#removeLast()#・#size()#をサポートし、
  #min()#操作も可能なものである。
  全ての操作の償却実行時間は定数である。
\end{exc}

次の問題は領域効率のよい#SLList#の解析・実装の理解度を測るためのものである。

\begin{exc}
  #SEList#が#Stack#のように使われるとき、つまり#SEList#は$#push(x)#\equiv #add(size(),x)#$と$#pop()#\equiv #remove(size()-1)#$によってのみ更新されるとき、これらの操作の償却実行時間はいずれも#b#の値に依らない定数であることを証明せよ。
\end{exc}

\begin{exc}
  #Deque#の操作をすべてサポートし、いずれの償却実行時間も#b#に依らない定数である#SEList#を設計・実装せよ。
\end{exc}

\begin{exc}
  ビット単位の排他的論理和\verb+^+によってふたつの#int#型の値を入れ替える方法を説明せよ。
  ただし、このときにみっつめの変数を使ってはならないものとする。
\end{exc}





