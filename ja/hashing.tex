\chapter{ハッシュテーブル}
\chaplabel{hashtables}
\chaplabel{hashing}

ハッシュテーブルは大きな集合$U=\{0,\ldots,2^{#w#}-1\}$の要素#n#個（Uの要素数と比べると、#n#は相対的に小さい整数）を格納するための効率的な方法だ。
\emph{ハッシュテーブル}という言葉が指すデータ構造はたくさんある。
\ejindex{hash table}{はっしゅてーぶる@ハッシュテーブル}%
この章の前半ではハッシュテーブルの一般的な実装ふたつを紹介する。
これはチェイン、または線形探索を使うものだ。

ハッシュテーブルは整数でないデータを格納することもよくある。
この場合\emph{ハッシュ値}というデータに対応する整数を使う。
\ejindex{hash code}{はっしゅち@ハッシュ値}%
この章の後半ではハッシュ値の生成方法について説明する。

この章で扱う手法は、ある範囲からランダムに生成した整数を必要とする。
サンプルコードではこのランダムな整数はハードコードされた定数になっている。
この定数は空気中のノイズを利用したランダムなビット列から得られる
\footnote{訳注：物理現象を利用するとランダムな整数が得られる、ということだけ把握すればこの本の理解には差し支えない。}。 % atmosphere noise randomness https://www.random.org/

\section{#ChainedHashTable#: チェイン法を使ったハッシュテーブル}
\seclabel{hashtable}

\index{ChainedHashTable@#ChainedHashTable#}%
\ejindex{chaining}{ちぇいんほう@チェイン法}%
\ejindex{hashing with chaining}{ちぇいんほうによるはっしんぐ@チェイン法によるハッシング}%
#ChainedHashTable#とは\emph{チェイン法}を使ってデータをリストの配列#t#に保存するデータ構造である。
整数#n#はすべてのリストにおける要素数の合計である。（\figref{chainedhashtable}を参照せよ。）
\codeimport{ods/ChainedHashTable.t.n}

\begin{figure}
   \begin{center}
     \includegraphics[width=\ScaleIfNeeded]{figs/chainedhashtable}
   \end{center}
   \caption{$#n#=14, #t.length#=16$である#ChainedHashTable#の例。この例では$#hash(x)#=6$である。}
   \figlabel{chainedhashtable}
\end{figure}
\ejindex{hash value}{はっしゅち@ハッシュ値}%
\index{hash(x)@#hash(x)#}%
データ#x#の\emph{ハッシュ値}#hash(x)#とは$\{0,\ldots,#t.length#-1\}$の中のある値である。
ハッシュ値が#i#であるデータはリスト#t[i]#に入れられる。
リストが長くなり過ぎないように、次の不変条件を保持する。
\[
    #n# \le #t.length#
\]
こうすると、リストの平均要素数は常に1以下（$#n#/#t.length# \le 1$）である。

ハッシュテーブルに要素#x#を追加するには配列#t#の大きさを増やす必要があるかどうかを確認し、必要があれば#t#を拡張する。
あとは#x#から$\{0,\ldots,#t.length#-1\}$内の整数であるハッシュ値#i#を計算し、#x#をリスト#t[i]#に追加すればよい。
\codeimport{ods/ChainedHashTable.add(x)}
配列を拡張するときは、#t#の大きさを二倍にし、元のテーブルに入っていた要素をすべて新しいテーブルに入れ直す。
これは#ArrayStack#のときと同じ戦略であり、あのときと同じ結果が適用できる。
すなわち、操作列についての拡張操作の償却実行時間は定数である。（必要ならページ\pageref{lem:arraystack-amortized}の\lemref{arraystack-amortized}を見直すこと。）

拡張のあとは#x#を#ChainedHashTable#リスト#t[hash(x)]#に追加すればよい。
\ref{chap:arrays}章や\ref{chap:linkedlists}章で説明したどのリストの実装を使っても、この操作は定数時間で可能である。

要素#x#をハッシュテーブルから削除するためには、リスト#t[hash(x)]#を#x#が見つかるまで辿ればよい。
\codeimport{ods/ChainedHashTable.remove(x)}
この実行時間は$#n#_{#i#}$をリスト#t[i]#の長さとするとき、$O(#n#_{#hash(x)#})$である。

ハッシュテーブルから要素#x#を見つけるのも同様である。
リスト#t[hash(x)]#を線形に探索すればよい。
\codeimport{ods/ChainedHashTable.find(x)}
これもリスト#t[hash(x)]#の長さに比例する時間がかかる。

ハッシュテーブルの性能はハッシュ関数の選択に大きく影響される。
良いハッシュ関数は要素を#t.length#個のリストに均等に分散する関数であり、このときの各リストの長さの期待値は$O(#n#/#t.length)# = O(1)$となる。
一方、よくないハッシュ関数はすべての要素を同じリストに追加してしまう。
すなわち、リスト#t[hash(x)]#の長さは#n#になってしまう。
次の小節ではよいハッシュ関数について説明する。

\subsection{乗算ハッシュ法}
\seclabel{multihash}

\ejindex{hashing!multiplicative}{はっしゅほう@ハッシュ法!じょうさん@乗算}%
\ejindex{multiplicative hashing}{じょうさんはっしゅほう@乗算ハッシュ法}%
乗算ハッシュ法は剰余算術（\secref{arrayqueue}で説明した）と整数の割り算からハッシュ値を計算する効率的な方法である。
$\ddiv$は割り算の商を求める演算子である。
形式的には任意の整数$a\ge 0$と$b\ge 1$について、$a\ddiv b = \lfloor a/b\rfloor$と定義される。

乗算ハッシュ法では、ある整数#d#（これは\emph{次数}と呼ばれる）について大きさ$2^{#d#}$であるハッシュテーブルを使う。
整数$#x#\in\{0,\ldots,2^{#w#}-1\}$のハッシュ値は次のように計算される。
\[
    #hash(x)# = ((#z#\cdot#x#) \bmod 2^{#w#}) \ddiv 2^{#w#-#d#}
\]
ここで#z#は$\{1,\ldots,2^{#w#}-1\}$のうちの奇数からランダムに選択される。
整数の演算は整数のビット数を$#w#$とするとき、$2^{#w#}$で勝手に剰余を取られることを利用すると、このハッシュ関数は非常に効率よく実現できる
\footnote{ほとんどのプログラミング言語ではこうなのだが、残念なことにRuby（やPythonなど）ではそうではない。#w#ビットの固定桁の演算の結果がビットに収まらなくなったときには、可変桁数の整数表現が使われるのである。}。
（\figref{multihashing}を参照せよ。）
さらに、$2^{#w#-#d#}$による整数の割り算は二進法で右側の$#w#-#d#$ビットを落とせば計算できる。（これはビットを右に$#w#-#d#$個だけシフトすればよく、実装は上の式よりも単純になる。）
\codeimport{ods/ChainedHashTable.hash(x)}

\begin{figure}
  \begin{center}
    \resizebox{.98\textwidth}{!}{
    \setlength{\arrayrulewidth}{1pt}
    \begin{tabular}{|lr@{}r|}\hline
    $2^#w#$ (4294967296)&            #1#&#00000000000000000000000000000000# \\
    #z# (4102541685)&                   &#11110100100001111101000101110101# \\
    #x# (42) &                          &#00000000000000000000000000101010# \\
    $#z#\cdot#x#$ &             #101000#&#00011110010010000101110100110010# \\
    $(#z#\cdot#x#)\bmod 2^{#w#}$ &      &#00011110010010000101110100110010# \\
    $((#z#\cdot#x#)\bmod 2^{#w#})\ddiv 2^{#w#-#d#}$ &&
                      \multicolumn{1}{@{}l|}{#00011110#} \\\hline
    \end{tabular}}
    \setlength{\arrayrulewidth}{.4pt}
  \end{center}
  \caption{$#w#=32$、$#d#=8$とした乗算ハッシュ法の操作}
  \figlabel{multihashing}
\end{figure}

次の補題は乗算ハッシュ法がうまくハッシュ値の衝突を避けることを示す。（証明はこの節の後半に回す。）

\begin{lem}\lemlabel{universal-hashing}
  #x#と#y#を$\{0,\ldots,2^{#w#}-1\}$内の任意の整数であって、$#x#\neq #y#$を満たすものとする。
  このとき$\Pr\{#hash(x)#=#hash(y)#\} \le 2/2^{#d#}$が成り立つ。
\end{lem}

\lemref{universal-hashing}より、#remove(x)#と#find(x)#の性能は簡単に解析できる。

\begin{lem}
任意のデータ#x#について、$#n#_{#x#}$を#x#がハッシュテーブルに現れる回数とするとき、リスト#t[hash(x)]#の長さの期待値は$#n#_{#x#} + 2$以下である。
\end{lem}

\begin{proof}
  $S$をハッシュテーブルに含まれる#x#ではない要素の集合とする。
  要素$#y#\in S$について、次の指示変数を定義する。
    \[ I_{#y#} = \left\{\begin{array}{ll}
       1 & \mbox{if $#hash(x)#=#hash(y)#$} \\
       0 & \mbox{otherwise}
       \end{array}\right.
    \]
  ここで、\lemref{universal-hashing}より、$\E[I_{#y#}] \le 2/2^{#d#}=2/#t.length#$である。
  リスト#t[hash(x)]#の長さの期待値は次のように求まる。
  \begin{eqnarray*}
   \E\left[#t[hash(x)].size()#\right] &=& \E\left[#n#_{#x#} + \sum_{#y#\in S} I_{#y#}\right] \\
    &=& #n#_{#x#} + \sum_{#y#\in S} \E [I_{#y#} ] \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#t.length# \\
    &\le& #n#_{#x#} + \sum_{#y#\in S} 2/#n# \\
    &\le& #n#_{#x#} + (#n#-#n#_{#x#})2/#n# \\
    &\le& #n#_{#x#} + 2
  \end{eqnarray*}
\end{proof}

続いて\lemref{universal-hashing}を証明する。
まずは整数論の定理からはじめる。
次の証明では$(b_r,\ldots,b_0)_2$と書いて、$\sum_{i=0}^r b_i2^i$を表す。ここで、$b_i$は0か1である。
すなわち、$(b_r,\ldots,b_0)_2$は二進表記で$b_r,\ldots,b_0$である整数のことである。
また、$\star$は値の不明な桁を表すとする。

\begin{lem}\lemlabel{hashing-mapping}
  $S$を$\{1,\ldots,2^{#w#}-1\}$内の奇数の集合とする。
  $q, i$は$S$の任意の要素とする。
  このとき、$#z#q\bmod 2^{#w#} = i$を満たす$#z#\in S$の要素が一意に存在する。
\end{lem}

\begin{proof}
  $#z#$を選ぶと$i$は決まるので、$#z#q\bmod 2^{#w#} = i$を満たす$#z#\in S$が一意に決まることを示せば良い。

  背理法で示す。
  整数#z#と#z'#が存在し$#z#>#z#'$であると仮定する。
  このとき、
  \[
     #z#q\bmod 2^{#w#} = #z#'q \bmod 2^{#w#} = i
  \]
  よって、
  \[
     (#z#-#z#')q\bmod 2^{#w#} = 0
  \]
  しかしこれはある整数$k$について次の式が成り立つことを意味する。
  \begin{equation}
    (#z#-#z#')q = k 2^{#w#} \eqlabel{factors}
  \end{equation}
  これを2進数として考えると
  \[
    (#z#-#z#')q = k\cdot(1,\underbrace{0,\ldots,0}_{#w#})_2
  \]
  なので、$(#z#-#z#')q$の末尾#w#桁はすべて0である。

  加えて、$q\neq 0$かつ$#z#-#z#'\neq 0$より$k\neq 0$である。
  $q$は奇数なのでこの二進表記の末尾桁は0ではない。
  \[
    q = (\star,\ldots,\star,1)_2
  \]
  $|#z#-#z#'| < 2^{#w#}$より、$#z#-#z#'$の末尾に連続して並ぶ0の個数は#w#未満である。
  \[
    #z#-#z#' = (\star,\ldots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2
  \]
  積$(#z#-#z#')q$の末尾に連続して並ぶ0の個数は#w#未満である。
  \[
   (#z#-#z#')q = (\star,\cdots,\star,1,\underbrace{0,\ldots,0}_{<#w#})_2
  \]
  以上より、$(#z#-#z#')q$は\myeqref{factors}を満たさず、矛盾する。
  背理法により\lemref{hashing-mapping}が満たされる。
\end{proof}

\lemref{hashing-mapping}から次の便利な事実がわかる。
#z#が$S$から一様な確率でランダムに選ばれるとき、#zt#は$S$上に一様分布する。
次の証明では#z#の最下位の1である桁を除いた、$#w#-1$桁のランダムなビットを考えるのがポイントだ。

\begin{proof}[Proof of \lemref{universal-hashing}]
条件$#hash(x)#=#hash(y)#$と「$#z# #x#\bmod2^{#w#}$の上位#d#ビットと$#z# #y#\bmod 2^{#w#}$の上位#d#ビットが等しい」は同値である。
この条件の必要条件は、$#z#(#x#-#y#)\bmod 2^{#w#}$の上位#d#ビットがすべて0である、またはすべて1であることである。
これは、$#zx#\bmod 2^{#w#} > #zy#\bmod 2^{#w#}$ならば次の条件である。
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} =
      (\underbrace{0,\ldots,0}_{#d#},\underbrace{\star,\ldots,\star}_{#w#-#d#})_2
      \eqlabel{all-zeros}
  \end{equation}
一方、$#zx#\bmod 2^{#w#} < #zy#\bmod 2^{#w#}$ならば次の条件である。
  \begin{equation}
      #z#(#x#-#y#)\bmod 2^{#w#} =
      (\underbrace{1,\ldots,1}_{#d#},\underbrace{\star,\ldots,\star}_{#w#-#d#})_2
      \eqlabel{all-ones}
  \end{equation}
よって、$#z#(#x#-#y#)\bmod 2^{#w#}$が\myeqref{all-zeros}か\myeqref{all-ones}のどちらかであることを示せばよい。

  $q$を、ある整数$r\ge 0$が存在し、$(#x#-#y#)\bmod 2^{#w#}=q2^r$を満たす一意な奇数とする。
  \lemref{hashing-mapping}より、$#z#q\bmod 2^{#w#}$の二進表現は$#w#-1$桁のランダムなビットを持つ。（最下位桁は1である。）
  \[
   #z#q\bmod 2^{#w#}  = (\underbrace{b_{#w#-1},\ldots,b_{1}}_{#w#-1},1)_2
  \]
  よって$#z#(#x#-#y#)\bmod 2^{#w#}=#z#q2^r\bmod 2^{#w#}$は$#w#-r-1$桁のランダムなビットを持つ。（その後1が続き、さらに$r$個の0が続く。）
  \[
  #z#(#x#-#y#)\bmod 2^{#w#}  =
  #z#q2^{r}\bmod 2^{#w#} =
      (\underbrace{b_{#w#-r-1},\ldots,b_{1}}_{#w#-r-1},1,\underbrace{0,0,\ldots,0}_{r})_2
  \]
  これで証明が終わる。
  $r > #w#-#d#$ならば$#z#(#x#-#y#)\bmod 2^{#w#}$の上位#d#ビットは0と1を共に含む。よって$#z#(#x#-#y#)\bmod 2^{#w#}$が\myeqref{all-zeros}または\myeqref{all-ones}である確率は0である。
  $#r#=#w#-#d#$ならば\myeqref{all-zeros}の確率は0だが、\myeqref{all-ones}である確率は$1/2^{#d#-1}=2/2^{#d#}$である。
  （これは$b_1,\ldots,b_{d-1}=1,\ldots,1$である必要があるためだ。）
  $r < #w#-#d#$ならば$b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=0,\ldots,0$か、$b_{#w#-r-1},\ldots,b_{#w#-r-#d#}=1,\ldots,1$である。
  いずれの場合の確率も$1/2^{#d#}$であり、またそれぞれの事象は互いに排反である。
  よって、このどちらかである確率は$2/2^{#d#}$である。
\end{proof}

\subsection{要約}

次の定理は#ChainedHashTable#の性能をまとめたものだ。

\begin{thm}\thmlabel{hashtable}
  #ChainedHashTable#は#USet#インターフェースを実装する。
  #grow()#のコストを無視すると、#ChainedHashTable#における#add(x)#・#remove(x)#・#find(x)#の期待実行時間は$O(1)$である
  \footnote{訳注：これが、ハッシュテーブルが第2章の配列や第3章の連結リストよりも強力な理由である（利用要件によるが）。つまりハッシュ関数の性能さえ良ければ、追加も削除も検索もほぼ定数時間で行える。}。

  さらに、空の#ChainedHashTable#に対して、$m$個の#add(x)#・#remove(x)#からなる任意の操作列を順に実行するとき、#grow()#の呼び出しに要する償却実行時間は$O(m)$である。 % TODO: YJ 原文ではtotal of O(m) timeと書いてあるがconsistencyのため償却実行時間のほうがよいのでは
\end{thm}

\section{#LinearHashTable#：線形探索法}
\seclabel{linearhashtable}

\index{LinearHashTable@#LinearHashTable#}%
#ChainedHashTable#はリストの配列を使うデータ構造であった。
#i#番目のリストは$#hash(x)#=#i#$である#x#を全て格納していた。
これに対して\emph{オープンアドレス法}と呼ばれる方法は配列#t#に直接要素を収めるものである。
\ejindex{open addressing}{おーぷんあどれすほう@オープンアドレス法}%
このやり方はこの節で説明する#LinearHashTable#が採用しているものだ
\footnote{訳注：たとえば、Pythonの最も一般的な実装であるCPythonでは、オープンアドレス法でdictionaryが実装されている。}
。
文献によっては\emph{線形探索法によるオープンアドレス}と呼ばれることもある。
\ejindex{linear probing}{せんけいたんさくほう@線形探索法}%

#LinearHashTable#の背後にあるアイデアは#i=hash(x)#である要素#x#を理想的には#t[i]#に入れたい、というものだ。
もしこれが（他の要素が既にそこに入っていて）ムリなら、$#t#[(#i#+1)\bmod#t.length#]$に要素を入れてみる。
これもムリなら$#t#[(#i#+2)\bmod#t.length#]$に入れてみる。
これを#x#が入れる場所が見つかるまで繰り返す。

#t#の値は次の三種類のいずれかだ。
\begin{enumerate}
  \item データの値：#USet#に入っている実際の値である。
  \item #null#：データが入っていないことを示す。
  \item #del#：データが入っていたがそれが削除されたことを示す。
\end{enumerate}
#LinearHashTable#の要素数を数えるカウンタ#n#%に加えて、上の一つ目と三つ目の個数の合計を数えるカウンタ#q#を用意する。
に加えて、データ値の個数と#del#の個数の合計を数えるカウンタ#q#を用意する。
#q#の値は#n#に#del#の個数を加えた値である。
これが効率的であるためには#t#が#q#より十分大きい必要がある。このとき、#t#には#null#である場所がたくさんある。
よって#LinearHashTable#の操作は不変条件$#t.length#\ge 2#q#$を常に満たすようにする。

整理すると、#LinearHashTable#は要素の配列#t#に加え、整数#n#、#q#を持つ。#n#はデータ値の個数、#q#は#null#でない値の個数を保持する。
さらに、ハッシュ関数の値域の大きさは2の冪に制限されていることが多いので、整数不変条件$#t.length#=2^#d#$を満たす整数#d#も持ち回る。
\codeimport{ods/LinearHashTable.t.n.q.d}

#LinearHashTable#の#find(x)#操作は単純である。
$#i#=#hash(x)#$として、#t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と順に、#t[i']=x#または#t[i']=null#を満たす添え字#i'#を探す。
#t[i']=x#のとき、#x#が見つかったとして#t[i']#を返す。
#t[i']=null#のとき、#x#はハッシュテーブルに含まれないとして#null#を返す。
\codeimport{ods/LinearHashTable.find(x)}

#LinearHashTable#の#add(x)#操作も簡単に実装できる。
#find(x)#を使えば#x#が入っているかどうか確認できる。
#x#が入っていなければ#t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と順に探し、#null#か#del#を見つけたらそこを#x#に書き換え、#n#と#q#をひとつずつ増やす。
\codeimport{ods/LinearHashTable.add(x)}

ここまでで#remove(x)#の実装も明らかだろう。
#t[i]#, $#t#[(#i#+1)\bmod #t.length#]$, $#t#[(#i#+2)\bmod #t.length#]$, ...と#t[i']=x#または#t[i']=null#である添え字#i'#を見つけるまで探す。
#t[i']=x#ならば#t[i']=del#とし#true#を返す。
#t[i']=null#ならば#x#はテーブルに入っていなかった（そのため削除できない）として#false#を返す。
\codeimport{ods/LinearHashTable.remove(x)}

#find(x)#・#add(x)#・#remove(x)#の正しさは簡単に確認できる。
ただし、これは#del#を使うことに依存している。
これらの操作は#null#でない値を#null#に書き換えないことに注意する。
そのため#t[i']=null#である添え字#i'#を見つけると、#x#は配列に入っていないことがわかる。
#t[i']#はずっと#null#であったといえるので、先立って、すなわち#i'#よりも先の添字に#add(x)#が要素を追加していることはないのである。


#null#でないエントリの数が$#t.length#/2$より大きいときに#add(x)#を呼ぶとき、またはデータの入っているエントリ数が#t.length/8#よりも小さいときに#remove(x)#を呼ぶと#resize()#が呼ばれる。
#resize()#は他の配列を使ったデータ構造の場合と同様に働く。
まず$2^{#d#} \ge 3#n#$を満たす最小の非負整数#d#を見つける。
大きさ$2^{#d#}$の配列#t#を割当て、古い配列の要素を全て移し替える。
この処理の過程で、#q#を#n#に等しくリセットする。
これは新しい配列#t#は#del#を含まないためである。
\codeimport{ods/LinearHashTable.resize()}

\subsection{線形探索法の解析}

#add(x)#・#remove(x)#・#find(x)#のいずれも#null#であるエントリを見つけると（あるいはその前に）終了する。
解析の直感的な説明としては、配列#t#の半分以上は#null#なので、線形探索法はすぐに#null#のエントリを見つけて処理を終えるはずである。
しかしこの直感を当てにしすぎてはいけない。
例えば#t#のエントリを平均的には２つだけ見れば良さそうだが、実はこれは正しくない。
この節ではハッシュ値は$\{0,\ldots,#t.length#-1\}$から一様な確率分布に従う独立な値であると仮定する。
これは現実的な仮定ではないが、これを仮定すれば線形探索法の解析が可能になる。
この節の後半でTabulation Hashingという、線形探索法の用途には「十分よい」ハッシュ法を説明する。
もうひとつの仮定として、#t#の添字はすべて#t.length#で剰余を取っているとする。
つまり単に#t[i]#と書いても$#t#[#i#\bmod#t.length#]$のことである。

\ejindex{run}{れんぞく@連続}%
%XXX: これ、 i == 0 と i == t.length - 1 のときはどうなの？
%XXX: runの訳語 % YJ 連続かなあ。technical termとして本文中で他に連続は使われていない。
\emph{#i#から始まる長さ$k$の連続 (run)}が発生するとはテーブルのエントリ$#t[i]#, #t[i+1]#,\ldots,#t#[#i#+k-1]$がいずれも#null#でなく、$#t#[#i#-1]=#t#[#i#+k]=#null#$であることをいう。
#t#の#null#でない要素の数は#q#で、#add(x)#は常に$#q#\le#t.length#/2$であることを保証する。
直前の#resize()#以後、#t#に挿入された#q#個の要素を$#x#_1,\ldots,#x#_{#q#}$とする。
仮定より、ハッシュ値$#hash#(#x#_j)$はいずれも一様分布に従う互いに独立な確率変数である。
ここまでの準備で線形探索法の解析における主要な補題を示せる。

\begin{lem}\lemlabel{linear-probing}
#i#を$\{0,\ldots,#t.length#-1\}$のある要素に固定する。
このときある定数$c(0<c<1)$が存在して、#i#から始まる長さ$k$の連続が発生する確率を$O(c^k)$と表せる。
\end{lem}

\begin{proof}
#i#から始まる長さ$k$の連続が発生するとき、相異なる$k$個の要素$#x#_j$が存在し、$#hash#(#x#_j)\in\{#i#,\ldots,#i#+k-1\}$を満たす。
この事象の発生確率は次のように計算できる。
\[
  p_k  = \binom{#q#}{k}\left(\frac{k}{#t.length#}\right)^k\left(\frac{#t.length#-k}{#t.length#}\right)^{#q#-k}
\]
これは$k$個の要素の選び方によらず、これら$k$個の要素はいずれも$k$箇所のうちのいずれかに、そして残りの$#q#-k$個の要素は残りの$#t.length#-k$箇所に割り振られなければならないためだ。
\footnote{$p_k$は#i#から始まる長さ$k$の連続が発生する確率よりも大きいことに注意する。これは$p_k$は必要条件$#t#[#i#-1]=#t#[#i#+k]=#null#$を要求しないためである。}

次の導出ではすこしズルをしている。
$r!$を$(r/e)^r$に置き換える部分である。
スターリング近似（\secref{factorials}）からこれは真の値との差は$O(\sqrt{r})$程度だとわかる。
これを許すと導出が簡単になるのである。
\excref{linear-probing}ではスターリング近似を使ったより厳密な計算を読者にやってもらう予定だ。

#t.length#が最小値を取るとき$p_k$は最大値を取る。
またデータ構造は不変条件$#t.length# \ge 2#q#$を保つ。
よって次の式が成り立つ。
% XXX: TALK これ、一行目の不等号で ((t.length-k)/(t.length))^{q-k} <= ((2q-k)/(2q))^{q-k} を仮定してる気がするけど、必ずしも成り立たなくないか？
\begin{align*}
   p_k & \le \binom{#q#}{k}\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
  & = \left(\frac{#q#!}{(#q#-k)!k!}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
  & \approx \left(\frac{#q#^{#q#}}{(#q#-k)^{#q#-k}k^k}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} && \text{[Stirling's approximation]} \\
  & = \left(\frac{#q#^{k}#q#^{#q#-k}}{(#q#-k)^{#q#-k}k^k}\right)\left(\frac{k}{2#q#}\right)^k\left(\frac{2#q#-k}{2#q#}\right)^{#q#-k} \\
 & = \left(\frac{#q#k}{2#q#k}\right)^k
     \left(\frac{#q#(2#q#-k)}{2#q#(#q#-k)}\right)^{#q#-k} \\
 & = \left(\frac{1}{2}\right)^k
     \left(\frac{(2#q#-k)}{2(#q#-k)}\right)^{#q#-k} \\
 & = \left(\frac{1}{2}\right)^k
     \left(1+\frac{k}{2(#q#-k)}\right)^{#q#-k} \\
 & \le \left(\frac{\sqrt{e}}{2}\right)^k
\end{align*}
最後の変形では$x>0$ならば$(1+1/x)^x \le e$であることを利用した。
ここで、$\sqrt{e}/{2}< 0.824360636 < 1$なので、補題が示された。
\end{proof}

\lemref{linear-probing}を使えば#find(x)#・#add(x)#・#remove(x)#の期待実行時間の上界は直接的に計算できる。
まずは最もシンプルな#find(x)#を呼ぶが#x#が#LinearHashTable#に入っていないときを考える。
この場合$#i#=#hash(x)#$は$\{0,\ldots,#t.length#-1\}$の値を取り、#t#の中身と独立な確率変数である。
#i#が長さ$k$の連続の一部なら、#find(x)#の実行時間は$O(1+k)$以下である。
よって実行時間の期待値の上界を計算できる。
% XXX: これ無限級数の各項が1以上（＝kをt.lengthまでの有限級数で止めなくても上界の計算だから問題ない）という保証はどうやって確保してるの？
\[
  O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k\Pr\{\text{#i# is part of a run of length $k$}\}\right)
\]
内側の和を取っている長さ$k$の連続はk回カウントされているので、これをまとめて$k^2$とすれば、上の和は次のように変形できる。
\begin{align*}
  & { } O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k^2\Pr\{\mbox{#i# starts a run of length $k$}\}\right) \\
  & \le O\left(1 + \left(\frac{1}{#t.length#}\right)\sum_{i=1}^{#t.length#}\sum_{k=0}^{\infty} k^2p_k\right) \\
  & = O\left(1 + \sum_{k=0}^{\infty} k^2p_k\right) \\
  & = O\left(1 + \sum_{k=0}^{\infty} k^2\cdot O(c^k)\right) \\
  & = O(1)
\end{align*}
最後の変形$\sum_{k=0}^{\infty} k^2\cdot O(c^k)$では指数級数の性質を使っている
\footnote{解析学の教科書ではこの和は比を計算して求める。すなわち、ある正の数$k_0$が存在し、任意の$k\ge k_0$について、$\frac{(k+1)^2c^{k+1}}{k^2c^k} < 1$を満たす。}。
以上より、#LinearHashTable#に入っていない#x#について、#find(x)#の期待実行時間は$O(1)$である。

#resize()#のコストを無視していいなら、この解析で#LinearHashTable#の解析を終わりだ。

まず上の#find(x)#の解析は、#add(x)#において#x#がテーブルに含まれないときにもそのまま適用できる。
#x#がテーブルに含まれるときの#find(x)#の解析は#add(x)#によって#x#を加えたときのコストと同じである。
最後に、#remove(x)#のコストも#find(x)#のコストと同じだ。

まとめると、#resize()#のコストを無視すれば、#LinearHashTable#の操作の期待実行時間はいずれも$O(1)$である。
リサイズのコストを考える場合は、\secref{arraystack}で#ArrayStack#の償却解析を行ったのと同様である。

\subsection{要約}

次の定理は#LinearHashTable#の性能をまとめたものだ。

\begin{thm}\thmlabel{linear-probing}
  #LinearHashTable#は#USet#インターフェースを実装する。
  #resize()#のコストを無視すると、#LinearHashTable#における#add(x)#・#remove(x)#・#find(x)#の期待実行時間は$O(1)$である。

  さらに、空の#LinearHashTable#に対して、$m$個の#add(x)#・#remove(x)#からなる操作の列を順に実行するとき、#resize()#にかかる時間の合計は$O(m)$である。
\end{thm}

\subsection{Tabulation Hashing}
\seclabel{tabulation}

\index{tabulation hashing}%
#LinearHashTable#の解析では強い仮定を置いていた。
すなわち、任意の相異なる要素$\{#x#_1,\ldots,#x#_#n#\}$についてそのハッシュ値$#hash#($x$_1),\ldots,#hash#(#x#_#n#)$が独立に一様な確率で$\{0,\ldots,#t.length#-1\}$内に分布するという仮定である。
これを実現するひとつのやり方は大きさ$2^{#w#}$の巨大な配列#tab#を準備し、すべてのエントリを互いに独立な#w#-bitの整数乱数で初期化することだ。
このとき、#hash(x)#は#tab[x.hashCode()]#から#d#ビットを整数として取り出せばよい。
\codeimport{ods/LinearHashTable.idealHash(x)}
\pcodeonly{ここで、#>>#は\emph{右シフト}演算子である。そのため、#x.hashCode() >> w-d#は#x#の#w#ビットのハッシュ値の、上位#d#ビットを抽出する。}

あいにく大きさ$2^{#w#}$の配列はメモリ使用量の観点から現実的でない。
\emph{Tabulation Hashing}では#w#ビットの整数の代わりに$#w#/#r#$個の$#r#$ビット整数で妥協する。
こうすれば大きさ$2^{#r#}$の配列$#w#/#r#$個で済むのである。
これらの配列に入っている整数はいずれも互いに独立な#w#ビットの乱数である。
#hash(x)#を計算するために、#x.hashCode()#を$#w#/#r#$個の#r#ビット整数に分け、それぞれを配列の添え字として使う。
その後各配列の値をビット単位の排他的論理和を計算し、この結果を#hash(x)#とする。
次のコードは$#w#=32, #r#=4$の場合のものである。
\codeimport{ods/LinearHashTable.hash(x)}
この場合、#tab#は4つの列と$2^{32/4}=256$の行からなる二次元配列である。
\pcodeonly{
上の#0xff#は\emph{16進数}である。
\ejindex{hexadecimal numbers}{16しんすう@16進数}%
16進数の各桁は16種類の値（0から9に加えて、それぞれ10から15を表すa, b, c, d, e, f）のうちのいずれかである。
例えば、$#0xff#=15\cdot 16 + 15 = 255$である。
#&#は\emph{ビット単位論理積}演算子である。
そのため、#h >> 8 & 0xff#は#h#のビットのうち、添字が8から15であるものを抽出する。}

任意の#x#について#hash(x)#は$\{0,\ldots,2^{#d#}-1\}$の値を一様な確率で取ることを簡単に確認できる。
少し計算すればハッシュ値のペアが互いに独立であることも確認できる。
これは#ChainedHashTable#における乗算ハッシュ法の代わりにTabulation Hashingを使えることを意味する。

残念ながら、相異なる任意の#n#個の値の組について、そのハッシュ値が互いに独立というわけではない。
しかしそうであっても、Tabulation Hashingは\thmref{linear-probing}で示した性質を保証するのに十分よいハッシュ法である。
この話題についてはこの章の終わりで参考文献を紹介する。

\section{ハッシュ値}
XXX: ハッシュ値とHash Codeは区別したほうがよいか?

\ejindex{hash code}{はっしゅち@ハッシュ値}%
前節のハッシュテーブルではデータに対して#w#ビットの整数を対応させていた。
しかしキーが整数でないことはよくある。
例えば文字列・オブジェクト・配列や他の複合データ型である。
こういうデータにもハッシュテーブルを使うにはこれらの型から#w#ビットのハッシュ値を計算すればよい。
このハッシュ値が満たすべき性質は次のものだ。

\begin{enumerate}
  \item #x#と#y#が等しいとき、#x.hashCode()#と#y.hashCode()#は等しい。

  \item #x#と#y#が等しくないとき、$#x.hashCode()#=#y.hashCode()#$である確率は小さい。（$1/2^{#w#}$に近いということだ。）
\end{enumerate}

一つ目の性質は、#x#をハッシュテーブルに入れたあと、#x#と等しい#y#を検索すると#x#がちゃんと見つかることを保証する。
二つ目の性質は、オブジェクトを整数に変換する際のロスを小さくするものだ。
これは相異なるふたつの要素はハッシュテーブルの違う場所に入ることが多いことを保証する。

\subsection{プリミティブな型のハッシュ値}

\ejindex{hash code!for primitive data}{はっしゅち@ハッシュ値!ぷりみてぃぶがた@プリミティブ型}%
#char#・#byte#・#int#・#float#などの小さいプリミティブな型のハッシュ値は簡単に計算できる。
これらの方はバイナリ表現があり、これは#w#ビット以下である。
\javaonly{(たとえばJavaでは#byte#は8ビット型であり、#float#は32ビット型である。)}\cpponly{(たとえばC++では#char#はたいてい8ビット型であり、#float#は32ビット型である。)}
このビット列を$\{0,\ldots,2^#w#-1\}$の範囲の整数であると解釈すればよい。
そうすれば、ふたつの異なる値は異なるハッシュ値を持つ。
また、ふたつの同じ値は同じハッシュ値を持つ。


#w#ビットよりも多くのビットを持つプリミティブ型は少ない。
ふつうある整数$c$が存在し、$c#w#$ビットである。
（Javaの#long#・#double#型は$c=2$である例である。）
これらのデータ型は$c$個のオブジェクトの複合型と考えられる。
この扱いは次の小節に譲る。

\subsection{複合オブジェクトのハッシュ値}
\seclabel{stringhash}

\ejindex{hash code!for compound objects}{はっしゅち@ハッシュ値!ふくごうおぶじぇくと@複合オブジェクト}%
複合オブジェクトのハッシュ値は、その構成要素のハッシュ値を組み合わせて計算する。
これは思うほど簡単でない。
いい感じのやり方はたくさん思いつく（例えばビット単位の排他的論理和を計算する）が、そのうちの多くはうまくいかない。
（\ref{exc:hash-hack-first}から\ref{exc:hash-hack-last}を参照せよ。）
しかし$2#w#$ビットの算術精度があれば単純でロバストな方法がある。
$P_0,\ldots,P_{r-1}$からなる複合オブジェクトがあり、それぞれのハッシュ値は$#x#_0,\ldots,#x#_{r-1}$であるとする。
このとき互いに独立な#w#ビットの乱数$#z#_0,\ldots,#z#_{r-1}$と、$2#w#$ビットのランダムな奇数#z#から、複合オブジェクトのハッシュ値を計算できる。
\[
   h(#x#_0,\ldots,#x#_{r-1}) =
   \left(\left(#z#\sum_{i=0}^{r-1} #z#_i #x#_i\right)\bmod 2^{2#w#}\right)
   \ddiv 2^{#w#}
\]
このハッシュ値の計算過程は最後に#z#をかけ、$2^{#w#}$で割っていることに注目してほしい。
これは$2#w#$ビットの中間結果に\secref{multihash}で紹介した乗算ハッシュ法を使って#w#ビットの最終結果を得ている。
#x0#・#x1#・#x2#の3つの構成要素からなる複合オブジェクトの場合の例を示す。 %  YJ 複合オブジェクトの各fieldを構成要素と訳した
\javaimport{junk/Point3D.x0.hashCode()}
\cppimport{ods/Point3D.hashCode()}
\pcodeimport{ods/Point3d.hashCode()}
実装が単純なだけでなく、次の定理はこの方法が良い性質を持つことを示す。

\begin{thm}\thmlabel{multihash}
$#x#_0,\ldots,#x#_{r-1}$と$#y#_0,\ldots,#y#_{r-1}$はいずれも、$\{0,\ldots,2^{#w#}-1\}$の要素である#w#ビットの整数からなる列とする。
さらに、少なくとも一箇所の添え字$i\in\{0,\ldots,r-1\}$で$#x#_i \neq #y#_i$が成り立つと仮定する。
このとき、次が成り立つ。
\[
   \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \}
        \le 3/2^{#w#}
\]
\end{thm}

\begin{proof}
最後の乗算ハッシュ法については後半に考える。
次の関数を定義する。
  \[
    h'(#x#_0,\ldots,#x#_{r-1}) =
       \left(\sum_{j=0}^{r-1} #z#_j #x#_j\right)\bmod 2^{2#w#}
  \]
  $h'(#x#_0,\ldots,#x#_{r-1}) =  h'(#y#_0,\ldots,#y#_{r-1})$であるとする。
  これは次のように書き直せる。
  \begin{equation}  \eqlabel{bighash-x}
      #z#_i(#x#_i-#y#_i) \bmod 2^{2#w#} = t
  \end{equation}
  ここで$t$は次のものである。
  \[
     t = \left(\sum_{j=0}^{i-1} #z#_j(#y#_j-#x#_j) + \sum_{j=i+1}^{r-1} #z#_j(#y#_j-#x#_j)\right) \bmod 2^{2#w#}
  \]
  $#x#_i> #y#_i$と仮定しても一般性を失わない。
  すると\myeqref{bighash-x}は次のようになる。
  \begin{equation}
      #z#_i(#x#_i-#y#_i) = t \eqlabel{bighash-xx}
  \end{equation}
  これは$#z#_i$と$(#x#_i-#y#_i)$はいずれも$2^{#w#}-1$以下なので、これらの積は$2^{2#w#}-2^{#w#+1}+1 < 2^{2#w#}-1$以下であるためである。
  仮定より$#x#_i-#y#_i\neq 0$なので、\myeqref{bighash-xx}は$#z#_i$について高々ひとつの解を持つ。
  $#z#_i$と$t$は互いに独立（$#z#_0,\ldots,#z#_{r-1}$は互いに独立である）なので、$h'(#x#_0,\ldots,#x#_{r-1})=h'(#y#_0,\ldots,#y#_{r-1})$を満たす$#z#_i$を選ぶ確率は$1/2^{#w#}$以下である。

  最後の処理は乗算ハッシュ法であり、$2#w#$ビットの中間結果$h'(#x#_0,\ldots,#x#_{r-1})$を#w#ビットの最終結果$h(#x#_0,\ldots,#x#_{r-1})$に縮める。
  \thmref{multihash}より、$h'(#x#_0,\ldots,#x#_{r-1})\neq h'(#y#_0,\ldots,#y#_{r-1})$ならば$\Pr\{h(#x#_0,\ldots,#x#_{r-1}) = h(#y#_0,\ldots,#y#_{r-1})\} \le 2/2^{#w#}$である。
  % TALK なぜここで証明中の定理を使っている？？定理の仮定を使うということ？？？
  以上より、次の式が成り立つ。
  \begin{align*}
    & \Pr\left\{\begin{array}{l}
          h(#x#_0,\ldots,#x#_{r-1}) \\
          \quad = h(#y#_0,\ldots,#y#_{r-1})\end{array}\right\} \\
      &= \Pr\left\{\begin{array}{ll}
            \mbox{$h'(#x#_0,\ldots,#x#_{r-1}) = h'(#y#_0,\ldots,#y#_{r-1})$ or} \\
            \mbox{$[h'(#x#_0,\ldots,#x#_{r-1}) \neq h'(#y#_0,\ldots,#y#_{r-1})$} \\
                  \mbox{\quad and } \\ \mbox{$#z#h'(#x#_0,\ldots,#x#_{r-1})\ddiv2^{#w#} = #z# h'(#y#_0,\ldots,#y#_{r-1})\ddiv 2^{#w#}]$}
          \end{array}\right\} \\
      &\le 1/2^{#w#} + 2/2^{#w#} = 3/2^{#w#} \enspace . \qedhere
  \end{align*}
\end{proof}
% 上の式、andがどこにかかっているのかを明示するために角括弧 [ ] を追加した

\ejindex{hash code!for strings}{はっしゅち@ハッシュ値!もじれつ@文字列}%
\ejindex{hash code!for arrays}{はっしゅち@ハッシュ値!はいれつ@配列}%
\subsection{配列と文字列のハッシュ値}
\seclabel{polyhash}

前小節の手法はオブジェクトが固定数の構成要素からなるときにはうまくいく。
しかし、可変長のオブジェクトをうまく扱えない。
なぜなら#w#ビットの乱数$#z#_i$を構成要素の数だけ使う必要があるためである。

必要なだけ$#z#_i$を生成するためには擬似乱数列を使えるが、その場合$#z#_i$は互いに独立ではなく、擬似乱数がハッシュ関数に対して悪影響を及ぼさないことを証明するのは難しい。
例えば\thmref{multihash}の証明における$t$と$#z#_i$の独立性は成り立たなくなる。

\ejindex{prime field}{そすうたい@素数体}%
ここでは素数体上の多項式を使ったハッシュ法を使う。
これは正規多項式の値を計算し、素数#p#による剰余を取るものだ。
次の定理は素数体上の多項式が普通の多項式と似た振る舞いをすることを主張する。

\begin{thm}\thmlabel{prime-polynomial}
% XXX: non-trivial polynomial の正確な定義は?
% リンクの証明によると非自明はf(x)=0でないこと。 http://eiche.theoinf.tu-ilmenau.de/fingerprint/ueber_polynome_modulo-englisch.pdf

 $#p#$を素数、$f(#z#) = #x#_0#z#^0 + #x#_1#z#^1 + \cdots + #x#_{r-1}#z#^{r-1}$を$#x#_i\in\{0,\ldots,#p#-1\}$を係数とする非自明($x_0=x_1=...=x_{r-1}=0$でない)な多項式とする。
 このとき、等式$f(#z#)\bmod #p# = 0$は$#z#\in\{0,\ldots,p-1\}$の範囲に高々$r-1$個の解を持つ。
\end{thm}

\thmref{prime-polynomial}より、$#z#\in\{0,\ldots,#p#-1\}$を使えば、$#x#_i\in \{0,\ldots,#p#-2\}$である整数の列$#x#_0,\ldots,#x#_{r-1}$のハッシュ値を計算できる。
\[
   h(#x#_0,\ldots,#x#_{r-1})
    = \left(#x#_0#z#^0+\cdots+#x#_{r-1}#z#^{r-1}+(#p#-1)#z#^r \right)\bmod #p#
\]

最後に追加された項$(#p#-1)#z#^r$に注意する。
$(#p#-1)$を整数列の末尾の要素として$#x#_0,\ldots,#x#_{r}$と考えると便利かもしれない。
この要素は整数列の要素のいずれとも異なる。
整数列の要素は$\{0,\ldots,#p#-2\}$の要素である。
$#p#-1$を列の終わりを示すマーカーだと考える。

次の定理はふたつの同じ長さの列について、#z#だけの小さなランダム性にも関わらず、良い出力を返すことを示すものだ。

\begin{thm}\thmlabel{stringhash-eqlen}
  pを$#p#>2^{#w#}+1$を満たす素数とする。
  $#x#_0,\ldots,#x#_{r-1}$と$#y#_0,\ldots,#y#_{r-1}$を$\{0,\ldots,2^{#w#}-1\}$の要素である#w#ビットの整数からなる列であるとする。
  $i\in\{0,\ldots,r-1\}$のうち少なくともひとつ$#x#_i \neq #y#_i$が成り立つと仮定する。
  このとき、次の式が成り立つ。
  \[
     \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \}
          \le (r-1)/#p#
  \]
\end{thm}

\begin{proof}
  等式$h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1})$は次のように変形できる。
  \begin{equation}  \eqlabel{strhash-eqlen}
    \left(
       (#x#_0-#y#_0)#z#^0+\cdots+(#x#_{r-1}-#y#_{r-1})#z#^{r-1}
    \right)\bmod #p# = 0
  \end{equation}
  $#x#_#i#\neq #y#_#i#$なので、この多項式は非自明である。
  よって\thmref{prime-polynomial}より#z#についての解は高々$r-1$個である。
  以上より、#z#を選んでこの解のうちの一つを引く確率は$(r-1)/#p#$以下である。
\end{proof}

このハッシュ関数はふたつの入力列の長さが異なる場合にも対応できる。
この場合、一方の入力列が他方の入力列を含んでいても構わない。
この関数は入力が無限列であってもそのまま問題なく処理できるのだ。
\[
  #x#_0,\ldots,#x#_{r-1}, #p#-1,0,0,\ldots
\]
長さ$r, r' (r > r')$のふたつの入力があるとき、ふたつの列は添え字$i=r$で異なる。
このとき\myeqref{strhash-eqlen}は次のようになる。
\[
  \left(
     \sum_{i=0}^{i=r'-1}(#x#_i-#y#_i)#z#^i + (#x#_{r'} - #p# + 1)#z#^{r'}
     +\sum_{i=r'+1}^{i=r-1}#x#_i#z#^i + (#p#-1)#z#^{r}
  \right)\bmod #p# = 0
\]
これは\thmref{prime-polynomial}より$#z#$について高々$r$個の解をもつ。
\thmref{stringhash-eqlen}と合わせると次のより一般的な定理が示せる。

\begin{thm}\thmlabel{stringhash}
  pを$#p#>2^{#w#}+1$を満たす素数とする。
  $#x#_0,\ldots,#x#_{r-1}$と$#y#_0,\ldots,#y#_{r'-1}$は$\{0,\ldots,2^{#w#}-1\}$の要素である#w#ビット整数からなる相異なる列であるとする。
  このとき次の式が成り立つ。
  \[
     \Pr\{ h(#x#_0,\ldots,#x#_{r-1}) =  h(#y#_0,\ldots,#y#_{r-1}) \}
          \le \max\{r,r'\}/#p#
  \]
\end{thm}

次のサンプルコードを見れば配列#x#を含むオブジェクトをこのハッシュ関数がどう扱うかがわかるだろう。
\javaimport{junk/GeomVector.hashCode()}
\cppimport{ods/GeomVector.hashCode()}
\pcodeimport{ods/GeomVector.hashCode()}

このコードは実装上の都合で衝突確率がやや大きくなっている。
特に#x[i].hashCode()#を31ビットに縮めるために\secref{multihash}で$#d#=31$とした乗算ハッシュ法を使っている。
これは素数$#p#=2^{32}-5$で剰余を取った足し算や掛け算を、符号なし63ビット整数で実行するためである。

よって、長い方の長さが$r$であるふたつの相異なる列のハッシュ値が一致する確率は次の値以下である。
\[
    2/2^{31} + r/(2^{32}-5)
\]
これは\thmref{stringhash}で求めた$r/(2^{32}-5)$よりも大きい。

\section{ディスカッションと練習問題}

ハッシュテーブルとハッシュ値は広大で活発な研究分野であり、この章ではほんのさわりを説明しただけである。
ハッシュ方のオンライン参考文献一覧は\cite{hashing}\ejindex{Bibliography on Hashing}{はっしゅほうのさんこうぶんけん@ハッシュ法の参考文献}2000近いエントリを含む。

ハッシュテーブルには他にも様々な実装がある。

\secref{hashtable}で説明したものはチェイン法\emph{hashing with chaining}である。\ejindex{hashing with chaining}{ちぇいんほうによるはっしゅほう@チェイン法によるハッシュ法}%
（各配列のエントリは要素のチェイン（#List#）である。）
チェイン法によるハッシュテーブルはIBMにてH. P. Luhnが1953年1月に出した内部報告書で提案された。この報告書は連結リストの最古の文献のうちの一つでもあると思われる。

\ejindex{open addressing}{おーぷんあどれすほう@オープンアドレス法}%
別の方法として、\emph{オープンアドレス法}がある。
これは全てのデータを配列に直接格納するものだ。
\secref{linearhashtable}で説明した#LinearHashTable#はこのやり方のうちのひとつである。
このアイデアもまた別のIBMのグループによって独立に1950年代に提案された。
オープンアドレス法は\emph{衝突の解決}のことを考えなければならない。
\ejindex{collision resolution}{しょうとつのかいけつ@衝突の解決}%
衝突とはふたつの値が配列の同じ位置に割当てられることだ。
このための方法にはいくつか種類がある。
それぞれ異なる性能保証があり、またこの章で説明したものよりも精巧なハッシュ関数を用いるものもある。

また別のハッシュテーブルの実装に関する話題としては、\emph{完全ハッシュ法}と呼ばれるものがある。
\ejindex{perfect hashing}{かんぜんはっしゅほう@完全ハッシュ法}%
これは#find(x)#の実行時間が最悪の場合でも$O(1)$になるハッシュ法だ。
データセットが静的な場合にはこれはデータセットに対する\emph{完全ハッシュ関数}を見つけることで実現できる。
\ejindex{perfect hash function}{かんぜんはっしゅかんすう@完全ハッシュ関数}%
\ejindex{hash function!perfect}{はっしゅかんすう@ハッシュ関数!かんぜん@完全}%
これはすべてのデータを別々の配列内の位置に対応させるハッシュ関数である。
データが動的な場合には完全ハッシュ法として\emph{FKS二段階ハッシュテーブル}
\ejindex{two-level hash table}{にだんかいはっしゅてーぶる@二段階ハッシュテーブル}%
\ejindex{hash table!two-level}{はっしゅてーぶる@ハッシュテーブル!にだんかい@二段階}%
\cite{fks84,dkkmrt94}
や\emph{cuckoo hashing} \cite{pr04}などが知られている。
\ejindex{cuckoo hashing}{かっこうはっしゅほう@カッコウハッシュ法}%
\ejindex{hash table!cuckoo}{はっしゅてーぶる@ハッシュテーブル!かっこう@カッコウ}%

この章で紹介したハッシュ関数は任意のデータセットに対してうまく動作することが証明できる既知の手法の中でおそらく最も実用的なものである。
別のよい方法として、CarterとWegmanによる先駆け的な研究成果であった\emph{ユニバーサルハッシュ法}を使ったものがある。いくつかのことなるシナリオのためのハッシュ関数が提案されている。\cite{cw79}
\ejindex{universal hashing}{ゆにばーさるはっしゅほう@ユニバーサルハッシュ法}%
\ejindex{hashing!universal}{はっしゅほう@ハッシュ法!ユニバーサル}%
\secref{tabulation}で説明したTabulation hashingは
CarterとWegmanの研究\cite{cw79}によるものだが、この手法を線形探索法（と他のいくつかのハッシュテーブルの実装）に適用した場合の解析はP\v{a}tra\c{s}cuとThorupの研究成果である。\cite{pt12}

\emph{乗算ハッシュ法}のアイデアは非常に古くからあり、
\ejindex{multiplicative hashing}{じょうさんはっしゅほう@乗算ハッシュ法}%
\ejindex{hashing!multiplicative}{はっしゅほう@ハッシュ法!じょうさん@乗算}%
おそらくこれはハッシュ法の伝承といってもよいだろう。\cite[Section~6.4]{k97v3}
しかし、\secref{multihash}で説明した乗数#z#をランダムな奇数から選ぶアイデアとその解析はDietzfelbingerらの研究成果である。\cite{dhkp97}
この乗算ハッシュ法は最もシンプルなもののうちの一つだが、衝突確率が$2/2^{#d#}$、つまり$2^{#w#}$から$2^{#d#}$への全ての関数からランダムに選出した場合（理想的な場合）とくらべて衝突確率が二倍になってしまう。
XXX: multiply-addの訳語 % YJ random generatorの文脈だとlinear congruential generatorが線形合同法と訳されるけどどうでしょ。あるいは乗算ハッシュと統一して、乗算合同ハッシュ法とか。　https://ja.wikipedia.org/wiki/%E7%B7%9A%E5%BD%A2%E5%90%88%E5%90%8C%E6%B3%95
\emph{multiply-add ハッシュ法}は次の関数を使う方法だ。
\ejindex{hashing!multiply-add}{はっしゅほう@ハッシュ法!multiply-add}%
\ejindex{multiply-add hashing}{multiply-addはっしゅほう@multiply-addハッシュ法}%
\[
   h(#x#) = ((#z##x# + b) \bmod 2^{#2w#}) \ddiv 2^{#2w#-#d#}
\]
ここで#z#と#b#いずれも$\{0,\ldots,2^{#2w#}-1\}$からランダムに選出される。
Multiply-addハッシュ法の衝突確率は$1/2^{#d#}$である。\cite{d96}
しかし、$2#w#$ビット精度の四則演算が必要である。

固定長の#w#ビットの整数列からハッシュ値を得る方法はたくさんある。
特に高速な方法は次のものだ。\cite{bhkkr99}
\[\begin{array}{l}
  h(#x#_0,\ldots,#x#_{r-1}) \\
   \quad = \left(\sum_{i=0}^{r/2-1} ((#x#_{2i}+#a#_{2i})\bmod 2^{#w#})((#x#_{2i+1}+#a#_{2i+1})\bmod 2^{#w#})\right) \bmod 2^{2#w#}
\end{array}
\]
ここで$r$は偶数であり、$#a#_0,\ldots,#a#_{r-1}$はいずれも$\{0,\ldots,2^{#w#}\}$からランダムに選出される。

この$2#w#$ビットのハッシュ値が衝突する確率は
$1/2^{#w#}$である。
これを乗算ハッシュ法（かMultiply-add）を使って#w#ビットに縮めることができる。
これは$r/2$回の$2#w#$ビット乗算だけで実現でき、これは高速である。
\secref{stringhash}の方法は$r$回の乗算が必要であった。
（$\bmod$の計算は#w#または$2#w#$ビットの足し算、掛け算では暗に実行される。）


\secref{polyhash}で説明した素数体を使った可変長配列のハッシュ法はDietzfelbinger \etal \cite{dgmp92}による。
この方法は$\bmod$を使うが、これは時間のかかる機械語命令であり、結果この方法は速くない。
剰余の法として$2^{#w#}-1$を使う工夫がある。
こうすると$\bmod$を加算とビット単位のand演算に置き換えられる。\cite[Section~3.6]{k97v2}
他の方法としては固定長の高速なハッシュ法を使って長さ$c>1$のブロックに対してハッシュ値を計算し、その結果の$\lceil r/c\rceil$個のハッシュ値の配列に素数体を使った方法でハッシュ値を求めるものがある。

\begin{exc}
  ある大学では生徒が初めて講義を履修するときに学生番号を発行する。
  この番号はひとつずつ増える整数で、何年も前に0から始まり、今では数百万になっている。
  百人の一年生が受講する講義にて、各生徒に学生番号から計算したハッシュ値を割り当てる。
  このとき下の二桁、あるいは上の二桁のどちらを使うのはいいアイデアだろうか?説明せよ。
\end{exc}

\begin{exc}
  \secref{multihash}の方法において、$#n#=2^{#d#}$かつ$#d#\le #w#/2$である場合を考える。
  \begin{enumerate}
    \item #z#によらず、持つ相異なるn個の入力であって、同じハッシュ値を持つものが存在することを示せ。（ヒント：これは簡単である。数論の知識などは必要ない。）
    \item #z#が与えられたとき、#n#個の同じハッシュ値を持つ値を求めよ。
	（これはより難しく、基本的な数論の知識が必要だ。）
  \end{enumerate}
\end{exc}

\begin{exc}
  \lemref{universal-hashing}で得た上界$2/2^{#d#}$はある意味で最適であることを示せ。 % TODO YJ 元の問題文は"$x=2^{#w#-#d#-2}$かつ$#y#=3#x#$のとき"の証明をすれば最適であることの証明になると言っているがこれは間違いでは。なので文を２つに分けるのはよいと思う。が、"ある意味"とはどういう意味？
  $x=2^{#w#-#d#-2}$かつ$#y#=3#x#$のとき、$\Pr\{#hash(x)#=#hash(y)#\}=2/2^{#d#}$であることを示せ。
  （ヒントとしては、$#zx#$と$#z#3#x#$の二進表記を考え、$#z#3#x# = #z#x#+2#z#x#$であることを利用せよ。）
\end{exc}

\begin{exc}\exclabel{linear-probing}
  \secref{factorials}で与えたスターリングの公式を使って、\lemref{linear-probing}を今度は誤魔化しなしで証明せよ。
\end{exc}

\begin{exc}
要素#x#を#LinearHashTable#に要素を追加するための簡略化された次のコードを見よ。
これは単純に#x#をはじめに見つけた#null#であるエントリに入れる。
このコードは非常に遅いことがあることを示せ。
すなわち、$O(#n#)$個の#add(x)#・#remove(x)#・#find(x)#からなる操作の列で$#n#^2$の実行時間がかかる例を挙げよ。
\codeimport{ods/LinearHashTable.addSlow(x)}
\end{exc}

\begin{exc}
昔のJavaでは#String#クラスの#hashCode()#メソッドは長い文字列の全ての文字を使ってはいなかった。
例えば16文字の文字列の場合は偶数番目の8文字だけを使っていた。
これがよくないアイデアであること、すなわち同じハッシュ値を持つ文字列がたくさん現れるような例を挙げよ。
\end{exc}

\begin{exc}\exclabel{hash-hack-first}
ふたつの#w#ビットの整数#x#と#y#からなるオブジェクトがあるとき、$#x#\oplus#y#$をハッシュ値とするのはよくないことを示せ。
すなわち、ハッシュ値が0となるようなオブジェクトの例をたくさん挙げよ。
\end{exc}

\begin{exc}
ふたつの#w#ビットの整数#x#と#y#からなるオブジェクトがあるとき、$#x#+#y#$をハッシュ値とするのはよくないことを示せ。
すなわち、同じハッシュ値を持つオブジェクトの集まりの例を挙げよ。
\end{exc}

\begin{exc}\exclabel{hash-hack-last}
ふたつの#w#ビットの整数#x#と#y#からなるオブジェクトがあるとする。
決定的な関数$h(#x#,#y#)$により#w#ビットの整数であるハッシュ値を計算するとする。
このときハッシュ値が一致するオブジェクトの集合であって、要素数の大きいものが存在することを示せ。
\end{exc}

\begin{exc}
  ある正の数#w#について、$p=2^{#w#}-1$であるとする。
  正の数$x$について次の式が成り立つ理由を説明せよ。
  \[
      (x\bmod 2^{#w#}) + (x\ddiv 2^{#w#}) \equiv x \bmod (2^{#w#}-1)
  \]
  （これは$x \bmod (2^{#w#}-1)$を計算するための方法として、
  \[
    #x = x&((1<<w)-1) + x>>w#
  \]
  を$#x# \le 2^{#w#}-1$を満たすまで繰り返すアルゴリズムを与えている。）
\end{exc}

\begin{exc}
標準ライブラリやこの本の#HashTable#・#LinearHashTable#を参考によく使われるハッシュテーブルの実装を見つけ、整数#x#に対して#find(x)#が線形時間で実行できるプログラムを実装せよ。
つまり、テーブルの中の同じ位置に対応付けられる#n#個の整数の集まりを見つけよ。

実装によって、単にコードを見れば十分だったり、あるいは試しに挿入・検索をしてみてその時間を測ってみたりする必要があるだろう。これはウェブサーバーへのDoS (denial of service)攻撃に使われることがある。\cite{cw03}
\index{algorithmic complexity attack}%
\end{exc}
