\chapter{Introduction}
\pagenumbering{arabic}

%世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。
データ構造とアルゴリズムに関するコースは世界の全てのコンピュータサイエンスカリキュラムに含まれている。データ構造はそれほどに重要だ。生活の質を上げるだけでなく、毎日のように人の命さえ救っている。データ構造によって数百万ドル、数十億ドルの規模にまでなった企業も多い。

% どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
% How can this be (important)? では?
なぜデータ構造はこんなにも重要なのだろう？立ち止まって考えてみると、私達は普段からデータ構造と接している。

\begin{itemize}
	\item ファイルを開く：ファイルシステム\ejindex{file system}{ふぁいるしすてむ@ファイルシステム}のデータ構造を使って、ファイルをハードディスクなどの上に配置し、検索できる。これは簡単ではない。ハードディスクには数億ものブロックがある。ファイルの内容はそのどこかに保存されるのだ。
	\item 連絡先を検索する：ダイヤル途中の部分的な情報にもとづき、連絡先リスト\ejindex{contact list}{れんらくさきりすと@連絡先リスト}から電話番号を見つけるためにデータ構造が使われる。これは簡単ではない。連絡先リストに含まれる情報はとても多いかもしれない。これまで電話や電子メールで連絡したことのある全ての人を想像してみてほしい。また、電話のプロセッサはあまり高性能でなく、メモリも潤沢でない。
	\item SNSにログインする：\ejindex{social network}{そーしゃるねっとわーく@ソーシャルネットワーク}ネットワークサーバーは、ログイン情報からアカウント情報を検索する。これは簡単ではない。人気のソーシャルネットワークには何億人ものアクティブユーザーがいる。
	\item Webページを検索する：\ejindex{web search}{うぇぶけんさく@ウェブ検索}検索エンジンは検索語からWebページを見つけるためにデータ構造を使う。これは簡単ではない。インターネットには85億以上のWebページがあり、それぞれのページに検索されるかもしれない単語が散りばめられている。
	\item 緊急サービス（9-1-1）に電話する：\ejindex{emergency services}{きんきゅうさーびす@緊急サービス}\index{9-1-1}緊急サービスネットワークはパトカー・救急車・消防車が速やかに現場に到着できるよう、電話番号と住所を対応づけるためにデータ構造を使う。これは重要だ。電話をかけた人は正確な住所を伝えられないかもしれず、この場面での遅れは生死を別つかもしれない。
\end{itemize}

\section{効率の必要性}

次の節ではよく使われるデータ構造にどんな操作ができるのかを見ていく。ちょっとしたプログラミング経験があれば、正しい結果を返すようにこれらの操作を実装することは難しくない。データを配列や連結リストに入れておいて、すべての要素を見てまわりながら、要素を追加したり削除したりすればよいのだ。

この実装は簡単だが効率がよくない。さて、このことを考える価値はあるだろうか？コンピュータはどんどん速くなっている。かんたんな実装で十分かもしれない。確認のためにざっくりと計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）個のアイテムを持つアプリケーションがあるとする。各アイテムを少なくとも一回は見たくなるという仮定はそれなりに妥当だろう。この場合、少なくとも100万（$10^6$）回このデータセットからあるアイテムを探すことになる。この$10^6$回のデータ探しで、毎回$10^6$個のアイテムをすべて確認すると、合計$10^6\times 10^6=10^{12}$（1兆）回データを読み出すことになる。

\paragraph{プロセッサの速度：}
本書の執筆の時点で、かなり高速なデスクトップコンピュータでも毎秒10億（$10^9$）回以上の操作は実行できない。\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作に普通は数サイクルが必要だ。}よってこのアプリケーションの完了には少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピューターにとっては16分は非常に長い時間だが、人はそのくらいの時間なら待っていられるだろう。（例えばコーヒーブレイクに向かうならそれで構わないだろう。）

\paragraph{大きなデータセット：}
Google\index{Google}を考えてみよう。Googleでは85億ものWebページからの検索を扱う。先ほどの計算では、このデータに対する問い合わせには少なくとも8.5秒かかる。だが実際にはそんなに時間がかからないことを知っているだろう。Web検索には8.5秒もかからないし、あるページがインデックスに含まれているかよりさらに複雑な問い合わせも実行する。執筆している時点でGoogleは1秒間に約$4,500$クエリを受けつける。つまり少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要なのだ。

\paragraph{解決策：}
以上の例から、アイテムの数#n#と実行される操作数$m$が共に大きくなると、データ構造の自明な実装では性能が追いつかなくなってくることがわかる。今の例で、（機械命令数で数えた）時間はおよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを上手に並べ、各操作のたびにすべてのデータを見て回らずにすますことだ。一見ムリそうに思うかもしれないが、データがどれだけあったとしても平均して2つのデータだけを参照すれば、探していたデータが見つかるデータ構造をのちに紹介する。1秒あたり10億命令を実行できるとして、10億個のデータ（兆、京、垓であっても）を含むデータ構造からデータを検索するのに、わずか$0.000000002$秒しかかからないのだ。

データの個数が増えても、実際に見て回る必要のあるデータの個数は非常にゆっくりとしか増えないデータ構造も紹介する。このデータ構造ではデータを整列しておく。例えば10億個のアイテムを整列しておけば、最大60個のアイテムを参照することでデータを見つけたり、更新したり、消したりできる。毎秒10億回の命令を実行できるコンピュータでは、これらの操作はほんの$0.00000006$秒のうちに実行できる。

この章の残りの部分では、この本を通して使う主な概念の一部を簡単に解説する。\secref{interface}は、この本で説明するデータ構造で実装するインターフェースをすべて説明するので必ず読んでほしい。残りの節では以下のことを説明する。
\begin{itemize}
\item 指数・対数・階乗関数や漸近（ビッグオー）記法・確率・ランダム化などの数学の復習
\item 計算のモデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの項目については、背景知識があってもなくても一度は飛ばし、必要に応じて戻り読みしてもよい。

\section{インターフェース}
\seclabel{interface}
データ構造について議論するときは、データ構造のインターフェースとその実装との違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がそれをどのようにやるかを表現する。

% TALK caprice 抽象データ型 (英：abstract data types)というふうに固有名詞であることを日英表記で強調したほうがよいのではないか。この本は固有名詞を太字にしないきらいがある。初学者は「抽象的な」「データの」型って？と形容詞のように読んでしまうかもしれない
\emph{インターフェース}\ejindex{interface}{いんたーふぇーす@インターフェース}\ejindex{abstract data type|see{interface}}{ちゅうしょうでーたがた@抽象データ型}（\emph{抽象データ型}と呼ばれることもある）は、データ構造がサポートする操作一式とその意味を定義する。インターフェースを見ても操作がどう実装されているかはわからない。サポートする操作の一覧とその引数、返り値の特徴だけを教えてくれる。 % types of arguments -> 型だけでなく色々な特徴がある

一方でデータ構造の\emph{実装}には、データ構造の内部表現と、実際に操作を行うアルゴリズムの定義を含む。そのため、ひとつのインターフェースに対する複数の実装がありうるる。例えば本書では\chapref{arrays}では配列を使って、\chapref{linkedlists}ではポインタを使って#List#インターフェースを実装してみせる。いずれも#List#インターフェースだが、実装方法は違っているのだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは要素を追加したり、特定のルールに従い次の要素を削除したりできる要素の集まりを表す。より正確にいうと、#Queue#インターフェースに実行できる操作は次のものである。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する。
  \item #remove()#：（以前に追加された）「次の値」#y#を#Queue#から削除し、#y#を返す。
\end{itemize}

#remove()#には引数がないことに注意する。#Queue#は\emph{取り出し規則}に従って次に削除する要素を決める。色々な取り出し規則がありうるが、主なものとしてFIFOやLIFO、優先度などがある。 % queueing discipline = 取り出し規則

\figref{queue}に示す\emph{FIFO（first-in-first-out、先入れ先出し）キュー}\ejindex{FIFO queue}{FIFOキュー} \ejindex{queue!FIFO}{きゅー@キュー!さきいれさきだし@先入れ先出し}では、追加したのと同じ順番で要素を削除する。これはコンビニのレジで並ぶ列と同じようなものだ。これは最も一般的な#Queue#なので、FIFOを付けずに単に#Queue#というときは普通このデータ構造のことを指す。FIFOキューの#add(x)#、#remove()#を、それぞれ#enqueue(x)#、#dequeue()#と呼ぶ流儀の教科書もある。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption{FIFOキュー}
  \figlabel{queue}
\end{figure}

% TALK caprice 同点要素は「同じ優先度を持つ要素」に言い換えた、一般的な用語ではないので。
\figref{prioqueue}に示す\emph{優先度付きキュー}では、
\ejindex{priority queue}{ゆうせんどつききゅー@優先度付きキュー}%
\ejindex{priority queue|seealso{heap}}{ゆうせんどつききゅー@優先度付きキュー|seealso{ヒープ}}%
\ejindex{queue!priority}{きゅー@キュー!ゆうせんどつき@優先度付き}%
#Queue#から要素を削除するとき最小のものを削除する。同じ優先度を持つ要素が複数あるときは、そのうちのどちらを削除してもよい。これは病院の救急室で重症患者を優先的に治療することに似ている。患者が到着するとまずはその症状の深刻さを見定め、待合室で待機してもらう。医師の手が空くと、最も重篤な患者から治療する。優先度付きキューにおける#remove()#操作を#deleteMin()#呼ぶ流儀の教科書もある。 % TODO: YJ I have never seen deleteMin though.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption{優先度付きキュー}
  \figlabel{prioqueue}
\end{figure}

他によく使う取り出し規則は\figref{stack}に示すLIFO（last-in-first-out、後入れ先出し）
\ejindex{LIFO queue}{LIFOキュー}%
\ejindex{LIFO queue|seealso{stack}}{LIFOキュー|seealso{スタック}}%
\ejindex{queue!LIFO}{きゅー@キュー!あといれさきだし@先入れ後出し}%
\ejindex{stack}{すたっく@スタック}%
だ。 \emph{LIFOキュー}では、最後に追加された要素が次に削除される。これは皿を積むように視覚化するとよい。積み上げられた皿をひとつずつ取るとき、皿は上から順に持って行かれる。この構造はとてもよく見かけるので#Stack#（スタック）という名前が付いている。#Stack#について話すときには#add(x)#、#remove()#のことを#push(x)#、#pop()#と呼ぶ。こうすればLIFO・FIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption{LIFOキュー（スタック）}
  \figlabel{stack}
\end{figure}

#Deque#（双方向キュー）はFIFOキューとLIFOキュー（スタック）の一般化だ。 #Deque#は先頭と末尾のある要素の列を表し、列の先頭または末尾に要素を追加できる。 #Deque#操作の名前はわかりやすく、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#だ。スタックは#addFirst()#と#removeFirst()#だけを使えば実装できることは知っておくと良いだろう。一方#addLast(x)#と#removeFirst()#だけを使えばFIFOキューになる。 % back = 末尾?

\subsection{#List#インターフェース：線形シーケンス}

この本に(FIFO-)#Queue#や#Stack#、#Deque#のインターフェースの話はあまり出てこない。なぜならこれらのインターフェースは#List#インターフェースにまとめられるからだ。\figref{list}に示す#List#\ejindex{List@#List#}{りすと@リスト}は、値の列$#x#_0,\ldots,#x#_{#n#-1}$を表現する。
#List#インターフェースは以下の操作を含む。

\begin{enumerate}
  \item #size()#: リストの長さ#n#を返す。
  \item #get(i)#: $#x#_{#i#}$の値を返す。
  \item #set(i,x)#: $#x#_{#i#}$の値を#x#にする。
  \item #add(i,x)#: #x#を#i#番目\footnote{コンピュータサイエンスでは序数を0からはじめることがある。例えばここで配列の#i#番目の要素と言っているのは、はじめから数えて$i+1$個目の要素のことである。}として追加し、$#x#_{#i#},\ldots,#x#_{#n#-1}$を後ろにずらす。\\
    すなわち、$j\in\{#i#,\ldots,#n#-1\}$について$#x#_{j+1}=#x#_j$とし、#n#をひとつ増やし、$#x#_i=#x#$とする。
  \item #remove(i)#: $#x#_{#i#}$を削除し、$#x#_{#i+1#},\ldots,#x#_{#n#-1}$を前にずらす。\\ 
    すなわち、$j\in\{#i#,\ldots,#n#-2\}$について$#x#_{j}=#x#_{j+1}$とし、#n#をひとつ減らす。
\end{enumerate}

これらの操作を使って#Deque#インターフェースを実装できる。 % sufficient -> 十分という意味

\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption{#List#は$0,1,2,\ldots,#n#-1$で添え字づけられた列を表現する。この#List#で#get(2)#を実行すると値$c$が返ってくる。}
  \figlabel{list}
\end{figure}


この後の章では(FIFO-)#Queue#、#Stack#、#Deque#のインターフェースについての話はほぼ出てこない。しかし、#Stack#と#Deque#という用語を「#List#インターフェースを実装したデータ構造」の名前として後の章で使うことがある。その場合、#Stack#と#Deque#という名前のついたそれらのデータ構造は、それぞれ#Stack#と#Deque#のインターフェースを非常に効率良く実装することにも使える、という事実を強調している。たとえば#ArrayDeque#は#List#インターフェースの実装であると同時に#Deque#の実装でもあり、#Deque#の操作をいずれも定数時間で実行できる。\footnote{実行時間についてはこの章の後半で説明するが、「定数時間で実行できる」とは要素がいくつあっても一定の時間で実行できるということであり、非常に効率がよいことを表す。}
% caprice unordered って入れたほうが USetのUが楽に理解できる
\subsection{#USet#インターフェース：順序付けられていない要素の集まり}

#USet#\index{USet@#USet#}インターフェースは重複がなく、順序付けられていない(unordered、#USet#のUはこの頭文字)要素の集まりを表現する。これは数学における\emph{集合}のようなものだ。#USet#には、#n#個の\emph{互いに相異なる}要素が含まれる。つまり、同じ要素が複数入っていることはない。また、要素の並び順は決まっていない。#USet#には以下の操作を実行できる。

\begin{enumerate}
\item #size()#：集合の要素数#n#を返す。
\item #add(x)#：要素#x#が集合に入っていなければ集合に追加する。\\
$#x# = #y#$を満たす集合の要素#y#が存在しないなら、集合に#x#を加える。#x#が集合に追加されたら#true#を返し、そうでなければ#false#を返す。
\item #remove(x)#：集合から#x#を削除する。\\
$#x# = #y#$を満たす集合の要素#y#を探し、集合から取り除く。そのような要素が見つかれば#y#を、見つからなければ#null#\footnote{訳注：#null#とは何もないことを示す記号である。}を返す。
\item #find(x)#：集合に#x#が入っていればそれを見つける。\\
$#x# = #y#$を満たす集合の要素#y#を見つける。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\end{enumerate}

上の定義で探したい#x#と見つかる（かもしれない）要素#y#とをわざわざ区別する必要がないように感じるかもしれない。
これを区別する理由は、別のもの（オブジェクト）である#x#と#y#とを何らかの基準で等しいと判定したくなることがあるからだ。\javaonly{\footnote{Javaでは、クラスの#equals(y)#・#hashCode()#メソッドをオーバーライドするとこれを行える。}}。これができるとキーを値に対応づけるインターフェースである\emph{辞書}(\emph{マップ})を実装するのに便利なのだ。   % TODO: YJ need better translation
\ejindex{dictionary}{じしょ@辞書}%
\ejindex{map}{まっぷ@マップ}%
% TODO 辞書、マップとハッシュテーブルの関わりを脚注で補足する

辞書（マップ）を作るために、まずは#Pair#\ejindex{pair}{ぺあ@ペア}という\emph{キー}と\emph{値}がペアになったオブジェクトを作る。 2つの#Pair#はキーが等しいければ（その値が等しいかどうかに関わらず）等しいとみなされる。#Pair#である$(#k#,#v#)$を#USet#に入れてから、$#x#=(#k#,#null#)$として#find(x)#を実行すると$#y#=(#k#,#v#)$が返ってくる。すなわち、キー#k#だけから値#v#が手に入るのだ。

\subsection{#SSet#インターフェース：ソートされた(sorted)要素の集まり}
\seclabel{sset}
% TODO caprice 全順序の脚注を入れる
\index{SSet@#SSet#}%
#SSet#インターフェースは順序づけされた要素の集まりを表現する。#SSet#には全順序な集合の要素が入る。すなわち任意の2つの要素#x#と#y#とを大小比較できる。サンプルコードでは、以下のように定義される#compare(x, y)#メソッドで比較を行うものとする。

\[
    #compare(x,y)#
      \begin{cases}
        {}<0 & \text{if $#x#<#y#$} \\
        {}>0 & \text{if $#x#>#y#$} \\
        {}=0 & \text{if $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%

#SSet#は#USet#と全く同じセマンティクスを持つ操作#size()#、#add(x)#、#remove(x)#をサポートする。#USet#と#SSet#の違いは#find(x)#にある。 % セマンティクス vs. 意味

\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: 順序づけられた集合から#x#の位置を特定する。\\
   すなわち$#y# \ge #x#$を満たす最小の要素#y#を見つける。
   もしこのような#y#が存在すればそれを返し、そうでないなら#null#を返す。
\end{enumerate}

この#find(x)#は、\emph{後継探索(XXX:訳語)}\ejindex{successor search}{こうけいたんさく@後継探索}と呼ばれることがある。#x#に等しい要素がなくても意味のある結果を返す点で#USet#の#find(x)#とは異なる。

#USet#、#SSet#における#find(x)#の区別は重要なのだが見過ごされがちである。 #SSet#は#USet#より多くの仕事をしてくれるぶん、実装が複雑で実行時間が長くなりがちだ。例えば、この本で述べる#SSet#の#find(x)#の実装では、要素数に対してその対数程度の時間がかかる。一方、\chapref{hashing}の#ChainedHashTable#による#USet#の実装では、#find(x)#の実行時間の期待値は定数である。#USet#にはなく#SSet#にある機能が必要ないときは#SSet#ではなく#USet#を使う方がよいだろう。

\section{数学的背景}
この節では本書で使う数学の記法や基礎知識を復習する。例えば対数やビッグオー記法、確率論などについて説明する。知っておいてほしい項目をまとめるに留め、丁寧な手ほどきはしない。背景知識が足りないと感じた読者はコンピュータサイエンスで使う数学の良い（無料の）教科書を読んでほしい。必要に応じて適切な箇所を読み、練習問題を解いてみるとよいだろう。
\cite{llm11}.

\subsection{指数と対数}

\ejindex{exponential}{しすうかんすう@指数関数}
$b^x$と書いて$b$の$x$乗を表す。$x$が正の整数なら、$b$にそれ自身を$x-1$回掛けた値になる。

\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x}
\]

$x$が負の整数なら、$b^x=1/b^{-x}$である。$x=0$なら、$b^x=1$である。
$b$が整数でないときも、指数関数$e^x$を使って冪乗を定義できる（$e$については後述する）。この$e^x$の定義は、指数級数による。こういう話をもっと知りたい人は微分積分学の教科書を読んでほしい。

\ejindex{logarithm}{たいすうかんすう@対数関数}
この本では，$\log_b k$と書いて\emph{$b$を底とする対数}を表す。これは次の式を満たす$x$として一意に決まる。

\[
    b^{x} = k
\]

底が2の対数を\emph{二進対数}という。この本に出てくる対数のほとんどは二進対数なので、底になにも書かずに$\log k$とある場合は、$\log_2 k$の省略記法とする。
\ejindex{binary logarithm}{にしんたいすう@二進対数}%
\ejindex{logarithm!binary}{たいすう@対数!にしん@二進}%

% TALK caprice 二分探索（英：binary search）と強調したい
対数の大雑把だがわかりやすいイメージを紹介しよう。$\log_b k$とは、$k$を何回$b$で割ると1以下になるかを表す数だと考えればよい。例えば、1回の比較で答えの候補を半分に絞り、最終的に答えの候補が1つに絞られるまでこれを繰り返すとして、最終的に何回の比較が必要になるかを見積もりたいとする。1回の比較で候補の数を$2$で割ることになるので、最初に$n+1$個の答えの候補があるなら、比較の回数は$\lceil \log_2(n+1) \rceil$以下だ（なお、このような手法を二分探索という）%
% TODO caprice ガウス記号の脚注をココにいれる
\footnote{訳注：$x$を実数とするとき、$\lceil x \rceil$は$x$以上の最小の整数を表す。$\floor x \rfloor$は$x$以下の最大の整数である。}。

\ejindex{natural logarithm}{しぜんたいすう@自然対数}%
\ejindex{logarithm!natural}{たいすう@対数!しぜん@自然}%
次のように定義される\emph{オイラーの定数}$e$を底とする対数もよく使う\footnote{訳注：日本ではeのことをネイピア数ということも多い。}。そこで、$\log_e k$のことを$\ln k$と書き、\emph{自然対数}と呼ぶ。\ejindex{Euler's constant}{おいらーのていすう@オイラーの定数}%
\index{e@$e$ (Euler's constant)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828
\]

自然対数は、次の一般的な積分の値が$e$になることから、よく登場する。
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k
\]

対数に関してよく使う操作は2つある。1つめは冪指数にある対数の除去だ。
\[
    b^{\log_b k} = k
\]

もう1つは底の変換操作だ。
\[
    \log_b k = \frac{\log_a k}{\log_a b}
\]

これら2つの操作を使うと、例えば自然対数と二進対数とを比較できる。
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} =
    (\ln 2)(\log k) \approx 0.693147\log k
\]

\subsection{階乗}
\seclabel{factorials}

\ejindex{factorials}{かいじょう@階乗}
この本には\emph{階乗関数}を使う場面がいくつかある。$n$が非負整数のとき、$n$の階乗$n!$は次のように定義される。
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n
\]

$n!$は、相異なる$n$要素の置換\ejindex{permutation}{ちかん@置換}の総数である。つまり、$n$個の要素を並べ変えたときの順列の総数が階乗になる。なお、$n=0$のとき、$0!$は1と定義される。

\ejindex{Stirling's Approximation}{すたーりんぐのきんじ@スターリングの近似}%
$n!$の大きさは\emph{スターリングの近似}を使って見積もれる。
\footnote{訳注：以下、スターリングの近似に関する議論は、初学者は飛ばしても良いと思われる。}%この訳注は不要そう -kshikano
\[
  n!
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)}
\]
ここで$\alpha(n)$は次の条件を満たす。
\[
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}
\]

スターリングの近似を使って$\ln(n!)$の近似値も計算できる。
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
% TALK caprice このカッコ内の記述についても「初学者は無視して良い」と脚注入れたい
（実際、$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$を$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$で近似するというのが、スターリングの近似の簡単な証明方法でもある。)

\ejindex{binomial coefficients}{にこうけいすう@二項係数}%
階乗関数に関連して、ここで\emph{二項係数}について説明する。$n$を非負整数、$k$を$\{0,\ldots,n\}$の要素とするとき、二項係数$\binom{n}{k}$は次のように定義される。
\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]
二項係数$\binom{n}{k}$は、大きさ$n$の集合における大きさ$k$の部分集合の個数である。言い換えると、集合$\{1,\ldots,n\}$から相異なる$k$個の整数を取り出すときの場合の数を表す値と解釈できる。 % TODO: YJ 元の文でもi.e.とあるが、これは定義ではなく一つの例なのでは？すなわち、と言ってしまってよいのだろうか。「例えば」の方が正確では。-- 「言い換えると」が適切だと思います -kshikano

\subsection{漸近記法}
\seclabel{asymptotic}

\ejindex{asymptotic notation}{ぜんきんきほう@漸近記法} \ejindex{big-Oh notation}{びっぐおーきほう@ビッグオー記法} \index{O@$O$ notation}
データ構造を分析するときは、さまざまな操作の実行時間について考察したい。しかし、正確な実行時間はコンピュータによって異なる。同じコンピュータ上でさえ実行のたびに異なるだろう。この本で操作の実行時間といったら、操作に際してコンピュータが実行する命令の数とする。この数を正確に計算するのは、単純なコードであっても困難な場合がある。そのため、正確な実行時間を求めるのではなく、\emph{漸近記法}（\emph{ビッグオー記法}）と呼ばれる方法で実行時間を見積もる。この方法では、ある関数$f(n)$について、次のように定義される関数の集合$O(f(n))$を考える。%節題でもある漸近記法という用語を前に出しておきます -kshikano
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{ある$c>0$と$n_0$が存在し、} \\
             \quad\mbox{任意の$n\ge n_0$について$g(n) \le c\cdot f(n)$を満たす}
     \end{array} \right\}
\]
イメージとしては、$n$が十分に大きいとき（つまりグラフの右のほうを見たとき）に$c\cdot f(n)$のほうが上にくるような関数$g(n)$を集めたものが集合$O(f(n))$だ。 % 「上から抑えられる」は既に知識がある人には伝わるが、初学者には伝われないのでは。 % TODO: YJ need revision

漸近記法は、関数を単純な形にするのに使う。たとえば、$5n\log n + 8n - 200$の代わりに$O(n \log n)$と書ける。これは次のように証明できる。
\begin{align*}
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ $n\ge 2$のとき（このとき$\log n \ge 1$）}
            \\
        & \le 13n\log n
\end{align*}
$c = 13$および$n_0 = 2$とすれば、関数$f(n)= 5n \log n + 8n-200$が集合$O(n \log n)$に含まれることがわかる。

漸近記法の便利な性質をいくつか挙げる。
まずは、任意の定数$c_1 < c_2$について以下が成り立つ。
\[ O(n^{c_1}) \subset O(n^{c_2}) \]
つづいて、任意の定数$ a, b, c> 0 $について以下が成り立つ。
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \]
これらの包含関係は、それぞれに正の値を掛けても保たれる。
たとえば$n$を掛けると次のようになる。
\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \]
一般的な慣習に従って、本書でもビッグオー記法を濫用する。すなわち、$f_1(n) = O(f(n))$と書いて$f_1(n) \in O(f(n))$であることを表す。そして、「この操作の実行時間は集合$O(f(n))$に\emph{含まれる}」ことを、単に「この操作の実行時間は$O(f(n))$だ」と言う。
これらの表現を認めると、冗長な記述が不要になるし、一連の等式で漸近記法を使えるようになる。

ビッグオー記法を濫用することで、たとえば次のような不思議な書き方ができる。
\[
  T(n) = 2\log n + O(1)
\]
これは正確に書くとこうなる。
\[
  T(n) \le 2\log n + [\mbox{$O(1)$のある要素]}
\]

$O(1)$という記法には別の問題もある。この記法には変数が入ってないので、どの変数が大きくなるのかわからないのだ。これは文脈から読み取る必要がある。上の例では、方程式の中に変数は$n$しかないので、$T(n)= 2 \log n + O(f(n))$のうちで$f(n) = 1$の場合であると読み取ることになる。

ビッグオー記法は、新しい記法でもコンピュータサイエンス独自の記法でもない。1894年には数学者のPaul Bachmannがこの記法を使っていた。その後しばらくして、コンピュータサイエンスにおいてアルゴリズムの実行時間を論ずる際に、この記法が非常に便利なことがわかったのだ。
次のコードを考えてみよう。

\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}

この関数を1回実行すると以下の処理が行われる。
\begin{itemize}
      \item 代入$1$回（#int\, i\, =\, 0#）
      \item 比較$#n#+1$回（#i < n#）
      \item インクリメント#n#回（#i++#）
      \item 配列のオフセット計算#n#回（#a[i]#）
      \item 間接代入#n#回（#a[i] = i#）
\end{itemize}
よって実行時間は以下のようになる。
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n#
\]
$a$、$b$、$c$、$d$、$e$はプログラムを実行するマシンに依存する定数で、それぞれ代入、比較、インクリメント、配列のオフセット計算、間接代入にかかる実行時間を表す。たった2行のコードについて実行時間を表すのに、こうも複雑な式がいるようでは、さらに複雑なコードやアルゴリズムは到底扱えないだろう。ビッグオー記法を使えば、実行時間を次のように簡潔に表せる。
\[
    T(#n#)= O(#n#)
\]
この式は、簡潔な表現にもかかわらず、最初の式と同じくらいの内容を表している。正確な実行時間は定数$a$、$b$、$c$、$d$、$e$に依存しており、これらの値がすべて判明しないと知りようがないからだ。がんばって値を実測してみても、得られる結論はそのマシンでしか有効でない。

ビッグオー記法を使えば、より抽象的な分析ができ、より複雑な関数も扱える。2つのアルゴリズムの実行時間がビッグオー記法で同じなら、どちらが速いか優劣はつけられない。一方のアルゴリズムが速いマシンもあれば、もう一方のアルゴリズムが速いマシンもあるだろう。しかし、2つのアルゴリズムの実行時間がビッグオー記法で異なるなら、\emph{#n#が十分大きい場合}、実行時間が小さいアルゴリズムのほうがどのようなマシンにおいても速いといえる。

ビッグオー記法を使って2つの異なる関数を比べる例を\figref{intro-asymptotics}に示す。これは$f_1(#n#)=15#n#$と$f_2(n)=2#n#\log#n#$のグラフである。$f_1(#n#)$は複雑な線形時間アルゴリズムの実行時間を表し、$f_2(#n#)$は分割統治に基づくシンプルなアルゴリズムの実行時間を表している。これを見ると、#n#が小さいうちは$f_1(#n#)$のほうが$f_2(#n#)$より大きいが、#n#が大きくなると大小関係が逆転することがわかる。つまり、nが十分大きいなら、実行時間が$f_1(#n#)$であるアルゴリズムのほうが圧倒的に性能がよい。ビッグオー記法の式$O(#n#)\subset O(#n#\log #n#)$は、この事実を示している。

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{$15#n#$と$2#n#\log#n#$の比較}
  \figlabel{intro-asymptotics}
\end{figure}

多変数関数に対して漸近記法を使うこともある。標準的な定義はないようだが、この本では次の定義を用いる。
\[
   O(f(n_1,\ldots,n_k)) =
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{ある$c>0$と$z$が存在し、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k)\ge z$を満たす任意の$n_1,\ldots,n_k$について、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$が成り立つ} \\
   \end{array}\right\}
\]
興味があるのは引数$n_1,\ldots,n_k$によって$g$が大きくなるときの状況であり、その状況はこの定義で把握できる。$f(n)$が$n$に関する増加関数なら、この定義は1変数の場合の$O(f(n))$の定義とも合致する。この本ではこれ程度の考察で十分だが、教科書によっては多変数関数と漸近記法に別の定義を与えている可能性もあるので注意してほしい。

% TALK 原文がrandomizationなのでランダム性ではなく乱択化では？
\subsection{ランダム性と確率}
\seclabel{randomization}

\ejindex{randomization}{らんたくか@乱択化}%
\ejindex{probability}{かくりつ@確率}%
\ejindex{randomized data structure}{らんたくでーたこうぞう@乱択データ構造}%
\ejindex{randomized algorithm}{らんたくあるごりずむ@乱択アルゴリズム}%
この本で扱うデータ構造には\emph{ランダム性}を利用するものがある。格納されているデータや実行する操作に加えて、サイコロの出目もふまえて実際の処理を決めるのだ。そのため同じことをしても実行時間が毎回同じとは限らない。こういうデータ構造を分析するときは\emph{期待実行時間}を考えるのがよい。
\ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
\ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%

形式的には、ランダム性を利用するデータ構造における操作の実行時間は確率変数である。そしてその\emph{期待値}を知りたい。全事象$U$の値をとる離散確率変数を$X$とするとき、$X$の期待値$E[X]$は次のように定義される。
\ejindex{expected value}{きたいち@期待値}%
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\}
\]

ここで、$\Pr\{\mathcal{E}\}$は事象$\mathcal{E}$の発生確率とする。この本の例では、データ構造の内部で発生するランダム性のみを考慮して確率を定める。データ構造に入ってくるデータや実行される操作列がランダムだとは仮定しないことには注意する。

期待値の最も重要な性質のひとつは\emph{期待値の線形性}である。
\ejindex{linearity of expectation}{きたいちのせんけいせい@期待値の線形性}%
任意のふたつの確率変数$X$と$Y$について次の式が成り立つ。
\[
   \E[X+Y] = \E[X] + \E[Y]
\]

より一般的には、任意の確率変数$ X_1,\ldots,X_k $について次の関係が成り立つ。
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i]
\]

期待値の線形性によって、（上の式の左辺のように）複雑な確率変数の期待値を、（右辺のような）より単純な確率変数の期待値に分解できる。

\emph{インジケータ確率変数}\ejindex{indicator random variable}{いんじけーたかくりつへんすう@インジケータ確率変数}はよく使う便利なトリックだ。この二値変数はなにかを数えるときに役立つ。例を見るとよくわかるだろう。表と裏が等しい確率で出るコインを$k$回投げたとき、表が出る回数の期待値を知りたいとする。
\ejindex{coin toss}{こいんなげ@コイン投げ}
直感的な答えは$k/2$だが、これを期待値の定義を使って証明すると次のようになる。

\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2
\end{align*}
% この「２項係数の性質」、原文自体が公式間違ってたのでプルリク送った上でここでも直しといた
この計算をするには、$\Pr\{X=i\} = \binom{k}{i}/2^k$および2項係数の性質$i\binom{k}{i}=k\binom{k-1}{i-1}$や$\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$を知っている必要がある。
% TALK インジケータ確率変数（英：indicator random variables）としたい
インジケータ変数と期待値の線形性を使えばはるかに簡単になる。$\{1,\ldots,k\}$の各$i$に対し以下のインジケータの確率変数を定義する。

\[
    I_i = \begin{cases}
           1 & \text{$i$番目のコイントスの結果が表のとき} \\
           0 & \text{そうでないとき}
          \end{cases}
\]
そして、$I_i$の期待値を計算する。
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \]
ここで、$X=\sum_{i=1}^k I_i$なので次のように所望の値が得られる。
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2
\end{align*}

この計算は少し長いものの確率の不思議な等式は使っていない。また各コイントスは$1/2$の確率で表が出るので、表は試行回数の半分くらい出るだろうという直感の説明にもなっている。

\section{計算モデル}
\seclabel{model}

本書ではデータ構造における操作の実行時間を理論的に分析する。これを正確に行うための計算の数学的なモデルが必要だ。そのために\emph{#w#ビットのワードRAM}モデルを使うことにする。
\ejindex{word-RAM}{わーどらむ@ワードRAM}%
\index{RAM}%
RAMはランダムアクセスマシン(Random Access Machine)の頭字語である。
このモデルではランダムアクセスメモリを使える。
ランダムアクセスメモリはセルの集まりで、これはそれぞれ#w#ビットのワードを格納できる。
\ejindex{word}{わーど@ワード}
つまり、各セルはw桁の２進数の集合$\{0,\ldots,2^{#w#}-1\}$のうちのいずれかひとつを表せる。
% TODO 剰余の脚注
ワードRAMモデルではワードの基本的な操作に一定の時間が必要である。基本的な操作とは算術演算（#+#, #-#, #*#, #/#, #%#）や比較（$<$, $>$, $=$, $\le$, $\ge$）、ビット単位の論理演算（ビット単位の論理積 ANDや論理和 OR、排他的論理和 XOR）である。

どのセルも一定の時間で読み書きできる。コンピュータのメモリはメモリ管理システムによって管理される。メモリ管理システムは必要に応じてメモリブロックを割り当てる、またメモリブロックの割り当てを解除する。サイズ$k$のメモリブロックの割当てには$O(k)$の時間がかかり、新しく割り当てられたメモリブロックへの参照（ポインタ）が返される。この参照はひとつのワードに収まるビットで表現できるとする。

ワード幅#w#はこのモデルの重要なパラメータである。
この本で#w#に置く仮定は、#n#をデータ構造に格納されうる要素数とするとき、$#w# > \log #n#$であるということだけだ。これは控えめな仮定である。なぜならこれが成り立たないとひとつのワードではデータ構造の要素数を表すことすらできないためである。

メモリ使用量はワード単位で測るので、データ構造のメモリ使用量とはデータ構造の使うワード数である。我々のデータ構造はみなある型#T#の値を格納し、#T#型の要素は1ワードのメモリで表現できると仮定する。
\javaonly{（実際に、Javaでは#T#型のオブジェクトの参照を格納しており、この参照は1ワードのメモリを占める。）}

\javaonly{#w#ビットのワードRAMモデルは、$#w#=32$とすると、（32ビット）Java仮想マシン（JVM）によく似ている。}
\cpponly{#w#ビットのワードRAMモデルは、$#w#=32$または$#w#=64$とすると、現代のデスクトップコンピュータ環境によく似ている。}
すなわちこの本に載っているデータ構造はいずれも一般的なコンピュータ上で動作するように実装できる。

\section{正しさ、時間複雑性、空間複雑性}

データ構造の性能を考えるとき重要な項目が3つある。
% TALK caprice ここも英語を加えたい
\begin{description}
  \item[正しさ：]データ構造はそのインタフェースを正しく実装しなければならない。
  \item[時間複雑性：]データ構造における操作の実行時間は短いほどよい。
  \item[空間複雑性：]データ構造のメモリ使用量は小さいほどよい。
\end{description}

この本は入門書なので正しいデータ構造のみを扱う。つまり、不正確な出力が得られることがあったり、更新をちゃんとしなかったりするデータ構造のことは考えない。
一方で、メモリ使用量を小さくに抑える工夫をするデータ構造は紹介する。
紹介する工夫の多くは操作の（漸近的な）実行時間には影響しないが、実際にはデータ構造を少し遅くするかもしれない。

データ構造の実行時間は次の三種類のいずれかを考えることが多い。

\begin{description}
\item[最悪実行時間：]
  \ejindex{running time}{じっこうじかん@実行時間}%
  \ejindex{running time!worst-case}{じっこうじかん@実行時間!さいあく@最悪}%
  \ejindex{worst-case running time}{さいあくじっこうじかん@最悪実行時間}%
  これは最も強力な実行時間の保証である。操作の最悪実行時間が$f(#n#)$ならば、操作の実行時間は\emph{決して}$f(#n#)$よりも長いことはない。
\item[償却実行時間：]
  % XXX: 原文には at most がついているが不要ではないか。O(f(n))と混同している？
  \ejindex{running time!amortized}{じっこうじかん@実行時間!しょうきゃく@償却}%
  \ejindex{amortized running time}{しょうきゃくじっこうじかん@償却実行時間}%
  償却実行時間が$f(#n#)$であるとは、典型的な操作のコストが$f(#n#)$であることを意味する。
  より正確には、$m$個の操作の列の実行時間の合計が$mf(#n#)$であることを意味する。
  いくつかの操作には$f(#n#)$より長い時間がかかるかもしれないが、操作の列全体として考えればひとつあたりの実行時間は$f(#n#)$なのである。 % TODO: YJ amortize = 償却: better translation?
\item[期待実行時間：]
  \ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%
  \ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
  期待実行時間が$f(#n#)$ならば、実行時間は確率変数（\secref{randomization}を参照）であり、この確率変数の期待値は$f(#n#)$である。
  なお期待値を計算する際に考えるランダム性は、データ構造の内部で行う選択のランダム性である。
\end{description}

最悪、償却、期待実行時間の違いを理解するのには、お金の例え話が役に立つ。家を買う費用のことを考えてみよう。 % finance

\paragraph{最悪コストと償却コスト}
\ejindex{amortized cost}{しょうきゃくこすと@償却コスト}%
家の価格が12万ドルだとする。毎月1200ドルの120ヶ月（10年）の住宅ローンでこの家が手に入るかもしれない。この場合、月額費用は最悪でも月1200ドルだ。

十分な現金を持っていれば12万ドルの一括払いで家を買うこともできる。こうするとこの家を購入代金を10年で償却した月額費用は以下のようになる。
\[
   \$120\,000 / 120\text{ヶ月} = \$1\,000\text{月あたり}
\]

これはローンの場合に支払う月額1200ドルよりだいぶ少ない。

\paragraph{最悪コストと期待コスト}
次に、12万ドルの家における火災保険を考えてみよう。保険会社が何十万件もの事例を調べた結果、大多数の家では火事を起こさず、わずかな数の家では煙による被害程度で済むボヤを起こり、ごく少数の家では全焼被害になることがわかった。保険会社はこの情報に基づき、12万ドルの家における火災被害額の期待値は月額10ドル相当であると判断し、儲けるために火災保険としては月額15ドルの料金設定を行なった。

決断のときだ。15ドルを最悪支払月額（かつ期待支払月額）とするその会社の火災保険に入るべきだろうか？それとも、いちかばちか、期待支払月額である月額10ドルを自分で貯める自家保険を行うことにして
\footnote{訳注：なぜ被保険者側が、保険会社しか知らないはずのその額を知っているのかは不問とする。この例の目的は現実の近似ではなく、異なる種類のコストに対する感覚を掴むことである。}
、月額5ドルの節約を選ぶべきだろうか？期待支払月額としては明らかに自家保険の方が安いが、実際支払月額が遥かに高くなる可能性を考慮しなければならない。すなわち自家保険では、低い確率ではあるが、家が全焼して実際支払月額が最悪支払月額である12万ドルになる可能性があるのだ。

この例は、我々がどちらを選ぶかは場合によって変わるのだ、と教えてくれる
\footnote{訳注：この例では、最悪支払月額の上限が大幅に低くなること、また支払月額の差が５ドルと比較的少額であることを考慮して、火災保険に入ることを選ぶ人が多いかもしれない。しかし驚くべきことに\emph{データ構造の世界では、最悪実行時間よりも償却・期待実行時間が低いことを優先する}ことの方が遥かに多い。というのも、最悪実行時間だけかかったときの損害が全焼ほど大きくはなく、またその確率も全焼よりはるかに小さく制御できることが多いからだ。}
。
償却・期待実行時間は最悪実行時間よりも小さいことが多い。最悪実行時間の長さに目をつむり、償却・期待実行時間でいえば短いからと妥協することにすれば、はるかに単純なデータ構造を採用できる場合がよくあるのだ。

\section{コードサンプル}
\pcodeonly{
この本のコードサンプルは擬似コードで書いた。
\ejindex{pseudocode}{ぎじこーど@擬似コード}%
ここ40年のどの一般的なプログラミング言語の経験を持つ人も理解しやすいコードを書いたつもりである。
この本におけるコードがどんなものかは、配列#a#の平均値を計算する次の擬似コードを見てみるとわかるだろう。
\pcodeimport{ods/Algorithms.average(a)}
このコードでは、変数への代入を表す記法は$\gets$である。
% WARNING: graphic typesetting of assignment operator
配列#a#の大きさを#len(a)#と書き、配列の添え字は0からはじまることにする。
このとき、#range(len(a))#は#a#の正しい添え字の集まりである。
コードを短くし、また読みやすくするために、部分配列代入を使うことがある。
次のふたつの関数は同じことをしている。
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
続くコードは配列のすべての要素を0にするものである。
\pcodeimport{ods/Algorithms.zero(a)}
このようなコードの実行時間を解析するときは#a[0:len(a)] = 1#や、#a[1:len(a)] = a[0:len(a)-1]#のような文は、定数時間では実行できないことに気をつけなければならない。
これらの実行時間は$O(#len(a)#)$である。

変数の代入においても同様の簡略記法を使う
#x,y=0,1#は、#x#を0に#y#を1にする。
また#x,y = y,x#は変数#x#と#y#の値を入れ替える。
\ejindex{swap}{すわっぷ@スワップ}

この本の擬似コードには、親しみがないかもしれない記法も小数だが使われている。
数学における（ふつうの）割り算の演算子を$/$と書く。
整数の除算を使う必要のある場面も多く、これを$#//#$演算子で表す。
$#a//b# = \lfloor a/b\rfloor$は、$a/b$の整数部分である。
そのため例えば、$3/2=1.5$だが、$#3//2# = 1$である。
\ejindex{integer division}{せいすうじょさん@整数除算}%
\ejindex{div operator}{じょさんえんざんし@除算演算子}%
整数除算における余りを計算する$\bmod$演算子を使うこともあるが、これはその際になってから定義することにする。
\ejindex{mod operator}{じょうよえんざんし@剰余演算子}%
ビット単位の演算子を使うこともある。
具体的には、左シフト（#<<#）、右シフト（#>>#）、ビット単位の論理積（#&#）、そしてビット単位の排他的論理和（#^#）である。
\ejindex{left shift}{ひだりしふと@左シフト}%
\ejindex{#<<#|see {left shift}}{#<<#|see{ひだりしふと@左シフト}}%
\ejindex{right shift}{みぎしふと@右シフト}%
\ejindex{#>>#|see {right shift}}{#>>#|see {みぎしふと@右シフト}}%
\ejindex{bitwise and}{びっとたんいろんりせき@ビット単位論理積}%
\ejindex{#&#|see {bitwise and}}{#&#|see {びっとたんいろんりせき@ビット単位論理積}}%
\ejindex{#^#|see {bitwise exclusive-or}}{#^#|see {びっとたんいろんりせき@ビット単位論理積}}%

この本の擬似コードはPythonのコードから機械的に翻訳したものである。
元となったコードは本のウェブサイトからダウンロードできる。
\footnote{ \url{http://opendatastructures.org}}である。もし擬似コードにおいて曖昧な点があれば、対応するPythonのコードを参照することができる。もしPythonが読めなければ、JavaとC++で書かれたコードもある。擬似コードの意味がわからず、PythonもC++もJavaも読めないなら、まだこの本を読むのは早いかもしれない。}

\notpcode{
この本のコードサンプルは\lang{}で書いた。
しかし、\lang{}に親しみのない人も読めるようシンプルに書いたつもりだ。
例えば#public#や#private#は出てこない。オブジェクト指向を前面に押し出すこともない。

B、C、C++、C\#、Objective-C、D、Java、JavaScriptなどをALGOL系の言語を書いたことのある人なら本書のコードの意味はわかるだろう。
完全な実装に興味のある読者はこの本に付属の\lang{}ソースコードを見てほしい。

この本は数学的な実行時間の解析と、対象のアルゴリズムを実装した\lang{}のコードとを共に含む。そのためソースコードと数式で同じ変数が出てくる。
このような変数は同じ書式で書く。
一番よく出てくるのは変数#n#\index{n@#n#}である。
#n#は常にデータ構造に格納されている要素の個数を表す。
}

\section{データ構造の一覧}

表~\ref{tab:summary-i}と表~\ref{tab:summary-ii}は本書で扱うデータ構造における性能の要約である。これらは\secref{interface}で説明した#List#や#USet#、#SSet#を実装する。
\Figref{dependencies}はこの本の各章の依存関係を示している。
\ejindex{dependencies}{いぞんかんけい@依存関係}%
破線の矢印は弱い依存関係を示している。これは章のごく一部の内容や結果のみに依存することを示す。

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#List#の実装} \\ \hline
% XXX: この本の定義に従うとき、二変数のO記法の中に+1を添える必要はあるか？
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{\emph{償却}実行時間を表す。}
\item[E]{\emph{期待}実行時間を表す。}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption{#List#・#USet#の実装の要約}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#SSet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{(Priority) #Queue# の実装} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{このデータ構造は#w#ビット整数のみを格納できる}
\javaonly{\item[X]{これは外部メモリモデルでの実行時間である。\chapref{btree}を参照せよ。}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption{#SSet#・優先度付き#Queue#の実装の要約}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{この本の内容の依存関係}
  \figlabel{dependencies}
\end{figure}

\section{ディスカッションと練習問題}

\secref{interface}で説明した#List#・#USet#・#SSet#インターフェースは、 Java Collections Framework\cite{oracle_collections}の影響を受けている。
\index{Java Collections Framework}%
これらはJava Collections Frameworkの#List#・#Set#・#Map#・#SortedSet#・#SortedMap#をシンプルにしたものである。
\javaonly{付属のソースコードは#USet#・#SSet#の実装を#Set#・#Map#・#SortedSet#・#SortedMap#の実装にするためのラッパークラスを含んでいる。}

この章で扱った漸近記法・対数・階乗・スターリングの近似・確率論の基礎などは、Leyman, Leighton, and Meyer\cite{llm11}の素晴らしい（そして無料の）本が扱っている。
分かりやすい微積分の教科書としては、無料で手に入るThompson\cite{t14}の古典的な教科書がある。この本には指数や対数の形式的な定義が書かれている。

基礎的な確率論については、特にコンピュータ・サイエンスに関連するものとしてRoss\cite{r01}の教科書がおすすめである。
漸近記法や確率論などを含むGraham, Knuth, and Patashnik\cite{gkp94}の教科書も参考になるだろう。

\javaonly{Javaプログラミング力を磨きたい読者のためには、オンラインのJavaのチュートリアル~\cite{oracle_tutorials}がある。}

\begin{exc}
練習問題は読者が問題に対する正しいデータ構造を選ぶ練習をするためのものだ。
利用可能な実装やインターフェースがあれば、それを使って解いてみてほしい。
（JavaならばJava Collections Frameworkが、C++ならばStandard Template Libraryがある。）

以下の問題はテキストの入力を一行ずつ読み、各行で適切なデータ構造の操作を実行することで解いてほしい。ただしファイルが百万行であっても数秒以内に処理できる程度に効率的な実装でなければならないものとする。

  \begin{enumerate}
    \item 入力を一行ずつ読み、その逆順で出力せよ。すなわち最後の入力行を最初に書き出し、最後から二番目の入力行を二番目に書き出す、というように出力せよ。

    \item  最初の50行入力を読み、それを逆順で出力せよ。その後続く50行を読み、それを逆順で出力せよ。これを読み取る行が無くなるまで繰り返し、最後に残っていた行（50行未満かもしれない）もやはり逆順で出力せよ。

      つまり、出力は50番目の行からはじまり、49、48、...、1番目の行が続く。
	  この次は100番目の行で、99、...、51番目の行が続く。

	 またプログラム実行中に50より多くの行を保持してはならない。

    \item 入力を一行ずつ読み取り、42行目以降で空行を見つけたら、その42行前の行を出力せよ。例えば、242行目が空行であれば、200行目を出力せよ。
	またプログラム実行中に43行以上の行を保持してはならない。

    \item 入力を一行ずつ読み取り、もしこれまでと重複のない行を見つけたら出力せよ。
	重複がたくさんあるファイルを読む場合にも、重複なく行を保持するのに必要なメモリより多くメモリを使わないように注意せよ。

    \item 入力を一行ずつ読み取り、それがこれまでに読んだことのある行と同じなら出力せよ。（最終的には、入力ファイルからある行がはじめて現れた箇所をそれぞれ除いたものが出力である。）
	ファイルが多くの重複行を含む場合でも、行を重複なく保持するのに必要なメモリよりは多くメモリを使わないように注意せよ。

    \item 入力を全て読み取り、短い順に並び替えて出力せよ。
	同じ長さの行があるときは、それらの行の順序は辞書順に並べるものとする。
	また、重複する行は一度だけ出力するものとする。

    \item 直前の問題で、重複する行は現れた回数だけ出力するように変更した問題を解け。

    \item 入力をすべて読み、全ての偶数番目の行を出力した後に全ての奇数番目の行を出力せよ。（なお、最初の行を0行目と数える。）

    \item 入力をすべて読み、ランダムに並び替えて出力せよ。
	どの行の内容も書き換えてはならない。
	また、入力とくらべて行を減らしたり増やしたりしてもいけない。
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}%
  \emph{Dyck word}とは+1, -1からなる列で、先頭から任意の#k#番目の値までの部分列（プレフィックス）の和がいずれも非負であるものである。
  例えば、$+1,-1,+1,-1$はDyck wordだが、$+1,-1,-1,+1$は$+1-1-1<0$なのでDyck wordではない。
  Dyck wordと#Stack#の#push(x)#・#pop()#操作の関係を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{matched string}{まっちしたもじれつ@マッチした文字列}%
  \ejindex{string!matched}{もじれつ@文字列!マッチした}%
  \emph{マッチした文字列}とは\{, \}, (, ), [, ]のからなる列で、すべての括弧が適切に対応しているものである。
  例えば、「\{\{()[]\}\}」はマッチした文字列だが、「\{\{()]\}」はふたつめの\{に対応する括弧が]であるためマッチした文字列ではない。
  長さ#n#の文字列が与えられたとき、この文字列がマッチしているかを$O(#n#)$で判定するにはスタックをどう使えばよいかを説明せよ。
\end{exc}

\begin{exc}
  #push(x)#・#pop()#操作のみが可能なスタック#s#が与えられる。
  FIFOキュー#q#だけを使って#s#の要素を逆順にする方法を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{Bag@#Bag#}{ばっぐ@バッグ}%
  #USet#を使って#Bag#を実装せよ。
  #Bag#は#USet#によく似たインターフェースである。
  #Bag#は#add(x)#・#remove(x)#・#find(x)#操作をサポートするが、重複する要素も格納するところが異なる。
  #Bag#の#find(x)#操作は#x#に等しい要素が1つ以上含まれているときそのうちのひとつを返す。
  さらに#Bag#は#findAll(x)#操作もサポートする。
  これは #Bag#に含まれる#x#に等しいすべての要素のリストを返す。
\end{exc}

\begin{exc}
  #List#・#USet#・#SSet#インターフェースを実装せよ。
  ただし効率的な実装でなくてもよい。
  ここで実装するものは、後の章で出てくるより効率的な実装の正しさや性能をテストするために役立つ。（最も簡単な方法は要素を配列に入れておく方法だ。）
\end{exc}

\begin{exc}
  直前の問題の実装の性能をアップするための思いつく工夫をいくつか試みよ。
  実験してみて、 #List#の#add(i,x)#・#remove(i)#の性能がどう向上したか考察せよ。
  #USet#・#SSet#の#find(x)#の性能はどうすれば向上しそうか考えてみよ。
  この問題はインターフェースの効率的な実装がどのくらい難しいかを実感するためのものである。
\end{exc}
