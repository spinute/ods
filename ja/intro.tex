\chapter{Introduction}
\pagenumbering{arabic}

%世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。
データ構造とアルゴリズムに関する授業は全世界でコンピュータサイエンスの課程に含まれている。データ構造はそれほど重要だ。生活の質を上げるだけでなく、毎日のように人の命さえ救っている。データ構造によって数百万ドル、数十億ドルの規模にまでなった企業も多い。

% どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
% How can this be (important)? では?
なぜデータ構造はこんなにも重要なのだろう？考えてみれば私たちは普段からさまざまなデータ構造と接している。

\begin{itemize}
	\item ファイルを開く：ファイルシステム\ejindex{file system}{ふぁいるしすてむ@ファイルシステム}のデータ構造を使って、ファイルをハードディスクなどの上に配置し、検索できる。ハードディスクには数億ものブロックがあり、ファイルの内容はどのブロックに保存されていてもおかしくないので、これは簡単なことではない。
	\item 電話番号を検索する：入力の途中で連絡先リスト\ejindex{contact list}{れんらくさきりすと@連絡先リスト}から電話番号を検索するためにデータ構造が使われている。連絡先リストには膨大な情報（過去に電話や電子メールをやり取りした全員）が含まれている可能性があること、電話端末には高速なプロセッサや潤沢なメモリは搭載されていないことを考えると、これは簡単なことではない。
	\item SNSにログインする：\ejindex{social network}{そーしゃるねっとわーく@ソーシャルネットワーク}ネットワークサーバーではログイン情報からアカウント情報を検索する。利用者が多いSNSには何億人ものアクティブなユーザーがいるので、これは簡単ではない。
	\item Webページを検索する：\ejindex{web search}{うぇぶけんさく@ウェブ検索}検索エンジンは検索語からWebページを見つけるためにデータ構造を使う。インターネットには85億以上のWebページがあり、それぞれのページに検索対象になりうる語句が大量に含まれているので、これは簡単なことではない。
	\item 緊急通報用番号（9-1-1）に電話する：\ejindex{emergency services}{きんきゅうさーびす@緊急サービス}\index{9-1-1}緊急通報電話では、パトカー、救急車、消防車を速やかに現場に手配できるよう、電話番号と住所を対応づけるためにデータ構造を使う。電話をかけた人は正確な住所を伝えられないかもしれず、この場面での遅れは生死を分かつこともあるので、これは重要な問題だ。
\end{itemize}

\section{効率の必要性}

次節では、よく使うデータ構造に対してどんな操作ができるのかを見ていく。ちょっとしたプログラミング経験があれば、正しい結果を返す操作を実装するのは難しくないだろう。データを配列や連結リストに入れ、すべての要素について順番に処理し、必要なら要素を追加したり削除したりするという実装にすればよい。

この実装は簡単だが、効率がよくない。とはいえ、効率について考える価値はあるだろうか？コンピュータはどんどん速くなっている。簡単な実装で十分かもしれない。それを確認するためにざっくりと計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）個の要素を持つアプリケーションがあるとする。各要素を少なくとも一回は見たくなるというのは、それなりに妥当な仮定だろう。この場合、少なくとも100万（$10^6$）回、このデータセットから要素を探すことになる。100万回にわたって100万個の要素をすべて確認すると、データを読み出す回数は合計で1兆（$10^6\times 10^6=10^{12}$）回になる。

\paragraph{プロセッサの速度：}
本書執筆時点では、かなり高速なデスクトップコンピュータでも、毎秒10億（$10^9$）回以上の操作は実行できない\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作に普通は数サイクルが必要だ。}。よって、このアプリケーションの完了には、少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピュータにとって16分は非常に長い時間だが、人間ならコーヒーブレイクを挟んでそれくらいの時間は待っていられるだろう。

\paragraph{大きなデータセット：}
Google\index{Google}について考えてみよう。Googleでは85億ものWebページを対象にした検索を扱っている。先ほどの計算では、このデータに対する問い合わせには少なくとも8.5秒かかる。これは私たちが知っているGoogleとは違う。GoogleのWeb検索には8.5秒もかからないし、Googleでは特定のページがインデックスに含まれているか以上に複雑な問い合わせを実行している。本書執筆時点で、Googleは1秒間に約$4,500$クエリを受け付ける。つまり、少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要だ。

\paragraph{解決策：}
以上の例からは、安直な実装のデータ構造だと、要素数#n#とデータ構造に対する操作数$m$が共に大きくなったときに性能が追いつかなくなることがわかる。これらの例の実行にかかる時間は、機械命令の数にしておよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを上手に並べ、各操作のたびに全要素を扱わないようにすることだ。一見すると不可能に思えるかもしれないが、要素がどれだけ多くても平均して2つの要素だけを参照すれば探していたデータが見つかるというデータ構造をのちに紹介する。毎秒10億回の命令を実行できるとして、10億個の要素、あるいは兆、京、垓におよぶ数の要素が含まれていても、検索にわずか$0.000000002$秒しかかからないのだ。

要素を整列して保持するデータ構造についても紹介する。このデータ構造では、何らかの操作の実行中に参照される要素の数が、データ構造に格納されている要素数に対する関数として見たときに非常にゆっくりとしか増えない。例えば、どんな操作であれ実行中に最大で60個のアイテムしか参照しないですむように、このデータ構造を整列された状態に維持できる。毎秒10億回の命令を実行できるコンピュータであれば、このデータ構造に対する操作がほんの$0.00000006$秒のうちに実行できることになる。

この章の残りの部分では、この本を通して使う主な概念の一部を簡単に解説する。\secref{interface}については、この本で説明するデータ構造で実装するインターフェースをすべて説明するので、必ず読んでほしい。残りの節では以下の内容を説明する。
\begin{itemize}
\item 指数・対数・階乗関数や漸近（ビッグオー）記法・確率・ランダム化などの数学の復習
\item 計算のモデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの内容については、背景知識がある人もない人も、いったん読み飛ばしてから必要に応じて読み直してもらえばよい。

\section{インターフェース}
\seclabel{interface}
データ構造について議論するときは、データ構造のインターフェースとその実装との違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がそれをどのようにやるかを表現する。

% TALK caprice 抽象データ型 (英：abstract data types)というふうに固有名詞であることを日英表記で強調したほうがよいのではないか。この本は固有名詞を太字にしないきらいがある。初学者は「抽象的な」「データの」型って？と形容詞のように読んでしまうかもしれない
\emph{インターフェース}\ejindex{interface}{いんたーふぇーす@インターフェース}\ejindex{abstract data type|see{interface}}{ちゅうしょうでーたがた@抽象データ型}（\emph{抽象データ型}と呼ばれることもある）は、データ構造がサポートする操作一式とその意味を定義する。インターフェースを見ても操作がどう実装されているかはわからない。サポートする操作の一覧とその引数、返り値の特徴だけを教えてくれる。 % types of arguments -> 型だけでなく色々な特徴がある

一方でデータ構造の\emph{実装}には、データ構造の内部表現と、実際に操作を行うアルゴリズムの定義を含む。そのため、ひとつのインターフェースに対する複数の実装がありうるる。例えば本書では\chapref{arrays}では配列を使って、\chapref{linkedlists}ではポインタを使って#List#インターフェースを実装してみせる。いずれも#List#インターフェースだが、実装方法は違っているのだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは要素を追加したり、特定のルールに従い次の要素を削除したりできる要素の集まりを表す。より正確にいうと、#Queue#インターフェースに実行できる操作は次のものである。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する。
  \item #remove()#：（以前に追加された）「次の値」#y#を#Queue#から削除し、#y#を返す。
\end{itemize}

#remove()#には引数がないことに注意する。#Queue#は\emph{取り出し規則}に従って次に削除する要素を決める。色々な取り出し規則がありうるが、主なものとしてFIFOやLIFO、優先度などがある。 % queueing discipline = 取り出し規則

\figref{queue}に示す\emph{FIFO（first-in-first-out、先入れ先出し）キュー}\ejindex{FIFO queue}{FIFOキュー} \ejindex{queue!FIFO}{きゅー@キュー!さきいれさきだし@先入れ先出し}では、追加したのと同じ順番で要素を削除する。これはコンビニのレジで並ぶ列と同じようなものだ。これは最も一般的な#Queue#なので、FIFOを付けずに単に#Queue#というときは普通このデータ構造のことを指す。FIFOキューの#add(x)#、#remove()#を、それぞれ#enqueue(x)#、#dequeue()#と呼ぶ流儀の教科書もある。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption{FIFOキュー}
  \figlabel{queue}
\end{figure}

% TALK caprice 同点要素は「同じ優先度を持つ要素」に言い換えた、一般的な用語ではないので。
\figref{prioqueue}に示す\emph{優先度付きキュー}では、
\ejindex{priority queue}{ゆうせんどつききゅー@優先度付きキュー}%
\ejindex{priority queue|seealso{heap}}{ゆうせんどつききゅー@優先度付きキュー|seealso{ヒープ}}%
\ejindex{queue!priority}{きゅー@キュー!ゆうせんどつき@優先度付き}%
#Queue#から要素を削除するとき最小のものを削除する。同じ優先度を持つ要素が複数あるときは、そのうちのどちらを削除してもよい。これは病院の救急室で重症患者を優先的に治療することに似ている。患者が到着するとまずはその症状の深刻さを見定め、待合室で待機してもらう。医師の手が空くと、最も重篤な患者から治療する。優先度付きキューにおける#remove()#操作を#deleteMin()#呼ぶ流儀の教科書もある。 % TODO: YJ I have never seen deleteMin though.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption{優先度付きキュー}
  \figlabel{prioqueue}
\end{figure}

他によく使う取り出し規則は\figref{stack}に示すLIFO（last-in-first-out、後入れ先出し）
\ejindex{LIFO queue}{LIFOキュー}%
\ejindex{LIFO queue|seealso{stack}}{LIFOキュー|seealso{スタック}}%
\ejindex{queue!LIFO}{きゅー@キュー!あといれさきだし@先入れ後出し}%
\ejindex{stack}{すたっく@スタック}%
だ。 \emph{LIFOキュー}では、最後に追加された要素が次に削除される。これは皿を積むように視覚化するとよい。積み上げられた皿をひとつずつ取るとき、皿は上から順に持って行かれる。この構造はとてもよく見かけるので#Stack#（スタック）という名前が付いている。#Stack#について話すときには#add(x)#、#remove()#のことを#push(x)#、#pop()#と呼ぶ。こうすればLIFO・FIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption{LIFOキュー（スタック）}
  \figlabel{stack}
\end{figure}

#Deque#（双方向キュー）はFIFOキューとLIFOキュー（スタック）の一般化だ。 #Deque#は先頭と末尾のある要素の列を表し、列の先頭または末尾に要素を追加できる。 #Deque#操作の名前はわかりやすく、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#だ。スタックは#addFirst()#と#removeFirst()#だけを使えば実装できることは知っておくと良いだろう。一方#addLast(x)#と#removeFirst()#だけを使えばFIFOキューになる。 % back = 末尾?

\subsection{#List#インターフェース：線形シーケンス}

この本に(FIFO-)#Queue#や#Stack#、#Deque#のインターフェースの話はあまり出てこない。なぜならこれらのインターフェースは#List#インターフェースにまとめられるからだ。\figref{list}に示す#List#\ejindex{List@#List#}{りすと@リスト}は、値の列$#x#_0,\ldots,#x#_{#n#-1}$を表現する。
#List#インターフェースは以下の操作を含む。

\begin{enumerate}
  \item #size()#: リストの長さ#n#を返す。
  \item #get(i)#: $#x#_{#i#}$の値を返す。
  \item #set(i,x)#: $#x#_{#i#}$の値を#x#にする。
  \item #add(i,x)#: #x#を#i#番目\footnote{コンピュータサイエンスでは序数を0からはじめることがある。例えばここで配列の#i#番目の要素と言っているのは、はじめから数えて$i+1$個目の要素のことである。}として追加し、$#x#_{#i#},\ldots,#x#_{#n#-1}$を後ろにずらす。\\
    すなわち、$j\in\{#i#,\ldots,#n#-1\}$について$#x#_{j+1}=#x#_j$とし、#n#をひとつ増やし、$#x#_i=#x#$とする。
  \item #remove(i)#: $#x#_{#i#}$を削除し、$#x#_{#i+1#},\ldots,#x#_{#n#-1}$を前にずらす。\\ 
    すなわち、$j\in\{#i#,\ldots,#n#-2\}$について$#x#_{j}=#x#_{j+1}$とし、#n#をひとつ減らす。
\end{enumerate}

これらの操作を使って#Deque#インターフェースを実装できる。 % sufficient -> 十分という意味

\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption{#List#は$0,1,2,\ldots,#n#-1$で添え字づけられた列を表現する。この#List#で#get(2)#を実行すると値$c$が返ってくる。}
  \figlabel{list}
\end{figure}


この後の章では(FIFO-)#Queue#、#Stack#、#Deque#のインターフェースについての話はほぼ出てこない。しかし、#Stack#と#Deque#という用語を「#List#インターフェースを実装したデータ構造」の名前として後の章で使うことがある。その場合、#Stack#と#Deque#という名前のついたそれらのデータ構造は、それぞれ#Stack#と#Deque#のインターフェースを非常に効率良く実装することにも使える、という事実を強調している。たとえば#ArrayDeque#は#List#インターフェースの実装であると同時に#Deque#の実装でもあり、#Deque#の操作をいずれも定数時間で実行できる。\footnote{実行時間についてはこの章の後半で説明するが、「定数時間で実行できる」とは要素がいくつあっても一定の時間で実行できるということであり、非常に効率がよいことを表す。}
% caprice unordered って入れたほうが USetのUが楽に理解できる
\subsection{#USet#インターフェース：順序付けられていない要素の集まり}

#USet#\index{USet@#USet#}インターフェースは重複がなく、順序付けられていない(unordered、#USet#のUはこの頭文字)要素の集まりを表現する。これは数学における\emph{集合}のようなものだ。#USet#には、#n#個の\emph{互いに相異なる}要素が含まれる。つまり、同じ要素が複数入っていることはない。また、要素の並び順は決まっていない。#USet#には以下の操作を実行できる。

\begin{enumerate}
\item #size()#：集合の要素数#n#を返す。
\item #add(x)#：要素#x#が集合に入っていなければ集合に追加する。\\
$#x# = #y#$を満たす集合の要素#y#が存在しないなら、集合に#x#を加える。#x#が集合に追加されたら#true#を返し、そうでなければ#false#を返す。
\item #remove(x)#：集合から#x#を削除する。\\
$#x# = #y#$を満たす集合の要素#y#を探し、集合から取り除く。そのような要素が見つかれば#y#を、見つからなければ#null#\footnote{訳注：#null#とは何もないことを示す記号である。}を返す。
\item #find(x)#：集合に#x#が入っていればそれを見つける。\\
$#x# = #y#$を満たす集合の要素#y#を見つける。そのような要素が見つかれば#y#を、見つからなければ#null#を返す。
\end{enumerate}

上の定義で探したい#x#と見つかる（かもしれない）要素#y#とをわざわざ区別する必要がないように感じるかもしれない。
これを区別する理由は、別のもの（オブジェクト）である#x#と#y#とを何らかの基準で等しいと判定したくなることがあるからだ。\javaonly{\footnote{Javaでは、クラスの#equals(y)#・#hashCode()#メソッドをオーバーライドするとこれを行える。}}。これができるとキーを値に対応づけるインターフェースである\emph{辞書}(\emph{マップ})を実装するのに便利なのだ。   % TODO: YJ need better translation
\ejindex{dictionary}{じしょ@辞書}%
\ejindex{map}{まっぷ@マップ}%
% TODO 辞書、マップとハッシュテーブルの関わりを脚注で補足する

辞書（マップ）を作るために、まずは#Pair#\ejindex{pair}{ぺあ@ペア}という\emph{キー}と\emph{値}がペアになったオブジェクトを作る。 2つの#Pair#はキーが等しいければ（その値が等しいかどうかに関わらず）等しいとみなされる。#Pair#である$(#k#,#v#)$を#USet#に入れてから、$#x#=(#k#,#null#)$として#find(x)#を実行すると$#y#=(#k#,#v#)$が返ってくる。すなわち、キー#k#だけから値#v#が手に入るのだ。

\subsection{#SSet#インターフェース：ソートされた(sorted)要素の集まり}
\seclabel{sset}
% TODO caprice 全順序の脚注を入れる
\index{SSet@#SSet#}%
#SSet#インターフェースは順序づけされた要素の集まりを表現する。#SSet#には全順序な集合の要素が入る。すなわち任意の2つの要素#x#と#y#とを大小比較できる。サンプルコードでは、以下のように定義される#compare(x, y)#メソッドで比較を行うものとする。

\[
    #compare(x,y)#
      \begin{cases}
        {}<0 & \text{if $#x#<#y#$} \\
        {}>0 & \text{if $#x#>#y#$} \\
        {}=0 & \text{if $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%

#SSet#は#USet#と全く同じセマンティクスを持つ操作#size()#、#add(x)#、#remove(x)#をサポートする。#USet#と#SSet#の違いは#find(x)#にある。 % セマンティクス vs. 意味

\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: 順序づけられた集合から#x#の位置を特定する。\\
   すなわち$#y# \ge #x#$を満たす最小の要素#y#を見つける。
   もしこのような#y#が存在すればそれを返し、そうでないなら#null#を返す。
\end{enumerate}

この#find(x)#は、\emph{後継探索(XXX:訳語)}\ejindex{successor search}{こうけいたんさく@後継探索}と呼ばれることがある。#x#に等しい要素がなくても意味のある結果を返す点で#USet#の#find(x)#とは異なる。

#USet#、#SSet#における#find(x)#の区別は重要なのだが見過ごされがちである。 #SSet#は#USet#より多くの仕事をしてくれるぶん、実装が複雑で実行時間が長くなりがちだ。例えば、この本で述べる#SSet#の#find(x)#の実装では、要素数に対してその対数程度の時間がかかる。一方、\chapref{hashing}の#ChainedHashTable#による#USet#の実装では、#find(x)#の実行時間の期待値は定数である。#USet#にはなく#SSet#にある機能が必要ないときは#SSet#ではなく#USet#を使う方がよいだろう。

\section{数学的背景}
この節では本書で使う数学の記法や基礎知識を復習する。例えば対数やビッグオー記法、確率論などについて説明する。知っておいてほしい項目をまとめるに留め、丁寧な手ほどきはしない。背景知識が足りないと感じた読者はコンピュータサイエンスで使う数学の良い（無料の）教科書を読んでほしい。必要に応じて適切な箇所を読み、練習問題を解いてみるとよいだろう。
\cite{llm11}.

\subsection{指数と対数}

\ejindex{exponential}{しすうかんすう@指数関数}
$b^x$と書いて$b$の$x$乗を表す。$x$が正の整数なら、$b$にそれ自身を$x-1$回掛けた値になる。

\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x}
\]

$ x $が負の整数なら、$b^x=1/b^{-x}$である。$x=0$なら、$b^x=1$である。
$b$が整数でないときも、（後述する）$e^x$を使ってべき乗を定義できる。この関数は指数級数を使って定義される。こういう話をもっと知りたい人は微積分学の教科書を読んでほしい。

\ejindex{logarithm}{たいすうかんすう@対数関数}
この本では$\log_b k$と書いて\emph{$b$を底とする対数}を表す。これは次の式を満たす$x$として一意に決まる、

\[
    b^{x} = k
\]

この本に出てくる対数の底はほとんどの場合2である。底が2の対数を\emph{二進対数}という。そのため、底になにも書かない$\log k$は$\log_2 k$の省略記法とする。
\ejindex{binary logarithm}{にしんたいすう@二進対数}%
\ejindex{logarithm!binary}{たいすう@対数!にしん@二進}%
% TALK caprice 二分探索（英：binary search）と強調したい
対数の大雑把だが分かりやすいイメージを紹介する。$\log_b k$とは$k$を何回$b$で割ると1以下になるかを表す数だと考えればよい。例を挙げよう。二分探索という手法を使うと、一回比較を行うたび答えの候補の個数が半分になる。そして答えの候補が1つに絞られるまでこれを繰り返す。最初に$n+1$個の答えの候補があるなら、二分検索において必要な比較の回数は$\lceil \log_2(n+1) \rceil$以下だ。
% TODO caprice ガウス記号の脚注をココにいれる
\footnote{訳注：$x$を実数とするとき、$\lceil x \rceil$は$x$以上の最小の整数である。$\floor x \rfloor$は$x$以下の最大の整数である。}

\ejindex{natural logarithm}{しぜんたいすう@自然対数}%
\ejindex{logarithm!natural}{たいすう@対数!しぜん@自然}%
\emph{自然対数}という別の対数もしばしば使う。$\ln k$と書いて$\log_e k$を表すことにする。ここで、$e$は次のように定義される\emph{オイラーの定数}だ。\footnote{訳注：日本ではeのことをネイピア数ということも多い。}
\ejindex{Euler's constant}{おいらーのていすう@オイラーの定数}%
\index{e@$e$ (Euler's constant)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828
\]

自然対数は頻繁に現れる。これは$e$が次のよく現れる積分の値だからだ。
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k
\]

よく使う対数の操作は2つある。ひとつは指数部からの取り出し操作だ。
\[
    b^{\log_b k} = k
\]

もうひとつは底の変換操作だ。
\[
    \log_b k = \frac{\log_a k}{\log_a b}
\]

これらふたつの操作を使うと、例えば自然対数と二進対数とを比較できる。
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} =
    (\ln 2)(\log k) \approx 0.693147\log k
\]

\subsection{階乗}
\seclabel{factorials}

\ejindex{factorials}{かいじょう@階乗}
この本で\emph{階乗関数}を使う箇所がいくつかある。$n$が非負整数のとき、$n!$（ 「$n$の階乗」と読む）は次のように定義される。
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n
\]

$n!$は$n$要素の相異なる順列\ejindex{permutation}{ちかん@置換}の個数である。つまり$n$個の相異なる要素の並べ方の数として階乗は現れる。なお、$n=0$のとき$0!$は1と定義される。

\ejindex{Stirling's Approximation}{すたーりんぐのきんじ@スターリングの近似}%
$n!$の大きさは\emph{スターリングの近似}を使って見積もれる。
\footnote{訳注：以下、スターリングの近似に関する議論は、初学者は飛ばしても良いと思われる。}
\[
  n!
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)}
\]
ここで$\alpha(n)$は次の条件を満たす。
\[
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}
\]

スターリングの近似を使って$\ln(n!)$の近似値も計算できる。
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
% TALK caprice このカッコ内の記述についても「初学者は無視して良い」と脚注入れたい
（スターリングの近似を証明する簡単な方法として、$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$を$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$で近似するやり方がある。)

\ejindex{binomial coefficients}{にこうけいすう@二項係数}%
階乗関数に関連する項目として\emph{二項係数}をここで説明する。$n$を非負整数、$k$を$\{0,\ldots,n\}$の要素とするとき、二項係数$\binom{n}{k}$は次のように定義される。

\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]
二項係数$\binom{n}{k}$は、大きさ$n$の集合における大きさ$k$の部分集合の個数である。すなわち、集合$\{1,\ldots,n\}$から相異なる$k$個の整数を取り出すときの場合の数を表す値と解釈できる。 % TODO: YJ 元の文でもi.e.とあるが、これは定義ではなく一つの例なのでは？すなわち、と言ってしまってよいのだろうか。「例えば」の方が正確では。

\subsection{漸近記法}
\seclabel{asymptotic}

\ejindex{asymptotic notation}{ぜんきんきほう@漸近記法} \ejindex{big-Oh notation}{びっぐおーきほう@ビッグオー記法} \index{O@$O$ notation}
データ構造を分析するとき、操作の実行時間を理解することが重要だ。しかし、正確な実行時間はコンピュータごとに異なる。同じコンピュータで実行する場合ですらバラつくだろう。操作の実行時間とは操作に必要なコンピュータへの命令数のことにする。単純なコードであってもこの数を正確に計算するのは骨が折れる。そのため正確な実行時間を求めるのではなく、\emph{ビッグオー記法}と呼ばれる記法を使って実行時間を見積もる。$f(n)$が関数のとき$O(f(n))$は次の関数の集合を表す。
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{ある$c>0$と$n_0$が存在し、} \\
             \quad\mbox{任意の$n\ge n_0$について$g(n) \le c\cdot f(n)$を満たす}
     \end{array} \right\}
\]
イメージとしては、$n$が十分に大きければ（つまりグラフの十分右の方では）$c\cdot f(n)$が$g(n)$より上にある関数$g(n)$を集めたものがこの集合だ。 % 「上から抑えられる」は既に知識がある人には伝わるが、初学者には伝われないのでは。 % TODO: YJ need revision

漸近表記を使えば関数を単純化できる。たとえば、$5n\log n + 8n - 200$の代わりに$O(n \log n)$と書ける。これは次のように証明できる。
\begin{align*}
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ $n\ge 2$のとき（このとき$\log n \ge 1$）}
            \\
        & \le 13n\log n
\end{align*}
$ c = 13 $および$ n_0 = 2 $とすれば、関数$ f(n)= 5n \log n + 8n-200 $が集合$ O(n \log n)$に含まれることがわかる。

漸近表記の便利な性質をいくつか挙げる。

まずは、任意の定数$c_1 < c_2$について以下が成り立つ。
\[ O(n^{c_1}) \subset O(n^{c_2}) \]

つづいて、任意の定数$ a, b, c> 0 $について以下が成り立つ。
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \]

これらの包含関係はそれぞれに正の値を掛けても保たれる。
たとえば$n$を掛けると次のようになる。

\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \]

一般的な慣習に従って本書でもビッグオー記法を濫用する。すなわち$f_1(n) = O(f(n))$ と書いて$f_1(n) \in O(f(n))$であることを表すことにする。また「この操作の実行時間は$O(f(n))$に\emph{含まれる}」ことを単に「この操作の実行時間は$O(f(n))$だ」と言う。
これらの表現を認めると、等式が連なる中で漸近記法を使えたり、語感が整ったりして便利である。

この書き方の、奇妙な例を挙げる。
\[
  T(n) = 2\log n + O(1)
\]

これは正確に書くとこうなる。
\[
  T(n) \le 2\log n + [\mbox{$O(1)$のある要素]}
\]

$O(1)$には別の問題もある。この記法には変数が入ってないので、どの変数が大きくなるのかわからないのだ。これは文脈から読み取る必要がある。上の例では、方程式の中に変数は$n$しかないので、$T(n)= 2 \log n + O(f(n))$の$f(n) = 1$であるものと読み取ることになる。

ビッグオー記法は新しい記法でも、コンピュータサイエンス独自のものでもない。1894年には数学者Paul Bachmannがこの記法を使っていた。その後しばらくしてコンピュータサイエンスにおいてアルゴリズムの実行時間を論ずるのにこの記法が非常に便利なことがわかったのだ。
次のコードを考えてみよう。

\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}

この関数を1回実行すると以下の処理が行われる。
\begin{itemize}
      \item 代入$1$回 (#int\, i\, =\, 0#)
      \item 比較$#n#+1$回 (#i < n#)
      \item インクリメント #n# 回(#i++#)
      \item 配列のオフセット計算 #n# 回 (#a[i]#)
      \item 間接代入#n#回 (#a[i] = i#)
\end{itemize}

よって実行時間は以下のようになる。
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n#
\]

$a$、$b$、$c$、$d$、$e$はプログラムを実行するマシンに依存する定数で、それぞれ代入、比較、インクリメント、配列のオフセット計算、間接代入の実行時間を表す。しかしたった2行のコードの実行時間を表す式がこうも複雑なので、より複雑なコードやアルゴリズムはこのやり方では扱えないだろう。ビッグオー記法を使うと次のように実行時間をより単純に表せる。
\[
    T(#n#)= O(#n#)
\]

この書き方はよりコンパクトだが、さっきの式と同じくらいのことを表している。正確な実行時間は定数$a$、$b$、$c$、$d$、$e$に依存している。これらの値がすべてわからないと、正確な実行時間はわからないのだ。これらの定数を明らかにするなんらかの努力（例えば実際に時間を測ってみる）をしても、得られる結論はそのマシンでしか有効でない。

ビッグオー記法を使えばより抽象的な分析ができ、より複雑な関数も扱える。ふたつのアルゴリズムの実行時間がビッグオー記法で書いたときに同じなら、どちらが速いのかはっきりとした勝ち負けがつかないかもしれない。あるマシンでは一方が速く、別のマシンでは他方が速いかもしれない。しかしふたつのアルゴリズムの実行時間がビッグオー記法で書いた時に異なるとわかれば、実行時間が小さい方は\emph{#n#が十分大きければ}どのようなマシンにおいても速いといえる。

ビッグオー記法を使って2つの異なる関数を比べる例を\figref{intro-asymptotics}示す。これは$f_1(#n#)=15#n#$と$f_2(n)=2#n#\log#n#$のグラフである。$f_1(#n#)$は複雑な線形時間アルゴリズムの実行時間を、$f_2(#n#)$は分割統治に基づくシンプルなアルゴリズムの実行時間を表している。これを見ると#n#が小さいうちは$f_1(#n#)$は$f_2(#n#)$より大きいが、#n#が大きくなると大小関係は逆転することがわかる。つまりnが十分大きいなら実行時間が$f_1(#n#)$であるアルゴリズムの方が圧倒的に性能がよいのだ。ビッグオー記法の式$O(#n#)\subset O(#n#\log #n#)$はこの事実を示している。

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{$15#n#$と$2#n#\log#n#$の比較}
  \figlabel{intro-asymptotics}
\end{figure}

多変数関数に対して漸近表記を使うこともある。標準的な定義は定まっていないようだが、この本では次の定義を用いる。

\[
   O(f(n_1,\ldots,n_k)) =
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{ある$c>0$と$z$が存在し、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k)\ge z$を満たす任意の$n_1,\ldots,n_k$について、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$が成り立つ。} \\
   \end{array}\right\}
\]

この定義を使えば我々が考えたいことを表現することが出来る。引数$n_1,\ldots,n_k$が$g$を大きくするときのことだ。この定義は$f(n)$が$n$の増加関数なら一変数の場合の$O(f(n))$の定義と同じだ。我々の目的のためにはこの定義で十分だが、教科書によっては多変数の場合の漸近記法を別の意味で定義していることには注意が必要だ。
% TALK 原文がrandomizationなのでランダム性ではなく乱択化では？
\subsection{ランダム性と確率}
\seclabel{randomization}

\ejindex{randomization}{らんたくか@乱択化}%
\ejindex{probability}{かくりつ@確率}%
\ejindex{randomized data structure}{らんたくでーたこうぞう@乱択データ構造}%
\ejindex{randomized algorithm}{らんたくあるごりずむ@乱択アルゴリズム}%
この本で扱うデータ構造には\emph{ランダム性}を利用するものがある。格納されているデータや実行する操作に加えて、サイコロの出目もふまえて実際の処理を決めるのだ。そのため同じことをしても実行時間が毎回同じとは限らない。こういうデータ構造を分析するときは\emph{期待実行時間}を考えるのがよい。
\ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
\ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%

形式的には、ランダム性を利用するデータ構造における操作の実行時間は確率変数である。そしてその\emph{期待値}を知りたい。全事象$U$の値をとる離散確率変数を$X$とするとき、$X$の期待値$E[X]$は次のように定義される。
\ejindex{expected value}{きたいち@期待値}%
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\}
\]

ここで、$\Pr\{\mathcal{E}\}$は事象$\mathcal{E}$の発生確率とする。この本の例では、データ構造の内部で発生するランダム性のみを考慮して確率を定める。データ構造に入ってくるデータや実行される操作列がランダムだとは仮定しないことには注意する。

期待値の最も重要な性質のひとつは\emph{期待値の線形性}である。
\ejindex{linearity of expectation}{きたいちのせんけいせい@期待値の線形性}%
任意のふたつの確率変数$X$と$Y$について次の式が成り立つ。
\[
   \E[X+Y] = \E[X] + \E[Y]
\]

より一般的には、任意の確率変数$ X_1,\ldots,X_k $について次の関係が成り立つ。
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i]
\]

期待値の線形性によって、（上の式の左辺のように）複雑な確率変数の期待値を、（右辺のような）より単純な確率変数の期待値に分解できる。

\emph{インジケータ確率変数}\ejindex{indicator random variable}{いんじけーたかくりつへんすう@インジケータ確率変数}はよく使う便利なトリックだ。この二値変数はなにかを数えるときに役立つ。例を見るとよくわかるだろう。表と裏が等しい確率で出るコインを$k$回投げたとき、表が出る回数の期待値を知りたいとする。
\ejindex{coin toss}{こいんなげ@コイン投げ}
直感的な答えは$k/2$だが、これを期待値の定義を使って証明すると次のようになる。

\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2
\end{align*}
% この「２項係数の性質」、原文自体が公式間違ってたのでプルリク送った上でここでも直しといた
この計算をするには、$\Pr\{X=i\} = \binom{k}{i}/2^k$および2項係数の性質$i\binom{k}{i}=k\binom{k-1}{i-1}$や$\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$を知っている必要がある。
% TALK インジケータ確率変数（英：indicator random variables）としたい
インジケータ変数と期待値の線形性を使えばはるかに簡単になる。$\{1,\ldots,k\}$の各$i$に対し以下のインジケータの確率変数を定義する。

\[
    I_i = \begin{cases}
           1 & \text{$i$番目のコイントスの結果が表のとき} \\
           0 & \text{そうでないとき}
          \end{cases}
\]
そして、$I_i$の期待値を計算する。
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \]
ここで、$X=\sum_{i=1}^k I_i$なので次のように所望の値が得られる。
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2
\end{align*}

この計算は少し長いものの確率の不思議な等式は使っていない。また各コイントスは$1/2$の確率で表が出るので、表は試行回数の半分くらい出るだろうという直感の説明にもなっている。

\section{計算モデル}
\seclabel{model}

本書ではデータ構造における操作の実行時間を理論的に分析する。これを正確に行うための計算の数学的なモデルが必要だ。そのために\emph{#w#ビットのワードRAM}モデルを使うことにする。
\ejindex{word-RAM}{わーどらむ@ワードRAM}%
\index{RAM}%
RAMはランダムアクセスマシン(Random Access Machine)の頭字語である。
このモデルではランダムアクセスメモリを使える。
ランダムアクセスメモリはセルの集まりで、これはそれぞれ#w#ビットのワードを格納できる。
\ejindex{word}{わーど@ワード}
つまり、各セルはw桁の２進数の集合$\{0,\ldots,2^{#w#}-1\}$のうちのいずれかひとつを表せる。
% TODO 剰余の脚注
ワードRAMモデルではワードの基本的な操作に一定の時間が必要である。基本的な操作とは算術演算（#+#, #-#, #*#, #/#, #%#）や比較（$<$, $>$, $=$, $\le$, $\ge$）、ビット単位の論理演算（ビット単位の論理積 ANDや論理和 OR、排他的論理和 XOR）である。

どのセルも一定の時間で読み書きできる。コンピュータのメモリはメモリ管理システムによって管理される。メモリ管理システムは必要に応じてメモリブロックを割り当てる、またメモリブロックの割り当てを解除する。サイズ$k$のメモリブロックの割当てには$O(k)$の時間がかかり、新しく割り当てられたメモリブロックへの参照（ポインタ）が返される。この参照はひとつのワードに収まるビットで表現できるとする。

ワード幅#w#はこのモデルの重要なパラメータである。
この本で#w#に置く仮定は、#n#をデータ構造に格納されうる要素数とするとき、$#w# > \log #n#$であるということだけだ。これは控えめな仮定である。なぜならこれが成り立たないとひとつのワードではデータ構造の要素数を表すことすらできないためである。

メモリ使用量はワード単位で測るので、データ構造のメモリ使用量とはデータ構造の使うワード数である。我々のデータ構造はみなある型#T#の値を格納し、#T#型の要素は1ワードのメモリで表現できると仮定する。
\javaonly{（実際に、Javaでは#T#型のオブジェクトの参照を格納しており、この参照は1ワードのメモリを占める。）}

\javaonly{#w#ビットのワードRAMモデルは、$#w#=32$とすると、（32ビット）Java仮想マシン（JVM）によく似ている。}
\cpponly{#w#ビットのワードRAMモデルは、$#w#=32$または$#w#=64$とすると、現代のデスクトップコンピュータ環境によく似ている。}
すなわちこの本に載っているデータ構造はいずれも一般的なコンピュータ上で動作するように実装できる。

\section{正しさ、時間複雑性、空間複雑性}

データ構造の性能を考えるとき重要な項目が3つある。
% TALK caprice ここも英語を加えたい
\begin{description}
  \item[正しさ：]データ構造はそのインタフェースを正しく実装しなければならない。
  \item[時間複雑性：]データ構造における操作の実行時間は短いほどよい。
  \item[空間複雑性：]データ構造のメモリ使用量は小さいほどよい。
\end{description}

この本は入門書なので正しいデータ構造のみを扱う。つまり、不正確な出力が得られることがあったり、更新をちゃんとしなかったりするデータ構造のことは考えない。
一方で、メモリ使用量を小さくに抑える工夫をするデータ構造は紹介する。
紹介する工夫の多くは操作の（漸近的な）実行時間には影響しないが、実際にはデータ構造を少し遅くするかもしれない。

データ構造の実行時間は次の三種類のいずれかを考えることが多い。

\begin{description}
\item[最悪実行時間：]
  \ejindex{running time}{じっこうじかん@実行時間}%
  \ejindex{running time!worst-case}{じっこうじかん@実行時間!さいあく@最悪}%
  \ejindex{worst-case running time}{さいあくじっこうじかん@最悪実行時間}%
  これは最も強力な実行時間の保証である。操作の最悪実行時間が$f(#n#)$ならば、操作の実行時間は\emph{決して}$f(#n#)$よりも長いことはない。
\item[償却実行時間：]
  % XXX: 原文には at most がついているが不要ではないか。O(f(n))と混同している？
  \ejindex{running time!amortized}{じっこうじかん@実行時間!しょうきゃく@償却}%
  \ejindex{amortized running time}{しょうきゃくじっこうじかん@償却実行時間}%
  償却実行時間が$f(#n#)$であるとは、典型的な操作のコストが$f(#n#)$であることを意味する。
  より正確には、$m$個の操作の列の実行時間の合計が$mf(#n#)$であることを意味する。
  いくつかの操作には$f(#n#)$より長い時間がかかるかもしれないが、操作の列全体として考えればひとつあたりの実行時間は$f(#n#)$なのである。 % TODO: YJ amortize = 償却: better translation?
\item[期待実行時間：]
  \ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%
  \ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
  期待実行時間が$f(#n#)$ならば、実行時間は確率変数（\secref{randomization}を参照）であり、この確率変数の期待値は$f(#n#)$である。
  なお期待値を計算する際に考えるランダム性は、データ構造の内部で行う選択のランダム性である。
\end{description}

最悪、償却、期待実行時間の違いを理解するのには、お金の例え話が役に立つ。家を買う費用のことを考えてみよう。 % finance

\paragraph{最悪コストと償却コスト}
\ejindex{amortized cost}{しょうきゃくこすと@償却コスト}%
家の価格が12万ドルだとする。毎月1200ドルの120ヶ月（10年）の住宅ローンでこの家が手に入るかもしれない。この場合、月額費用は最悪でも月1200ドルだ。

十分な現金を持っていれば12万ドルの一括払いで家を買うこともできる。こうするとこの家を購入代金を10年で償却した月額費用は以下のようになる。
\[
   \$120\,000 / 120\text{ヶ月} = \$1\,000\text{月あたり}
\]

これはローンの場合に支払う月額1200ドルよりだいぶ少ない。

\paragraph{最悪コストと期待コスト}
次に、12万ドルの家における火災保険を考えてみよう。保険会社が何十万件もの事例を調べた結果、大多数の家では火事を起こさず、わずかな数の家では煙による被害程度で済むボヤを起こり、ごく少数の家では全焼被害になることがわかった。保険会社はこの情報に基づき、12万ドルの家における火災被害額の期待値は月額10ドル相当であると判断し、儲けるために火災保険としては月額15ドルの料金設定を行なった。

決断のときだ。15ドルを最悪支払月額（かつ期待支払月額）とするその会社の火災保険に入るべきだろうか？それとも、いちかばちか、期待支払月額である月額10ドルを自分で貯める自家保険を行うことにして
\footnote{訳注：なぜ被保険者側が、保険会社しか知らないはずのその額を知っているのかは不問とする。この例の目的は現実の近似ではなく、異なる種類のコストに対する感覚を掴むことである。}
、月額5ドルの節約を選ぶべきだろうか？期待支払月額としては明らかに自家保険の方が安いが、実際支払月額が遥かに高くなる可能性を考慮しなければならない。すなわち自家保険では、低い確率ではあるが、家が全焼して実際支払月額が最悪支払月額である12万ドルになる可能性があるのだ。

この例は、我々がどちらを選ぶかは場合によって変わるのだ、と教えてくれる
\footnote{訳注：この例では、最悪支払月額の上限が大幅に低くなること、また支払月額の差が５ドルと比較的少額であることを考慮して、火災保険に入ることを選ぶ人が多いかもしれない。しかし驚くべきことに\emph{データ構造の世界では、最悪実行時間よりも償却・期待実行時間が低いことを優先する}ことの方が遥かに多い。というのも、最悪実行時間だけかかったときの損害が全焼ほど大きくはなく、またその確率も全焼よりはるかに小さく制御できることが多いからだ。}
。
償却・期待実行時間は最悪実行時間よりも小さいことが多い。最悪実行時間の長さに目をつむり、償却・期待実行時間でいえば短いからと妥協することにすれば、はるかに単純なデータ構造を採用できる場合がよくあるのだ。

\section{コードサンプル}
\pcodeonly{
この本のコードサンプルは擬似コードで書いた。
\ejindex{pseudocode}{ぎじこーど@擬似コード}%
ここ40年のどの一般的なプログラミング言語の経験を持つ人も理解しやすいコードを書いたつもりである。
この本におけるコードがどんなものかは、配列#a#の平均値を計算する次の擬似コードを見てみるとわかるだろう。
\pcodeimport{ods/Algorithms.average(a)}
このコードでは、変数への代入を表す記法は$\gets$である。
% WARNING: graphic typesetting of assignment operator
配列#a#の大きさを#len(a)#と書き、配列の添え字は0からはじまることにする。
このとき、#range(len(a))#は#a#の正しい添え字の集まりである。
コードを短くし、また読みやすくするために、部分配列代入を使うことがある。
次のふたつの関数は同じことをしている。
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
続くコードは配列のすべての要素を0にするものである。
\pcodeimport{ods/Algorithms.zero(a)}
このようなコードの実行時間を解析するときは#a[0:len(a)] = 1#や、#a[1:len(a)] = a[0:len(a)-1]#のような文は、定数時間では実行できないことに気をつけなければならない。
これらの実行時間は$O(#len(a)#)$である。

変数の代入においても同様の簡略記法を使う
#x,y=0,1#は、#x#を0に#y#を1にする。
また#x,y = y,x#は変数#x#と#y#の値を入れ替える。
\ejindex{swap}{すわっぷ@スワップ}

この本の擬似コードには、親しみがないかもしれない記法も小数だが使われている。
数学における（ふつうの）割り算の演算子を$/$と書く。
整数の除算を使う必要のある場面も多く、これを$#//#$演算子で表す。
$#a//b# = \lfloor a/b\rfloor$は、$a/b$の整数部分である。
そのため例えば、$3/2=1.5$だが、$#3//2# = 1$である。
\ejindex{integer division}{せいすうじょさん@整数除算}%
\ejindex{div operator}{じょさんえんざんし@除算演算子}%
整数除算における余りを計算する$\bmod$演算子を使うこともあるが、これはその際になってから定義することにする。
\ejindex{mod operator}{じょうよえんざんし@剰余演算子}%
ビット単位の演算子を使うこともある。
具体的には、左シフト（#<<#）、右シフト（#>>#）、ビット単位の論理積（#&#）、そしてビット単位の排他的論理和（#^#）である。
\ejindex{left shift}{ひだりしふと@左シフト}%
\ejindex{#<<#|see {left shift}}{#<<#|see{ひだりしふと@左シフト}}%
\ejindex{right shift}{みぎしふと@右シフト}%
\ejindex{#>>#|see {right shift}}{#>>#|see {みぎしふと@右シフト}}%
\ejindex{bitwise and}{びっとたんいろんりせき@ビット単位論理積}%
\ejindex{#&#|see {bitwise and}}{#&#|see {びっとたんいろんりせき@ビット単位論理積}}%
\ejindex{#^#|see {bitwise exclusive-or}}{#^#|see {びっとたんいろんりせき@ビット単位論理積}}%

この本の擬似コードはPythonのコードから機械的に翻訳したものである。
元となったコードは本のウェブサイトからダウンロードできる。
\footnote{ \url{http://opendatastructures.org}}である。もし擬似コードにおいて曖昧な点があれば、対応するPythonのコードを参照することができる。もしPythonが読めなければ、JavaとC++で書かれたコードもある。擬似コードの意味がわからず、PythonもC++もJavaも読めないなら、まだこの本を読むのは早いかもしれない。}

\notpcode{
この本のコードサンプルは\lang{}で書いた。
しかし、\lang{}に親しみのない人も読めるようシンプルに書いたつもりだ。
例えば#public#や#private#は出てこない。オブジェクト指向を前面に押し出すこともない。

B、C、C++、C\#、Objective-C、D、Java、JavaScriptなどをALGOL系の言語を書いたことのある人なら本書のコードの意味はわかるだろう。
完全な実装に興味のある読者はこの本に付属の\lang{}ソースコードを見てほしい。

この本は数学的な実行時間の解析と、対象のアルゴリズムを実装した\lang{}のコードとを共に含む。そのためソースコードと数式で同じ変数が出てくる。
このような変数は同じ書式で書く。
一番よく出てくるのは変数#n#\index{n@#n#}である。
#n#は常にデータ構造に格納されている要素の個数を表す。
}

\section{データ構造の一覧}

表~\ref{tab:summary-i}と表~\ref{tab:summary-ii}は本書で扱うデータ構造における性能の要約である。これらは\secref{interface}で説明した#List#や#USet#、#SSet#を実装する。
\Figref{dependencies}はこの本の各章の依存関係を示している。
\ejindex{dependencies}{いぞんかんけい@依存関係}%
破線の矢印は弱い依存関係を示している。これは章のごく一部の内容や結果のみに依存することを示す。

\begin{table}
\vspace{56pt}
\begin{center}
\resizebox{.98\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#List#の実装} \\ \hline
% XXX: この本の定義に従うとき、二変数のO記法の中に+1を添える必要はあるか？
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[A]{\emph{償却}実行時間を表す。}
\item[E]{\emph{期待}実行時間を表す。}
\end{tablenotes}
\end{threeparttable}}
\end{center}
\caption{#List#・#USet#の実装の要約}
\tablabel{summary-i}
\end{table}

\begin{table}
\begin{center}
\begin{threeparttable}
\begin{tabular}{|l|l|l|l|} \hline
\multicolumn{4}{|c|}{#SSet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{I} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{I} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{(Priority) #Queue# の実装} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\begin{tablenotes}
\item[I]{このデータ構造は#w#ビット整数のみを格納できる}
\javaonly{\item[X]{これは外部メモリモデルでの実行時間である。\chapref{btree}を参照せよ。}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\caption{#SSet#・優先度付き#Queue#の実装の要約}
\tablabel{summary-ii}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{この本の内容の依存関係}
  \figlabel{dependencies}
\end{figure}

\section{ディスカッションと練習問題}

\secref{interface}で説明した#List#・#USet#・#SSet#インターフェースは、 Java Collections Framework\cite{oracle_collections}の影響を受けている。
\index{Java Collections Framework}%
これらはJava Collections Frameworkの#List#・#Set#・#Map#・#SortedSet#・#SortedMap#をシンプルにしたものである。
\javaonly{付属のソースコードは#USet#・#SSet#の実装を#Set#・#Map#・#SortedSet#・#SortedMap#の実装にするためのラッパークラスを含んでいる。}

この章で扱った漸近記法・対数・階乗・スターリングの近似・確率論の基礎などは、Leyman, Leighton, and Meyer\cite{llm11}の素晴らしい（そして無料の）本が扱っている。
分かりやすい微積分の教科書としては、無料で手に入るThompson\cite{t14}の古典的な教科書がある。この本には指数や対数の形式的な定義が書かれている。

基礎的な確率論については、特にコンピュータ・サイエンスに関連するものとしてRoss\cite{r01}の教科書がおすすめである。
漸近記法や確率論などを含むGraham, Knuth, and Patashnik\cite{gkp94}の教科書も参考になるだろう。

\javaonly{Javaプログラミング力を磨きたい読者のためには、オンラインのJavaのチュートリアル~\cite{oracle_tutorials}がある。}

\begin{exc}
練習問題は読者が問題に対する正しいデータ構造を選ぶ練習をするためのものだ。
利用可能な実装やインターフェースがあれば、それを使って解いてみてほしい。
（JavaならばJava Collections Frameworkが、C++ならばStandard Template Libraryがある。）

以下の問題はテキストの入力を一行ずつ読み、各行で適切なデータ構造の操作を実行することで解いてほしい。ただしファイルが百万行であっても数秒以内に処理できる程度に効率的な実装でなければならないものとする。

  \begin{enumerate}
    \item 入力を一行ずつ読み、その逆順で出力せよ。すなわち最後の入力行を最初に書き出し、最後から二番目の入力行を二番目に書き出す、というように出力せよ。

    \item  最初の50行入力を読み、それを逆順で出力せよ。その後続く50行を読み、それを逆順で出力せよ。これを読み取る行が無くなるまで繰り返し、最後に残っていた行（50行未満かもしれない）もやはり逆順で出力せよ。

      つまり、出力は50番目の行からはじまり、49、48、...、1番目の行が続く。
	  この次は100番目の行で、99、...、51番目の行が続く。

	 またプログラム実行中に50より多くの行を保持してはならない。

    \item 入力を一行ずつ読み取り、42行目以降で空行を見つけたら、その42行前の行を出力せよ。例えば、242行目が空行であれば、200行目を出力せよ。
	またプログラム実行中に43行以上の行を保持してはならない。

    \item 入力を一行ずつ読み取り、もしこれまでと重複のない行を見つけたら出力せよ。
	重複がたくさんあるファイルを読む場合にも、重複なく行を保持するのに必要なメモリより多くメモリを使わないように注意せよ。

    \item 入力を一行ずつ読み取り、それがこれまでに読んだことのある行と同じなら出力せよ。（最終的には、入力ファイルからある行がはじめて現れた箇所をそれぞれ除いたものが出力である。）
	ファイルが多くの重複行を含む場合でも、行を重複なく保持するのに必要なメモリよりは多くメモリを使わないように注意せよ。

    \item 入力を全て読み取り、短い順に並び替えて出力せよ。
	同じ長さの行があるときは、それらの行の順序は辞書順に並べるものとする。
	また、重複する行は一度だけ出力するものとする。

    \item 直前の問題で、重複する行は現れた回数だけ出力するように変更した問題を解け。

    \item 入力をすべて読み、全ての偶数番目の行を出力した後に全ての奇数番目の行を出力せよ。（なお、最初の行を0行目と数える。）

    \item 入力をすべて読み、ランダムに並び替えて出力せよ。
	どの行の内容も書き換えてはならない。
	また、入力とくらべて行を減らしたり増やしたりしてもいけない。
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}%
  \emph{Dyck word}とは+1, -1からなる列で、先頭から任意の#k#番目の値までの部分列（プレフィックス）の和がいずれも非負であるものである。
  例えば、$+1,-1,+1,-1$はDyck wordだが、$+1,-1,-1,+1$は$+1-1-1<0$なのでDyck wordではない。
  Dyck wordと#Stack#の#push(x)#・#pop()#操作の関係を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{matched string}{まっちしたもじれつ@マッチした文字列}%
  \ejindex{string!matched}{もじれつ@文字列!マッチした}%
  \emph{マッチした文字列}とは\{, \}, (, ), [, ]のからなる列で、すべての括弧が適切に対応しているものである。
  例えば、「\{\{()[]\}\}」はマッチした文字列だが、「\{\{()]\}」はふたつめの\{に対応する括弧が]であるためマッチした文字列ではない。
  長さ#n#の文字列が与えられたとき、この文字列がマッチしているかを$O(#n#)$で判定するにはスタックをどう使えばよいかを説明せよ。
\end{exc}

\begin{exc}
  #push(x)#・#pop()#操作のみが可能なスタック#s#が与えられる。
  FIFOキュー#q#だけを使って#s#の要素を逆順にする方法を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{Bag@#Bag#}{ばっぐ@バッグ}%
  #USet#を使って#Bag#を実装せよ。
  #Bag#は#USet#によく似たインターフェースである。
  #Bag#は#add(x)#・#remove(x)#・#find(x)#操作をサポートするが、重複する要素も格納するところが異なる。
  #Bag#の#find(x)#操作は#x#に等しい要素が1つ以上含まれているときそのうちのひとつを返す。
  さらに#Bag#は#findAll(x)#操作もサポートする。
  これは #Bag#に含まれる#x#に等しいすべての要素のリストを返す。
\end{exc}

\begin{exc}
  #List#・#USet#・#SSet#インターフェースを実装せよ。
  ただし効率的な実装でなくてもよい。
  ここで実装するものは、後の章で出てくるより効率的な実装の正しさや性能をテストするために役立つ。（最も簡単な方法は要素を配列に入れておく方法だ。）
\end{exc}

\begin{exc}
  直前の問題の実装の性能をアップするための思いつく工夫をいくつか試みよ。
  実験してみて、 #List#の#add(i,x)#・#remove(i)#の性能がどう向上したか考察せよ。
  #USet#・#SSet#の#find(x)#の性能はどうすれば向上しそうか考えてみよ。
  この問題はインターフェースの効率的な実装がどのくらい難しいかを実感するためのものである。
\end{exc}
