\chapter{イントロダクション}
\pagenumbering{arabic}

%世界中のコンピュータサイエンスカリキュラムにデータ構造とアルゴリズムに関するコースが含まれている。データ構造は重要だ。生活の質を上げ、日常生活を効率的にしてくれる。数百万ドル、数十億ドルの企業にはデータ構造を中心に作られたものも多い。
データ構造とアルゴリズムに関する授業は全世界でコンピュータサイエンスの課程に含まれている。データ構造はそれほど重要だ。生活の質を上げるだけでなく、毎日のように人の命さえ救っている。データ構造によって数百万ドル、数十億ドルの規模にまでなった企業も多い。

% どういうものなのだろう？深く考えるのをやめると、データ構造と普段から接していることに気づく。
% How can this be (important)? では?
なぜデータ構造はこんなにも重要なのだろう？考えてみれば私たちは普段からさまざまなデータ構造と接している。

\begin{itemize}
	\item ファイルを開く：ファイルシステム\ejindex{file system}{ふぁいるしすてむ@ファイルシステム}のデータ構造を使って、ファイルをハードディスクなどの上に配置し、検索できる。ハードディスクには数億ものブロックがあり、ファイルの内容はどのブロックに保存されていてもおかしくないので、これは簡単なことではない。
	\item 電話番号を検索する：入力の途中で連絡先リスト\ejindex{contact list}{れんらくさきりすと@連絡先リスト}から電話番号を検索するためにデータ構造が使われている。連絡先リストには膨大な情報（過去に電話や電子メールをやり取りした全員）が含まれている可能性があること、電話端末には高速なプロセッサや潤沢なメモリは搭載されていないことを考えると、これは簡単なことではない。
	\item SNSにログインする：\ejindex{social network}{そーしゃるねっとわーく@ソーシャルネットワーク}ネットワークサーバーではログイン情報からアカウント情報を検索する。利用者が多いSNSには何億人ものアクティブなユーザーがいるので、これは簡単なことではない。
	\item Webページを検索する：\ejindex{web search}{うぇぶけんさく@ウェブ検索}検索エンジンは検索語からWebページを見つけるためにデータ構造を使う。インターネットには85億以上のWebページがあり、それぞれのページに検索対象になりうる語句が大量に含まれているので、これは簡単なことではない。
	\item 緊急通報用番号（9-1-1）に電話する：\ejindex{emergency services}{きんきゅうさーびす@緊急サービス}\index{9-1-1}緊急通報電話では、パトカー、救急車、消防車を速やかに現場に手配できるよう、電話番号と住所を対応付けるためにデータ構造を使う。電話をかけた人は正確な住所を伝えられないかもしれず、この場面での遅れは生死を分かつこともあるので、これは重要な問題だ。
\end{itemize}

\section{効率の必要性}

次節では、よく使うデータ構造に対してどんな操作ができるのかを見ていく。ちょっとしたプログラミング経験があれば、正しい結果を返す操作を実装するのは難しくないだろう。データを配列や連結リストに入れ、すべての要素について順番に処理し、必要なら要素を追加したり削除したりするという実装にすればよい。

この実装は簡単だが、効率がよくない。とはいえ、効率について考える価値はあるだろうか？コンピュータはどんどん速くなっている。簡単な実装で十分かもしれない。それを確認するためにざっくりと計算をしてみよう。

\paragraph{操作の数：}
まあまあの大きさのデータセット、例えば100万（$10^6$）個の要素を持つアプリケーションがあるとする。各要素を少なくとも一回は見たくなるというのは、それなりに妥当な仮定だろう。この場合、少なくとも100万（$10^6$）回、このデータセットから要素を探すことになる。100万回にわたって100万個の要素をすべて確認すると、データを読み出す回数は合計で1兆（$10^6\times 10^6=10^{12}$）回になる。

\paragraph{プロセッサの速度：}
本書執筆時点では、かなり高速なデスクトップコンピュータでも、毎秒10億（$10^9$）回以上の操作は実行できない\footnote{コンピュータの速度はせいぜい数ギガヘルツ（数十億回/秒）であり、各操作にふつうは数サイクルが必要だ。}。よって、このアプリケーションの完了には、少なくとも$10^{12}/10^9=1000$秒、すなわち約16分40秒かかる。コンピュータにとって16分は非常に長い時間だが、人間ならコーヒーブレイクを挟んでそれくらいの時間は待っていられるだろう。

\paragraph{大きなデータセット：}
Google\index{Google}について考えてみよう。Googleでは85億ものWebページを対象にした検索を扱っている。先ほどの計算では、このデータに対する問い合わせには少なくとも8.5秒かかる。これは私たちが知っているGoogleとは違う。GoogleのWeb検索には8.5秒もかからないし、Googleでは特定のページがインデックスに含まれているか以上に複雑な問い合わせを実行している。本書執筆時点で、Googleは1秒間に約$4,500$クエリを受け付ける。つまり、少なくとも$4,500 \times 8.5 = 38,250$ものサーバーが必要だ。

\paragraph{解決策：}
以上の例からは、安直な実装のデータ構造だと、要素数#n#とデータ構造に対する操作数$m$が共に大きくなったときに性能が追いつかなくなることがわかる。これらの例の実行にかかる時間は、機械命令の数にしておよそ$#n#\times m$だ。

解決策はもちろん、データ構造内のデータを上手に並べ、各操作のたびに全要素を扱わないようにすることだ。一見すると不可能に思えるかもしれないが、要素がどれだけ多くても平均して2つの要素だけを参照すれば探していたデータが見つかるというデータ構造をのちに紹介する。毎秒10億回の命令を実行できるとして、10億個の要素、あるいは兆、京、垓におよぶ数の要素が含まれていても、検索にわずか$0.000000002$秒しかかからないのだ。

要素を整列して保持するデータ構造についても紹介する。このデータ構造では、何らかの操作の実行中に参照される要素の数が、データ構造に格納されている要素数に対する関数として見たときに非常にゆっくりとしか増えない。例えば、どんな操作であれ実行中に最大で60個のアイテムしか参照しないで済むように、このデータ構造を整列された状態に維持できる。毎秒10億回の命令を実行できるコンピュータであれば、このデータ構造に対する操作がほんの$0.00000006$秒のうちに実行できることになる。

この章の残りの部分では、この本を通して使う主な概念の一部を簡単に解説する。\secref{interface}については、この本で説明するデータ構造で実装するインターフェースをすべて説明するので、必ず読んでほしい。残りの節では以下の内容を説明する。
\begin{itemize}
\item 指数、対数、階乗関数や漸近（ビッグオー）記法、確率、ランダム化などの数学の復習
\item 計算のモデル
\item 正しさと実行時間、メモリ使用量
\item 残りの章の概要
\item サンプルコードと組版の規則
\end{itemize}
これらの内容については、背景知識がある人もない人も、いったん読み飛ばしてから必要に応じて読み直してもらえばよい。

\section{インターフェース}
\seclabel{interface}
データ構造について議論するときは、データ構造のインターフェースとその実装との違いを理解することが重要だ。インターフェースはデータ構造が何をするかを、実装はデータ構造がそれをどのようにやるかを表現する。

% TALK caprice 抽象データ型 (英：abstract data types)というふうに固有名詞であることを日英表記で強調したほうがよいのではないか。この本は固有名詞を太字にしないきらいがある。初学者は「抽象的な」「データの」型って？と形容詞のように読んでしまうかもしれない
\emph{インターフェース（interface）}\ejindex{interface}{いんたーふぇーす@インターフェース}\ejindex{abstract data type|see{interface}}{ちゅうしょうでーたがた@抽象データ型}は、\emph{抽象データ型（abstract data type）}とも呼ばれ、あるデータ構造がサポートしている操作一式と、それらの操作の意味（セマンティクス）を定義するものである。インターフェースを見ても、データ構造がサポートしている操作がどう実装されているかはわからない。インターフェースからわかるのは、そのデータ構造がサポートしている操作の一覧と、それらの操作に対する引数および返り値の特徴だけである。 % types of arguments -> 型だけでなく色々な特徴がある

一方、データ構造の\emph{実装（implementation）}には、データ構造の内部表現と、実際に操作を行うアルゴリズムの定義が含まれる。そのため、1つのインターフェースに対して複数の実装がありうる。例えば本書では、\chapref{arrays}では配列を使って#List#インターフェースを実装し、\chapref{linkedlists}ではポインタを使って#List#インターフェースを実装する。どちらも同じ#List#インターフェースだが、実装の方法が異なるというわけだ。

\subsection{#Queue#、#Stack#、#Deque#インターフェース}

#Queue#インターフェースは、要素の集まりを表しており、その集まりに対して要素の追加および特定のルールに従った削除ができる。より正確に言うと、#Queue#インターフェースには次の操作が実行できる。

\begin{itemize}
  \item #add(x)#：値#x#を#Queue#に追加する
  \item #remove()#：（以前に追加された）「次の値」#y#を#Queue#から削除し、#y#を返す
\end{itemize}

#remove()#は引数を取らない。#Queue#では、さまざまな\emph{取り出し規則}に従って削除する要素が決まる。代表的な取り出し規則としては、FIFO、優先度付き、LIFO、といったものがある。 % queueing discipline = 取り出し規則

\figref{queue}に\emph{FIFOキュー}\ejindex{FIFO queue}{FIFOキュー} \ejindex{queue!FIFO}{きゅー@キュー!さきいれさきだし@先入れ先出し}を示す。FIFOはfirst-in-first-out（先入れ先出し）を意味し、追加したのと同じ順番で要素を削除する。これはコンビニのレジに並ぶ列と同じように動作する。最も一般的な#Queue#なので、FIFOを付けずに単に「キュー」といえば、ふつうはこのデータ構造のことを指す。FIFOキューにおける#add(x)#、#remove()#を、それぞれ#enqueue(x)#、#dequeue()#と呼ぶ流儀の教科書もある。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/queue}}
  \caption{FIFOキュー}
  \figlabel{queue}
\end{figure}

% TALK caprice 同点要素は「同じ優先度を持つ要素」に言い換えた、一般的な用語ではないので。
\figref{prioqueue}に\emph{優先度付きキュー（priority queue）}%
\ejindex{priority queue}{ゆうせんどつききゅー@優先度付きキュー}%
\ejindex{priority queue|seealso{heap}}{ゆうせんどつききゅー@優先度付きキュー|seealso{ヒープ}}%
\ejindex{queue!priority}{きゅー@キュー!ゆうせんどつき@優先度付き}%
を示す。
優先度付きキューでは、#Queue#から要素を削除するとき、最小のものを削除する。同じ優先度を持つ要素が複数あるときは、そのうちのどれを削除してもよい。優先度付きキューの動作は、病院の救急室で重症患者を優先的に治療する場面に似ている。患者が到着したらまず症状の深刻さを見定め、待合室で待機してもらい、医師の手が空いたら最も重篤な患者から治療するという具合だ。優先度付きキューにおける#remove()#操作を#deleteMin()#と呼ぶ流儀の教科書もある。 % TODO: YJ I have never seen deleteMin though.

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/prioqueue}}
  \caption{優先度付きキュー}
  \figlabel{prioqueue}
\end{figure}

キューに対する取り出し規則でもうひとつよく使うのは、\figref{stack}に示すLIFO（last-in-first-out、後入れ先出し）
\ejindex{LIFO queue}{LIFOキュー}%
\ejindex{LIFO queue|seealso{stack}}{LIFOキュー|seealso{スタック}}%
\ejindex{queue!LIFO}{きゅー@キュー!あといれさきだし@先入れ後出し}%
\ejindex{stack}{すたっく@スタック}%
だ。この\emph{LIFOキュー}では、最後に追加された要素が次に削除される。LIFOキューの動作は、皿を積んだ状態として視覚化できる。積み上げられた皿を1つずつ取るとき、皿は上から順に持っていく。この構造はとてもよく見かけるので、#Stack#（スタック）という特別な名前が付いている。#Stack#と呼ぶ場合は、#add(x)#と#remove()#のことを、それぞれ#push(x)#および#pop()#と呼ぶ。これによりLIFOとFIFOの取り出し規則を区別できる。

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/stack}}
  \caption{LIFOキュー（スタック）}
  \figlabel{stack}
\end{figure}

FIFOキューとLIFOキュー（スタック）を一般化した#Deque#というインターフェースもある。#Deque#は双方向キューと呼ばれ、先頭と末尾を持った要素の列を表しており、先頭または末尾に要素を追加できる。#Deque#における操作には、#addFirst(x)#、#removeFirst()#、#addLast(x)#、 #removeLast()#というわかりやすい名前が付いている。#addFirst()#および#removeFirst()#だけを使ってスタックを実装できることは覚えておくとよいだろう。一方、#addLast(x)#および#removeFirst()#だけを使えばFIFOキューを実装できる。 % back = 末尾?

\subsection{#List#インターフェース：線形シーケンス}
\seclabel{listintro}
この本には#Queue#（FIFOキュー）や#Stack#（LIFOキュー）、#Deque#といったインターフェースの話はあまり出てこない。なぜなら、これらのインターフェースは#List#インターフェースとしてまとめられるからだ。\figref{list}に#List#\ejindex{List@#List#}{りすと@リスト}インターフェースを示す。#List#インターフェースは、値の列$#x#_0,\ldots,#x#_{#n#-1}$と、その列に対する以下のような操作からなる。

\begin{enumerate}
  \item #size()#: リストの長さ#n#を返す
  \item #get(i)#: $#x#_{#i#}$の値を返す
  \item #set(i,x)#: $#x#_{#i#}$の値を#x#にする
  \item #add(i,x)#: #x#を#i#番め\footnote{コンピュータサイエンスでは序数を0から始めることがある。例えば、ここで配列の#i#番めの要素とは、先頭から数えて$i+1$個めの要素のことである。}として追加し、$#x#_{#i#},\ldots,#x#_{#n#-1}$を後ろにずらす。\\
    すなわち、$j\in\{#i#,\ldots,#n#-1\}$について$#x#_{j+1}=#x#_j$とし、#n#をひとつ増やし、$#x#_i=#x#$とする
  \item #remove(i)#: $#x#_{#i#}$を削除し、$#x#_{#i+1#},\ldots,#x#_{#n#-1}$を前にずらす。\\ 
    すなわち、$j\in\{#i#,\ldots,#n#-2\}$について$#x#_{j}=#x#_{j+1}$とし、#n#をひとつ減らす
\end{enumerate}

これらの操作を使って#Deque#インターフェースを実装できる。 % sufficient -> 十分という意味

\begin{eqnarray*}
  #addFirst(x)# &\Rightarrow& #add(0,x)# \\
  #removeFirst()# &\Rightarrow& #remove(0)#  \\
  #addLast(x)# &\Rightarrow& #add(size(),x)# \\
  #removeLast()# &\Rightarrow& #remove(size()-1)#
\end{eqnarray*}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/list}}
  \caption{#List#は$0,1,2,\ldots,#n#-1$で添字づけられた列を表現する。この#List#で#get(2)#を実行すると値$c$が返ってくる}
  \figlabel{list}
\end{figure}


以降の章では、#Queue#（FIFOキュー）、#Stack#（LIFOキュー）、#Deque#の各インターフェースについての話はほぼ出てこない。しかし、#Stack#と#Deque#という用語を「#List#インターフェースを実装したデータ構造」の名前として後の章で使うことがある。その場合は、#Stack#と#Deque#という名前で呼ぶデータ構造を使うことで、それぞれ#Stack#と#Deque#のインターフェースを非常に効率良く実装できるという事実を強調している。例えば、#ArrayDeque#は#List#インターフェースの実装であると同時に#Deque#の実装でもあり、#Deque#の操作をいずれも定数時間で実行できる\footnote{実行時間についてはこの章の後半で説明する。「定数時間で実行できる」とは、要素がいくつあっても一定の時間で実行できるということであり、非常に効率が良いことを表す。}。

% caprice unordered って入れたほうが USetのUが楽に理解できる
\subsection{#USet#インターフェース：順序付けられていない要素の集まり}

#USet#\index{USet@#USet#}インターフェースは、重複がなく順序付けられていない要素の集まりを表現する（#USet#のUはunorderedの意味）。#USet#インターフェースは数学における\emph{集合（set）}のようなものだ。#USet#には、#n#個の\emph{互いに相異なる}要素が含まれる。つまり、同じ要素が複数入っていることはない。また、#USet#では要素の並び順は決まっていない。#USet#には以下の操作を実行できる。

\begin{enumerate}
\item #size()#：集合の要素数#n#を返す
\item #add(x)#：要素#x#が集合に入っていなければ集合に追加する。\\
$#x# = #y#$を満たす集合の要素#y#が存在しないなら、集合に#x#を加える。#x#が集合に追加されたら#true#を返し、そうでなければ#false#を返す
\item #remove(x)#：集合から#x#を削除する。\\
$#x# = #y#$を満たす集合の要素#y#を探し、集合から取り除く。そのような要素が見つかれば#y#を、見つからなければ#null#\footnote{訳注：#null#とは何もないことを示す記号である。}を返す
\item #find(x)#：集合に#x#が入っていればそれを見つける。\\
$#x# = #y#$を満たす集合の要素#y#を見つける。そのような要素が見つかれば#y#を、見つからなければ#null#を返す
\end{enumerate}

上の定義で、探したい#x#と、見つかる（かもしれない）要素#y#とを、わざわざ区別する必要はないように感じるかもしれない。
これらを区別する理由は、別のもの（オブジェクト）である#x#と#y#とを、何らかの基準で等しいと判定したい場合があるからだ\javaonly{\footnote{Javaでは、クラスの#equals(y)#、#hashCode()#メソッドをオーバーライドするとこれを行える。}}。そのような判定ができると、キーを値に対応付けるインターフェースを実装するのに都合がいい。そうしたインターフェースは\emph{辞書（dictionary）}や\emph{マップ（map）}と呼ばれる。% TODO: YJ need better translation
\ejindex{dictionary}{じしょ@辞書}%
\ejindex{map}{まっぷ@マップ}%
% TODO 辞書、マップとハッシュテーブルの関わりを脚注で補足する

辞書（マップ）を作るために、まずは#Pair#\ejindex{pair}{ぺあ@ペア}という、\emph{キー}と\emph{値}が対になったオブジェクトを作る。2つの#Pair#は、キーが等しければ（その値が等しいかどうかにかかわらず）等しいとみなす。#Pair#である$(#k#,#v#)$を#USet#に入れてから、$#x#=(#k#,#null#)$として#find(x)#を実行すると、$#y#=(#k#,#v#)$が返ってくる。すなわち、キー#k#だけから値#v#が手に入る。

\subsection{#SSet#インターフェース：ソートされた要素の集まり}
\seclabel{sset}
% TODO caprice 全順序の脚注を入れる
\index{SSet@#SSet#}%
#SSet#インターフェースは順序付けされた要素の集まりを表現する（#SSet#のSはsortedの意味）。#SSet#には全順序集合の要素が入る。全順序集合とは、任意の2つの要素#x#と#y#について大小を比較できるような集合をいう。本書のサンプルコードでは、以下のように定義される#compare(x, y)#メソッドで比較を行うものとする。

\[
    #compare(x,y)#
      \begin{cases}
        {}<0 & \text{if $#x#<#y#$} \\
        {}>0 & \text{if $#x#>#y#$} \\
        {}=0 & \text{if $#x#=#y#$}
      \end{cases}
\]
\index{compare@#compare(x,y)#}%

#SSet#は、#USet#とまったく同じセマンティクスを持つ操作#size()#、#add(x)#、#remove(x)#をサポートする。#USet#と#SSet#の違いは#find(x)#にある。 % セマンティクス vs. 意味

\begin{enumerate}
\setcounter{enumi}{3}
\item #find(x)#: 順序付けられた集合から#x#の位置を特定する。\\
   すなわち$#y# \ge #x#$を満たす最小の要素#y#を見つける。
   もしそのような#y#が存在すればそれを返し、存在しないなら#null#を返す
\end{enumerate}

#SSet#の#find(x)#は\emph{後継探索（successor search）}\ejindex{successor search}{こうけいたんさく@後継探索}と呼ばれることがある。#x#に等しい要素がなくても意味のある結果を返すという点で、#USet#の#find(x)#とは異なる。

#USet#、#SSet#における#find(x)#の区別は、重要だが見落とされることが多い。#SSet#は、#USet#より機能が多いが、それだけ実装が複雑で実行時間が長くなりがちだ。例えば、この本で述べる#SSet#の#find(x)#の実装は、いずれも集合に含まれる要素数の対数オーダーの時間がかかる。一方、\chapref{hashing}の#ChainedHashTable#による#USet#の実装では、#find(x)#の実行時間の期待値は定数オーダーである。#USet#にはない#SSet#の機能が必要でない限り、#SSet#ではなく#USet#を使うほうがよいだろう。

\section{数学的背景}
この節では本書で使う数学の記法や基礎知識を復習する。例えば対数やビッグオー記法、確率論などについて説明する。知っておいてほしい項目をまとめるに留め、丁寧な手ほどきはしない。背景知識が足りないと感じた読者はコンピュータサイエンスで使う数学の良い（無料の）教科書を読んでほしい。必要に応じて適切な箇所を読み、練習問題を解いてみるとよいだろう
\cite{llm11}。

\subsection{指数と対数}

\ejindex{exponential}{しすうかんすう@指数関数}
$b^x$と書いて$b$の$x$乗を表す。$x$が正の整数なら、$b$にそれ自身を$x-1$回掛けた値になる。
\[
    b^x = \underbrace{b\times b\times \cdots \times b}_{x}
\]
$x$が負の整数なら、$b^x=1/b^{-x}$である。$x=0$なら、$b^x=1$である。
$b$が整数でないときも、指数関数$e^x$を使って冪乗を定義できる（$e$については後述する）。この$e^x$の定義は、指数級数による。こういう話をもっと知りたい人は微分積分学の教科書を読んでほしい。

\ejindex{logarithm}{たいすうかんすう@対数関数}
この本では、$\log_b k$と書いて\emph{$b$を底とする対数}を表す。これは次の式を満たす$x$として一意に決まる。
\[
    b^{x} = k
\]
底が2の対数を\emph{二進対数（binary logarithm）}という。この本に出てくる対数のほとんどは二進対数なので、底に何も書かずに$\log k$とある場合は、$\log_2 k$の省略記法とする。
\ejindex{binary logarithm}{にしんたいすう@二進対数}%
\ejindex{logarithm!binary}{たいすう@対数!にしん@二進}%

% TALK caprice 二分探索（英：binary search）と強調したい
対数の大雑把だがわかりやすいイメージを紹介しよう。$\log_b k$とは、$k$を何回$b$で割ると1以下になるかを表す数だと考えればよい。例えば、1回の比較で答えの候補を半分に絞り、最終的に答えの候補が1つに絞られるまでこれを繰り返すとして、最終的に何回の比較が必要になるかを見積もりたいとする。1回の比較で候補の数を$2$で割ることになるので、最初に$n+1$個の答えの候補があるなら、比較の回数は$\lceil \log_2(n+1) \rceil$以下だ（なお、このような手法を二分探索という）%
% TODO caprice ガウス記号の脚注をココにいれる
\footnote{訳注：$x$を実数とするとき、$\lceil x \rceil$は$x$以上の最小の整数を表す。$\lfloor x \rfloor$は$x$以下の最大の整数である。}。

\ejindex{natural logarithm}{しぜんたいすう@自然対数}%
\ejindex{logarithm!natural}{たいすう@対数!しぜん@自然}%
次のように定義される\emph{オイラーの定数（Euler's constant）}$e$を底とする対数もよく使う\footnote{訳注：$e$はネイピア数とも呼ぶ。}。そこで、$\log_e k$のことを$\ln k$と書き、\emph{自然対数（natural logarithm）}と呼ぶ。\ejindex{Euler's constant}{おいらーのていすう@オイラーの定数}%
\index{e@$e$ (Euler's constant)}%
\[
   e = \lim_{n\rightarrow\infty} \left(1+\frac{1}{n}\right)^n
   \approx  2.71828
\]
自然対数は、次の一般的な積分の値が$e$になることから、よく登場する。
\[
    \int_{1}^{k} 1/x\,\mathrm{d}x  = \ln k
\]
対数に関してよく使う操作は2つある。1つめは冪指数にある対数の除去だ。
\[
    b^{\log_b k} = k
\]
もう1つは底の変換操作だ。
\[
    \log_b k = \frac{\log_a k}{\log_a b}
\]
これら2つの操作を使うと、例えば自然対数と二進対数とを比較できる。
\[
   \ln k = \frac{\log k}{\log e} = \frac{\log k}{(\ln e)/(\ln 2)} =
    (\ln 2)(\log k) \approx 0.693147\log k
\]

\subsection{階乗}
\seclabel{factorials}

\ejindex{factorials}{かいじょう@階乗}
この本には\emph{階乗関数（factorials）}を使う場面がいくつかある。$n$が非負整数のとき、$n$の階乗$n!$は次のように定義される。
\[
   n! = 1\cdot2\cdot3\cdot\cdots\cdot n
\]
$n!$は、相異なる$n$要素の置換\ejindex{permutation}{ちかん@置換}の総数である。つまり、$n$個の要素を並べ変えたときの順列の総数が階乗になる。なお、$n=0$のとき、$0!$は1と定義される。

\ejindex{Stirling's Approximation}{すたーりんぐのきんじ@スターリングの近似}%
$n!$の大きさは\emph{スターリングの近似（Stirling's Approximation）}を使って見積もれる
\footnote{訳注：以下、スターリングの近似に関する議論は、初学者は飛ばしてもよいと思われる。}。%この訳注は不要そう -kshikano
\[
  n!
   = \sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\alpha(n)}
\]
ここで$\alpha(n)$は次の条件を満たす。
\[
   \frac{1}{12n+1} <  \alpha(n) < \frac{1}{12n}
\]
スターリングの近似を使って$\ln(n!)$の近似値も計算できる。
\[
   \ln(n!) = n\ln n - n + \frac{1}{2}\ln(2\pi n) + \alpha(n)
\]
% TALK caprice このカッコ内の記述についても「初学者は無視して良い」と脚注入れたい
（実際、$\ln(n!)=\ln 1 + \ln 2  + \cdots + \ln n$を$\int_1^n \ln n\,\mathrm{d}n = n\ln n - n +1$で近似するというのが、スターリングの近似の簡単な証明方法でもある。）

\ejindex{binomial coefficients}{にこうけいすう@二項係数}%
階乗関数に関連して、ここで\emph{二項係数（bonimial coefficients）}について説明する。$n$を非負整数、$k$を$\{0,\ldots,n\}$の要素とするとき、二項係数$\binom{n}{k}$は次のように定義される。
\[
   \binom{n}{k} = \frac{n!}{k!(n-k)!}
\]
二項係数$\binom{n}{k}$は、大きさ$n$の集合における大きさ$k$の部分集合の個数である。言い換えると、集合$\{1,\ldots,n\}$から相異なる$k$個の整数を取り出すときの場合の数を表す値と解釈できる。 % TODO: YJ 元の文でもi.e.とあるが、これは定義ではなく一つの例なのでは？すなわち、と言ってしまってよいのだろうか。「例えば」のほうが正確では。-- 「言い換えると」が適切だと思います -kshikano

\subsection{漸近記法}
\seclabel{asymptotic}

\ejindex{asymptotic notation}{ぜんきんきほう@漸近記法} \ejindex{big-Oh notation}{びっぐおーきほう@ビッグオー記法} \index{O@$O$ notation}
データ構造を分析するときは、さまざまな操作の実行時間について考察したい。しかし、正確な実行時間はコンピュータによって異なる。同じコンピュータ上でさえ実行のたびに異なるだろう。この本で操作の実行時間といったら、操作に際してコンピュータが実行する命令の数とする。この数を正確に計算するのは、単純なコードであっても困難な場合がある。そのため、正確な実行時間を求めるのではなく、\emph{漸近記法（asymptotic notation）}あるいは\emph{ビッグオー記法（big-Oh notation）}と呼ばれる方法で実行時間を見積もる。この方法では、ある関数$f(n)$について、次のように定義される関数の集合$O(f(n))$を考える。%節題でもある漸近記法という用語を前に出しておきます -kshikano
\[
   O(f(n)) = \left\{
     \begin{array}{l}
       g(n):\mbox{ある$c>0$と$n_0$が存在し、} \\
             \quad\mbox{任意の$n\ge n_0$について$g(n) \le c\cdot f(n)$を満たす}
     \end{array} \right\}
\]
イメージとしては、$n$が十分に大きいとき（つまりグラフの右のほうを見たとき）に$c\cdot f(n)$のほうが上にくるような関数$g(n)$を集めたものが集合$O(f(n))$だ。 % 「上から抑えられる」はすでに知識がある人には伝わるが、初学者には伝われないのでは。 % TODO: YJ need revision

漸近記法は、関数を単純な形にするのに使う。例えば、$5n\log n + 8n - 200$の代わりに$O(n \log n)$と書ける。これは次のように証明できる。
\begin{align*}
       5n\log n + 8n - 200
        & \le 5n\log n + 8n \\
        & \le 5n\log n + 8n\log n & \mbox{ $n\ge 2$のとき（このとき$\log n \ge 1$）}
            \\
        & \le 13n\log n
\end{align*}
$c = 13$および$n_0 = 2$とすれば、関数$f(n)= 5n \log n + 8n-200$が集合$O(n \log n)$に含まれることがわかる。

漸近記法の便利な性質をいくつか挙げる。
まずは、任意の定数$c_1 < c_2$について以下が成り立つ。
\[ O(n^{c_1}) \subset O(n^{c_2}) \]
続いて、任意の定数$ a, b, c> 0 $について以下が成り立つ。
\[ O(a) \subset O(\log n) \subset O(n^{b}) \subset O({c}^n) \]
これらの包含関係は、それぞれに正の値を掛けても保たれる。
例えば$n$を掛けると次のようになる。
\[ O(n) \subset O(n\log n) \subset O(n^{1+b}) \subset O(n{c}^n) \]
一般的な慣習に従って、本書でもビッグオー記法を濫用する。すなわち、$f_1(n) = O(f(n))$と書いて$f_1(n) \in O(f(n))$であることを表す。そして、「この操作の実行時間は集合$O(f(n))$に\emph{含まれる}」ことを、単に「この操作の実行時間は$O(f(n))$だ」と言う。
これらの表現を認めると、冗長な記述が不要になるし、一連の等式で漸近記法を使えるようになる。

ビッグオー記法を濫用することで、例えば次のような不思議な書き方ができる。
\[
  T(n) = 2\log n + O(1)
\]
これは正確に書くとこうなる。
\[
  T(n) \le 2\log n + [\mbox{$O(1)$のある要素]}
\]
$O(1)$という記法には別の問題もある。この記法には変数が入ってないので、どの変数が大きくなるのかわからないのだ。これは文脈から読み取る必要がある。上の例では、方程式の中に変数は$n$しかないので、$T(n)= 2 \log n + O(f(n))$のうちで$f(n) = 1$の場合であると読み取ることになる。

ビッグオー記法は、新しい記法でもコンピュータサイエンス独自の記法でもない。1894年には数学者のPaul Bachmannがこの記法を使っていた。その後しばらくして、コンピュータサイエンスにおいてアルゴリズムの実行時間を論ずる際に、この記法が非常に便利なことがわかったのだ。
次のコードを考えてみよう。

\javaimport{junk/Simple.snippet()}
\cppimport{ods/Simple.snippet()}

この関数を1回実行すると以下の処理が行われる。
\begin{itemize}
      \item 代入$1$回（#int\, i\, =\, 0#）
      \item 比較$#n#+1$回（#i < n#）
      \item インクリメント#n#回（#i++#）
      \item 配列のオフセット計算#n#回（#a[i]#）
      \item 間接代入#n#回（#a[i] = i#）
\end{itemize}
よって実行時間は以下のようになる。
\[
    T(#n#)=a + b(#n#+1) + c#n# + d#n# + e#n#
\]
$a$、$b$、$c$、$d$、$e$はプログラムを実行するマシンに依存する定数で、それぞれ代入、比較、インクリメント、配列のオフセット計算、間接代入にかかる実行時間を表す。たった2行のコードについて実行時間を表すのに、こうも複雑な式がいるようでは、さらに複雑なコードやアルゴリズムは到底扱えないだろう。ビッグオー記法を使えば、実行時間を次のように簡潔に表せる。
\[
    T(#n#)= O(#n#)
\]
この式は、簡潔な表現にもかかわらず、最初の式と同じくらいの内容を表している。正確な実行時間は定数$a$、$b$、$c$、$d$、$e$に依存しており、これらの値がすべて判明しないと知りようがないからだ。がんばって値を実測してみても、得られる結論はそのマシンでしか有効でない。

ビッグオー記法を使えば、より抽象的な分析ができ、より複雑な関数も扱える。2つのアルゴリズムの実行時間がビッグオー記法で同じなら、どちらが速いか優劣はつけられない。一方のアルゴリズムが速いマシンもあれば、もう一方のアルゴリズムが速いマシンもあるだろう。しかし、2つのアルゴリズムの実行時間がビッグオー記法で異なるなら、\emph{#n#が十分大きい場合}、実行時間が小さいアルゴリズムのほうがどのようなマシンにおいても速いといえる。

ビッグオー記法を使って2つの異なる関数を比べる例を\figref{intro-asymptotics}に示す。これは$f_1(#n#)=15#n#$と$f_2(n)=2#n#\log#n#$のグラフである。$f_1(#n#)$は複雑な線形時間アルゴリズムの実行時間を表し、$f_2(#n#)$は分割統治に基づくシンプルなアルゴリズムの実行時間を表している。これを見ると、#n#が小さいうちは$f_1(#n#)$のほうが$f_2(#n#)$より大きいが、#n#が大きくなると大小関係が逆転することがわかる。つまり、nが十分大きいなら、実行時間が$f_1(#n#)$であるアルゴリズムのほうが圧倒的に性能がよい。ビッグオー記法の式$O(#n#)\subset O(#n#\log #n#)$は、この事実を示している。

\begin{figure}
  \begin{center}
    \newlength{\tmpa}\setlength{\tmpa}{.98\linewidth}
    \addtolength{\tmpa}{-4mm}
    \resizebox{\tmpa}{!}{\input{images/bigoh-1.tex}}\\[4ex]
    \resizebox{.98\linewidth}{!}{\input{images/bigoh-2.tex}}
  \end{center}
  \caption{$15#n#$と$2#n#\log#n#$の比較}
  \figlabel{intro-asymptotics}
\end{figure}

多変数関数に対して漸近記法を使うこともある。標準的な定義はないようだが、この本では次の定義を用いる。
\[
   O(f(n_1,\ldots,n_k)) =
   \left\{\begin{array}{@{}l@{}}
             g(n_1,\ldots,n_k):\mbox{ある$c>0$と$z$が存在し、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k)\ge z$を満たす任意の$n_1,\ldots,n_k$について、} \\
             \qquad \mbox{$g(n_1,\ldots,n_k) \le c\cdot f(n_1,\ldots,n_k)$が成り立つ} \\
   \end{array}\right\}
\]
興味があるのは引数$n_1,\ldots,n_k$によって$g$が大きくなるときの状況であり、その状況はこの定義で把握できる。$f(n)$が$n$に関する増加関数なら、この定義は1変数の場合の$O(f(n))$の定義とも合致する。この本ではこの程度の考察で十分だが、教科書によっては多変数関数と漸近記法に別の定義を与えている可能性もあるので注意してほしい。

% TALK 原文がrandomizationなのでランダム性ではなく乱択化では？ -- 乱択化のほうがいいとおもいます -kshikano
\subsection{ランダム性と確率}
\seclabel{randomization}

\ejindex{randomization}{らんたくか@乱択化}%
\ejindex{probability}{かくりつ@確率}%
\ejindex{randomized data structure}{らんたくでーたこうぞう@乱択データ構造}%
\ejindex{randomized algorithm}{らんたくあるごりずむ@乱択アルゴリズム}%
この本で扱うデータ構造には\emph{乱択化（randomization）}を利用するものがある。乱択化では、格納されているデータや実行する操作に加えて、サイコロの出目も踏まえて実際の処理を決める。そのため、同じことをしても実行時間が毎回同じとは限らない。このようなデータ構造を分析するときは\emph{期待実行時間（expected running time）}を考えるのがよい。
\ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
\ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%

乱択化を利用するデータ構造における操作の実行時間は形式的には確率変数であり、その\emph{期待値（expected value）}を知りたい。可算個の事象全体を$U$とし、その上で定義された離散確率変数を$X$とすると、$X$の期待値$E[X]$は次のように定義される。
\ejindex{expected value}{きたいち@期待値}%
\[
    \E[X] = \sum_{x\in U} x\cdot\Pr\{X=x\}
\]
ここで、$\Pr\{\mathcal{E}\}$は事象$\mathcal{E}$の発生確率とする。この本におけるすべての例では、乱択化されたデータ構造におけるランダムな選択についてのみ、これらの確率が関係している。つまり、データ構造に入ってくるデータや実行される操作列がランダムであるような状況は想定していない。

期待値の最も重要な性質のひとつは\emph{期待値の線形性（linearity of expectation）}である。
\ejindex{linearity of expectation}{きたいちのせんけいせい@期待値の線形性}%
任意の2つの確率変数$X$と$Y$について次の式が成り立つ。
\[
   \E[X+Y] = \E[X] + \E[Y]
\]
より一般的には、任意の確率変数$ X_1,\ldots,X_k $について次の関係が成り立つ。
\[
   \E\left[\sum_{i=1}^k X_i\right] = \sum_{i=1}^k \E[X_i]
\]
期待値の線形性によって、（上の式の左辺のように）複雑な確率変数の期待値を、（右辺のような）より単純な確率変数の和に分解できる。

% indicator random variable: 標示確率変数という訳もある
便利でよく使う手法に、\emph{インジケータ確率変数（indicator random variable）}\ejindex{indicator random variable}{いんじけーたかくりつへんすう@インジケータ確率変数}と呼ばれる二値の変数を定義するというものがある。この二値変数は、何かを数えるときに役立つ。例を見るとよくわかるだろう。表と裏が等しい確率で出るコインを$k$回投げたとき、表が出る回数の期待値を知りたいとする。
\ejindex{coin toss}{こいんなげ@コイン投げ}
直観的な答えは$k/2$だが、これを期待値の定義を使って証明すると次のようになる。
\begin{align*}
   \E[X] & = \sum_{i=0}^k i\cdot\Pr\{X=i\} \\
         & = \sum_{i=0}^k i\cdot\binom{k}{i}/2^k \\
         & = k\cdot \sum_{i=0}^{k-1}\binom{k-1}{i}/2^k \\
         & = k/2
\end{align*}
% この「２項係数の性質」、原文自体が公式間違ってたのでプルリク送った上でここでも直しといた
この計算をするには、$\Pr\{X=i\} = \binom{k}{i}/2^k$および二項係数の性質$i\binom{k}{i}=k\binom{k-1}{i-1}$や$\sum_{i=0}^{k} \binom{k}{i} = 2^{k}$を知っている必要がある。

% TALK インジケータ確率変数（英：indicator random variables）としたい -- 単純な用語の不統一なので直してあります -kshikano
インジケータ確率変数と期待値の線形性を使えば、この期待値をはるかに簡単に求められる。$\{1,\ldots,k\}$の各$i$に対し、以下のインジケータ確率変数を定義する。
\[
    I_i = \begin{cases}
           1 & \text{$i$番めのコイントスの結果が表のとき} \\
           0 & \text{そうでないとき}
          \end{cases}
\]
そして、$I_i$の期待値を計算する。
\[ \E[I_i] = (1/2)1 + (1/2)0 = 1/2 \]
ここで、$X=\sum_{i=1}^k I_i$なので次のように所望の値が得られる。
\begin{align*}
   \E[X] & = \E\left[\sum_{i=1}^k I_i\right] \\
         & = \sum_{i=1}^k \E[I_i] \\
         & = \sum_{i=1}^k 1/2 \\
         & = k/2
\end{align*}
少し長い計算ではあるが、不思議な変数はどこにも出てこないし、込み入った確率の計算もない。また、各コイントスでは$1/2$の確率で表が出るので、試行回数の半分くらいは表が出るだろうという直観にも合致する。

\section{計算モデル}
\seclabel{model}

この本では、データ構造における操作の実行時間を理論的に分析する。その正確な分析には、計算についての数学的なモデルが必要だ。そのような数学的モデルとして、\emph{#w#ビットのワードRAM（word-RAM）}を使うことにする。
\ejindex{word-RAM}{わーどらむ@ワードRAM}%
\index{RAM}%
ここでいうRAMは、Random Access Machineの頭字語である。
#w#ビットのワードRAMモデルでは、それぞれに#w#ビットのワードを格納できるセルを集めたランダムアクセスメモリを使える。
\ejindex{word}{わーど@ワード}
これはすなわち、メモリの各セルで#w#桁の2進数を表せる、つまり集合$\{0,\ldots,2^{#w#}-1\}$のうちのいずれかひとつをメモリの各セルで表せるということである。

% TODO 剰余の脚注
ワードRAMモデルでは、ワードに対する基本的な操作に一定の時間を要する。ここでいう基本的な操作とは、算術演算（#+#、#-#、#*#、#/#、#%#）や比較（$<$、$>$、$=$、$\le$、$\ge$）、ビット単位の論理演算（ビット単位の論理積ANDや論理和OR、排他的論理和XOR）を指す。

ランダムアクセスメモリでは、どのセルも一定の時間で読み書きできる。コンピュータのメモリはメモリ管理システムによって管理されており、このメモリ管理システムを通じて必要なサイズのメモリブロックの割り当てや解除ができる。サイズ$k$のメモリブロックの割り当てには$O(k)$の時間がかかり、新しく割り当てられたメモリブロックへの参照（ポインタ）が返される。この参照は1ワードに収まるビットで表現できるものとする。

ワード幅#w#は、このモデルにとって重要なパラメータである。
この本では、#w#について、データ構造に格納されうる要素数が#n#であれば$#w# > \log #n#$ということしか仮定しない。これは控えめな仮定である。なぜなら、せめてこれが成り立たないと、データ構造の要素数を1ワードで表すことすらできないからである。

メモリ使用量はワード単位で測るので、データ構造で使うワード数がそのままデータ構造のメモリ使用量になる。この本のデータ構造は、すべて型#T#の値を格納し、#T#型の要素は1ワードのメモリで表現できると仮定する
\javaonly{（実際、Javaでは#T#型のオブジェクトの参照を格納しており、この参照は1ワードのメモリを占める。）}。

\javaonly{#w#ビットのワードRAMモデルは、$#w#=32$とすると、（32ビット）Java仮想マシン（JVM）によく似ている。}
\cpponly{#w#ビットのワードRAMモデルは、$#w#=32$または$#w#=64$とすると、現代のデスクトップコンピュータ環境によく似ている。}
すなわち、この本に載っているデータ構造は、いずれも一般的なコンピュータ上で動作するように実装できる。

% この文脈でのcomplexityは、「複雑性」より「計算量」のほうが一般的に思えます（Sipserの訳本では一貫して「複雑さ」と言っているけれど）。他章で「計算量」を使っているのと、この本では複雑性という訳語のほうた適切な話題も出ないので、「計算量」を提案します -kshikano
\section{正しさ、時間計算量、空間計算量}

データ構造の性能を考えるとき重要な項目が3つある。
% TALK caprice ここも英語を加えたい
\begin{description}
  \item[正しさ：]データ構造はそのインターフェースを正しく実装しなければならない。
  \item[時間計算量（time complexity）：]データ構造における操作の実行時間は短いほどよい。
  \item[空間計算量（space complexity）：]データ構造のメモリ使用量は小さいほどよい。
\end{description}

この本は入門書なので、上記のうち「正しさ」については大前提とする。つまり、不正確な出力が得られるデータ構造や、適切に更新されないデータ構造については考えない。
一方で、メモリ使用量を小さく抑えるための工夫を施したデータ構造については紹介していく。
通常、そうした工夫によって操作の（漸近的な）実行時間が変わることはないが、実際のデータ構造の動作が少し遅くなる可能性はある。

データ構造に関して実行時間を議論するときは、次の三種類のいずれかを保証するという話になることが多い。

% コメントにもありますが、実行時間を指す箇所はf(#n#)でなくO(f(#n#))とするほうがよいと思います。 -kshikano
\begin{description}
\item[最悪実行時間（worst-case running time）：]
  \ejindex{running time}{じっこうじかん@実行時間}%
  \ejindex{running time!worst-case}{じっこうじかん@実行時間!さいあく@最悪}%
  \ejindex{worst-case running time}{さいあくじっこうじかん@最悪実行時間}%
  実行時間に対する保証の中で、最も強力なもの。あるデータ構造の操作について最悪実行時間が$f(#n#)$であるといったら、そのような操作の実行時間が$f(#n#)$より長くなることは\emph{決して}ない。
\item[償却実行時間（amortized running time）：]
  % XXX: 原文には at most がついているが不要ではないか。O(f(n))と混同している？ -- O(f(n))にしたほうがいいと思います。さらにat mostの意味は「越えないこと」で表現 -kshikano
  \ejindex{running time!amortized}{じっこうじかん@実行時間!しょうきゃく@償却}%
  \ejindex{amortized running time}{しょうきゃくじっこうじかん@償却実行時間}%
  償却実行時間が$f(#n#)$であるとは、典型的な操作にかかるコストが$f(#n#)$を超えないことを意味する。
  より正確には、$m$個の操作にかかる実行時間を合計しても、$mf(#n#)$を超えないことを意味する。
  いくつかの操作には$f(#n#)$より長い時間がかかるかもしれないが、操作の列全体として考えれば、1つあたりの実行時間は$f(#n#)$という意味だ。 % TODO: YJ amortize = 償却: better translation? --「償却」が一般的な訳語だと思います。 -kshikano
\item[期待実行時間（expected running time）：]
  \ejindex{running time!expected}{じっこうじかん@実行時間!きたい@期待}%
  \ejindex{expected running time}{きたいじっこうじかん@期待実行時間}%
  期待実行時間が$f(#n#)$であるとは、実行時間が確率変数（\secref{randomization}を参照）であり、その確率変数の期待値が$f(#n#)$であることを意味する。
  この期待値を計算する際に考えるランダム性は、そのデータ構造内で起こる選択におけるランダム性である。
\end{description}

最悪実行時間、償却実行時間、期待実行時間の違いを理解するには、お金の例で考えてみるとよい。家を購入する費用について考えてみよう。 % finance

\paragraph{最悪コストと償却コスト}
\ejindex{amortized cost}{しょうきゃくこすと@償却コスト}%
家の価格が12万ドルだとする。毎月1200ドルを120ヶ月（10年）にわたって支払うという住宅ローンを組むことで、この家が手に入るとしよう。この場合、月額費用は最悪でも月1200ドルだ。

十分な現金を持っていれば、12万ドルの一括払いでこの家を買うこともできる。その場合、この家の購入代金を10年で償却すると考えて月額費用を計算すれば、以下のようになる。
\[
   120\,000\text{ドル} / 120\text{ヶ月} = \text{毎月}\$1\,000
\]
これはローンの場合に支払う月額1200ドルよりだいぶ少ない。

\paragraph{最悪コストと期待コスト}
次に、12万ドルの家に火災保険をかけることを考えてみよう。保険会社が何十万件もの事例を調べた結果、大多数の家では火事を起こさず、いくつかの家では煙による被害程度で済むボヤを起こし、ごく少数の家では全焼被害に至ることがわかった。保険会社は、この情報に基づいて12万ドルの家における火災被害額の期待値を月額10ドル相当と判断し、自社の儲けを考慮して、火災保険の掛金を月額15ドルに設定した。あなたは保険会社に勤めていて、それらの数字を知っていたと仮定しよう。

決断のときだ。最悪コストが月額15ドルのこの火災保険に入るべきだろうか？それとも、期待コストである月額10ドル
% \footnote{訳注：この期待コストは保険会社しか知らないはずだが、それをなぜ被保険者が知っているかは不問とする。この例の目的は現実の近似ではなく、異なる種類のコスト間に生じるトレードオフを知ることである。}
% この訳注は不要だと思う。これが気になる人も多くなさそうだし、ここでこの問題に目を向けさせる意味もなさそう % caprice いや、これは重要。僕は気になったしそのせいでこの例を読む際に白けた。簡単のため被保険者が保険会社勤務との仮定を追加した
を自分で積み立てることにして、月額5ドルを節約するという賭けに出るべきだろうか？
明らかに、自分で積み立てるほうが安上がりになると期待できるが、いざという場合のコストがはるかに高くなる可能性を考慮しなければならない。すなわち、低い確率ではあるが、家が全焼して実際のコストが12万ドルになる可能性がある。

この2つの例からわかるように、どちらのコストを優先するかは場合によって変わる
\footnote{訳注：火災保険の例では、いざという場合のコストが大幅に低くなること、月額の差が5ドルと比較的少額であることから、火災保険を選ぶ人が多いかもしれない。しかし驚くべきことに\emph{データ構造の世界では、最悪実行時間よりも償却実行時間や期待実行時間が低いことを優先する}ことも多い。いざという場合の損害が家屋の全焼ほど大きくなく、また、その確率も家屋の全焼よりはるかに小さく制御できることが多いからだろう。} % XXX: この訳注は疑問。最悪実行時間が一番基本的でかつ実用的にも重要だと僕は思う -> 少し表現を弱くしました
。
償却実行時間と期待実行時間は、最悪実行時間より小さいことが多い。最悪実行時間の長さに目をつむり、償却実行時間や期待実行時間が小さいからと妥協すれば、はるかに単純なデータ構造を採用できる場合がよくあるのだ。

\section{コードサンプル}
\pcodeonly{
この本のコードサンプルは擬似コードで書いた。
\ejindex{pseudocode}{ぎじこーど@擬似コード}%
ここ40年に登場した一般的なプログラミング言語の経験がある人であれば理解しやすいコードを書いたつもりである。
この本におけるコードがどんなものかは、配列#a#の平均値を計算する次の擬似コードを見てみるとわかるだろう。
\pcodeimport{ods/Algorithms.average(a)}
このコードでは、変数への代入を表す記法は$\gets$である。
% WARNING: graphic typesetting of assignment operator
配列#a#の大きさを#len(a)#と書き、配列の添字は0から開始するものとする。
このとき、#range(len(a))#は#a#の正しい添字の集まりである。
コードを短く、また読みやすくするために、部分配列代入を使うことがある。
次の2つの関数は同じことをしている。
\pcodeimport{ods/Algorithms.left_shift_a(a).left_shift_b(a)}
以下のコードは配列のすべての要素を0にするものである。
\pcodeimport{ods/Algorithms.zero(a)}
このようなコードの実行時間を解析するときは、#a[0:len(a)] = 1#や#a[1:len(a)] = a[0:len(a)-1]#のような文が定数時間では実行できないことに気をつけなければならない。
これらの実行時間は$O(#len(a)#)$である。

変数の代入においても同様の簡略記法を使う。
#x,y=0,1#は、#x#を0に#y#を1にする。
また#x,y = y,x#は変数#x#と#y#の値を入れ替える。
\ejindex{swap}{すわっぷ@スワップ}

この本の擬似コードで使っている記法のうちいくつかは、馴染みが薄い人がいるかもしれない。
数学における（ふつうの）割り算の演算子は、この本の疑似コードでは$/$と書いている。
整数の除算が必要な場面も多く、これは$#//#$演算子で表している。
$#a//b# = \lfloor a/b\rfloor$は、$a/b$の整数部分を表す。
そのため、例えば$3/2=1.5$だが、$#3//2# = 1$である。
\ejindex{integer division}{せいすうじょさん@整数除算}%
\ejindex{div operator}{じょさんえんざんし@除算演算子}%
整数除算における余りを計算する$\bmod$演算子を使うこともあるが、これは実際に使う場面になってから定義する。
\ejindex{mod operator}{じょうよえんざんし@剰余演算子}%
ビット単位の演算子を使うこともある。
具体的には、左シフト（#<<#）、右シフト（#>>#）、ビット単位の論理積（#&#）、ビット単位の排他的論理和（#^#）を使う。
\ejindex{left shift}{ひだりしふと@左シフト}%
\ejindex{#<<#|see {left shift}}{#<<#|see{ひだりしふと@左シフト}}%
\ejindex{right shift}{みぎしふと@右シフト}%
\ejindex{#>>#|see {right shift}}{#>>#|see {みぎしふと@右シフト}}%
\ejindex{bitwise and}{びっとたんいろんりせき@ビット単位論理積}%
\ejindex{#&#|see {bitwise and}}{#&#|see {びっとたんいろんりせき@ビット単位論理積}}%
\ejindex{#^#|see {bitwise exclusive-or}}{#^#|see {びっとたんいろんりせき@ビット単位論理積}}%

この本の擬似コードはPythonのコードから機械的に翻訳したものである。
もとになるPythonコードは書籍のWebサイト\footnote{ \url{http://opendatastructures.org}}からダウンロードできる。
もし擬似コードに曖昧な点があれば、対応するPythonのコードを参照してほしい。Pythonが読めなければ、JavaとC++で書かれたコードもある。擬似コードの意味がわからず、PythonもC++もJavaも読めないなら、まだこの本を読むのは早いかもしれない。}

\notpcode{
この本のサンプルコードは\lang{}で書いた。
ただし、\lang{}に親しみのない人でも読めるよう、簡潔に書いたつもりだ。
例えば、#public#や#private#は出てこない。オブジェクト指向を前面に押し出すこともない。

B、C、C++、C\#、Objective-C、D、Java、JavaScriptといったALGOL系の言語を書いたことのある人なら、本書のコードの意味はわかるだろう。
完全な実装に興味がある読者は、この本に付属する\lang{}のソースコードを見てほしい。

この本には、数学的な実行時間の解析と、対象のアルゴリズムを実装した\lang{}のコードが両方とも含まれている。そのため、ソースコードと数式とで同じ変数が出てくる。
このような変数は同じ書式で書く\footnote{訳注：ソースコード中に現れる変数名は、英語のままにしている。}。
特によく出てくるのは、変数#n#\index{n@#n#}である。
#n#は常にデータ構造に格納されている要素の個数を表す。
}

\section{データ構造の一覧}

\tabref{summary-i}と\tabref{summary-ii}に、この本で扱うデータ構造の性能を要約する。これらは、\secref{interface}で説明した#List#、#USet#、#SSet#を実装する。
\Figref{dependencies}には、この本の各章の依存関係を示す。
\ejindex{dependencies}{いぞんかんけい@依存関係}%
破線の矢印は、章のごく一部の内容や結果のみに依存することを示す。

\begin{table}
\caption{#List#、#USet#の実装の要約}
\tablabel{summary-i}
\begin{center}
\setlength{\extrarowheight}{3pt}
\begin{threeparttable}
\begin{tabular}{|l|l|l@{\hspace*{1em}}|l|} \hline
\multicolumn{4}{|c|}{#List#の実装} \\ \hline
% XXX: この本の定義に従うとき、二変数のO記法の中に+1を添える必要はあるか？
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# & \\ \hline
#ArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A} & \sref{arraystack} \\
#ArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{arraydeque} \\
#DualArrayDeque# & $O(1)$ & $O(1+\min\{#i#,#n#-#i#\})$\tnote{A} & \sref{dualarraydeque}\\
#RootishArrayStack# & $O(1)$ & $O(1+#n#-#i#)$\tnote{A}  & \sref{rootisharraystack} \\
#DLList# & $O(1+\min\{#i#,#n#-#i#\})$ & $O(1+\min\{#i#,#n#-#i#\})$  & \sref{dllist} \\
#SEList# & $O(1+\min\{#i#,#n#-#i#\}/#b#)$ & $O(#b#+\min\{#i#,#n#-#i#\}/#b#)$\tnote{A}  & \sref{selist} \\
#SkiplistList# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E}  & \sref{skiplistlist} \\ \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{#USet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#ChainedHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{hashtable} \\ 
#LinearHashTable# & $O(1)$\tnote{E} & $O(1)$\tnote{A,E} & \sref{linearhashtable} \\ \hline
\end{tabular}
\vspace{3pt}
\begin{tablenotes}[online,flushleft]
\item[A]{\emph{償却}実行時間を表す}
\item[E]{\emph{期待}実行時間を表す}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\begin{table}
\caption{#SSet#、優先度付き#Queue#の実装の要約}
\tablabel{summary-ii}
\begin{center}
\setlength{\extrarowheight}{3pt}
\begin{threeparttable}
\begin{tabular}{|l|l@{\hspace*{1em}}|l|l|} \hline
\multicolumn{4}{|c|}{#SSet#の実装} \\ \hline
 & #find(x)# & #add(x)#/#remove(x)# & \\ \hline
#SkiplistSSet# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{skiplistset} \\ 
#Treap# & $O(\log #n#)$\tnote{E} & $O(\log #n#)$\tnote{E} & \sref{treap} \\ 
#ScapegoatTree# & $O(\log #n#)$ & $O(\log #n#)$\tnote{A} & \sref{scapegoattree} \\
#RedBlackTree# & $O(\log #n#)$ & $O(\log #n#)$ & \sref{redblacktree} \\ 
#BinaryTrie#\tnote{Int} & $O(#w#)$ & $O(#w#)$ & \sref{binarytrie} \\ 
#XFastTrie#\tnote{Int} & $O(\log #w#)$\tnote{A,E} & $O(#w#)$\tnote{A,E} & \sref{xfast} \\ 
#YFastTrie#\tnote{Int} & $O(\log #w#)$\tnote{A,E} & $O(\log #w#)$\tnote{A,E} & \sref{yfast} \\ 
\javaonly{#BTree# & $O(\log #n#)$ & $O(B+\log #n#)$\tnote{A} & \sref{btree} \\ 
#BTree#\tnote{X} & $O(\log_B #n#)$ & $O(\log_B #n#)$ & \sref{btree} \\ } \hline
\multicolumn{4}{c}{} \\[2ex] \hline
\multicolumn{4}{|c|}{（優先度付き）#Queue#の実装} \\ \hline
 & #findMin()# & #add(x)#/#remove()# & \\ \hline
#BinaryHeap# & $O(1)$ & $O(\log #n#)$\tnote{A} & \sref{binaryheap} \\ 
#MeldableHeap# & $O(1)$ & $O(\log #n#)$\tnote{E} & \sref{meldableheap} \\ \hline
\end{tabular}
\vspace{3pt}
\begin{tablenotes}[online,flushleft]
\item[A]{\emph{償却}実行時間を表す}
\item[E]{\emph{期待}実行時間を表す}
\item[Int]{このデータ構造は#w#ビットで表現できる整数のみを格納できる}
\javaonly{\item[X]{これは外部メモリモデル（\chapref{btree}を参照）での実行時間である}}
\end{tablenotes}
%\renewcommand{\thefootnote}{\arabic{footnote}}
\end{threeparttable}
\end{center}
\end{table}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/dependencies}
  \end{center}
  \caption{この本の内容の依存関係}
  \figlabel{dependencies}
\end{figure}

\section{ディスカッションと練習問題}

\secref{interface}で説明した#List#、#USet#、#SSet#の各インターフェースは、Java Collections Framework \cite{oracle_collections}の影響を受けている。
\index{Java Collections Framework}%
これらは、Java Collections Frameworkにおける#List#、#Set#、#Map#、#SortedSet#、#SortedMap#を単純化したものだと考えられる。
\javaonly{付属のソースコードには、#USet#、#SSet#の実装を#Set#、#Map#、#SortedSet#、#SortedMap#の実装にするためのラッパークラスが含まれている。}

この章で扱った漸近記法、対数、階乗、スターリングの近似、確率論の基礎などは、Leyman, Leighton, and Meyerによる素晴らしい（そして無料の）書籍\cite{llm11}で扱われている。
微積分のわかりやすい無料の教科書としては、Thompsonによる古典的な教科書\cite{t14}がある。この本には指数や対数の形式的な定義が書かれている。

確率論の基礎については、特にコンピュータサイエンスに関連するものとして、Rossの教科書\cite{r01}がおすすめである。
漸近記法や確率論については、Graham, Knuth, and Patashnikの教科書\cite{gkp94}も参考になるだろう。

\javaonly{Javaのプログラミング力を磨きたい読者のためには、オンラインのJavaのチュートリアル~\cite{oracle_tutorials}がある。}

\begin{exc}
この練習問題は、読者が問題に対する正しいデータ構造を選ぶ練習をするためにある。
利用可能な実装やインターフェース（JavaならばJava Collections Framework、C++ならばStandard Template Library）があれば、それを使って解いてみてほしい。

以下の問題は、テキストの入力を1行ずつ読み、各行で適切なデータ構造の操作を実行することで解いてほしい。ファイルが百万行であっても数秒以内に処理できる程度には効率的な実装にすること。

  \begin{enumerate}
    \item 入力を1行ずつ読み、その逆順で出力せよ。すなわち、最後の入力行を最初に書き出し、最後から2行めを2番めに書き出す、というように出力せよ。

    \item  先頭から50行の入力を読み、それを逆順で出力せよ。その後、続く50行を読み、それを逆順で出力せよ。これを読み取る行がなくなるまで繰り返し、最後に残った行（50行未満かもしれない）もやはり逆順で出力せよ。

      つまり、出力は50行めから始まり、49行め、48行め、…、1行めが続く。
	  その次は、100行め、99行め、…、51行めが続く。

	 なお、プログラムの実行中に50行より多くの行を保持してはならない。

    \item 入力を1行ずつ読み取り、42行め以降で空行を見つけたら、その42行前の行を出力せよ。例えば、242行めが空行であれば200行めを出力せよ。
	なお、プログラムの実行中に43行以上の行を保持してはならない。

    \item 入力を1行ずつ読み取り、それまでに読み込んだことがある行と重複しない行を見つけたら出力せよ。
	重複が多いファイルを読む場合でも、重複なく行を保持するのに必要なメモリより多くのメモリを使わないように注意せよ。

    \item 入力を1行ずつ読み取り、それまでに読み込んだことがある行と同じなら出力せよ（最終的には、ある行が入力ファイルに初めて現れた箇所をそれぞれ除いたものが出力になる）。
	重複が多いファイルを読む場合でも、重複なく行を保持するのに必要なメモリより多くのメモリを使わないように注意せよ。

    \item 入力をすべて読み取り、短い順に並べ替えて出力せよ。
	同じ長さの行があるときは、それらの行は辞書順に並べるものとする。
	また、重複する行は一度だけ出力するものとする。

    \item 直前の問題で、重複する行については現れた回数だけ出力するように変更せよ。

    \item 入力をすべて読み取り、すべての偶数番めの行を出力したあとに、すべての奇数番めの行を出力せよ（最初の行を0行めと数える）。

    \item 入力をすべて読み取り、ランダムに並べ替えて出力せよ。
	どの行の内容も書き換えてはならない。
	また、入力より行が増えたり減ったりしてもいけない。
  \end{enumerate}
\end{exc}

\begin{exc}
  \index{Dyck word}%
  \emph{Dyck word}とは、$+1$と$-1$からなる列で、先頭から任意の#k#番めの値までの部分列（プレフィックス）の和がいずれも非負なものとする。
  例えば、$+1,-1,+1,-1$はDyck wordだが、$+1,-1,-1,+1$は$+1-1-1<0$なのでDyck wordではない。
  Dyck wordと、スタックの#push(x)#操作および#pop()#操作との関係を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{matched string}{まっちしたもじれつ@マッチした文字列}%
  \ejindex{string!matched}{もじれつ@文字列!マッチした}%
  \emph{マッチした文字列}とは\{, \}, (, ), [, ]からなる列で、すべての括弧が適切に対応しているものとする。
  例えば、「\{\{()[]\}\}」はマッチした文字列だが、「\{\{()]\}」は2つめの\{に対応する括弧が]であるためマッチした文字列ではない。
  長さ#n#の文字列が与えられたとき、この文字列がマッチしているかどうかを$O(#n#)$で判定するのにスタックをどう使えばよいかを説明せよ。
\end{exc}

\begin{exc}
  #push(x)#操作と#pop()#操作のみが可能なスタック#s#が与えられたとする。
  FIFOキュー#q#だけを使って#s#の要素を逆順にする方法を説明せよ。
\end{exc}

\begin{exc}
  \ejindex{Bag@#Bag#}{ばっぐ@バッグ}%
  #USet#を使って#Bag#を実装せよ。
  #Bag#は、#USet#によく似たインターフェースで、#add(x)#操作、#remove(x)#操作、#find(x)#操作をサポートする。
  #USet#との違いは、#Bag#では重複する要素も格納する点である。
  #Bag#の#find(x)#操作では、#x#に等しい要素が1つ以上含まれているとき、そのうちの1つを返す。
  さらに、#Bag#は#findAll(x)#操作もサポートする。
  これは、#Bag#に含まれる#x#に等しいすべての要素のリストを返す操作である。
\end{exc}

\begin{exc}
  #List#インターフェース、#USet#インターフェース、#SSet#インターフェースを実装せよ。
  効率的な実装でなくてもよい。
  ここで実装するものは、後の章のより効率的な実装の正しさや性能をテストするのに役立つ（最も簡単なのは要素を配列に入れておく方法だ）。
\end{exc}

\begin{exc}
  直前の問題の実装について、性能をアップする工夫として思いつくものをいくつか試みよ。
  実験してみて、#List#インターフェースの#add(i,x)#操作と#remove(i)#操作の性能がどう向上したかを考察せよ。
  どうすれば、#USet#インターフェースと#SSet#インターフェースの#find(x)#操作の性能を向上できそうか考えてみよ。
  この問題は、インターフェースの効率的な実装がどれくらい難しいかを実感するためのものである。
\end{exc}
