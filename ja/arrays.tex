\chapter{配列を使ったリスト}
\chaplabel{arrays}

この章では\emph{backing array}と呼ばれる配列にデータを入れて、#List#・#Queue#インターフェースを実装する方法を解説する
\footnote{訳注：管見によればbacking arrayの一般的な訳語は存在しない。意訳するならば「裏でも要素が一並びになっている配列」だろうか。頻出用語ではなく、単に配列(array)と読み替えて問題ないだろう。重要なことは、裏でも要素が一並びになっているため、この章で述べる利点・欠点が生じていることである。}。
\index{backing array}%
次の表は、この章で説明するデータ構造の操作時間を要約したものだ。

\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}

データをひとつの配列に入れるデータ構造には以下のような利点・欠点がある。

\begin{itemize}
  \item 配列の任意の要素に一定の時間でアクセスできる。
  そのため#get(i)#・#set(i,x)#を定数時間で実行できる。

  \item 配列は動的ではない。リストの真ん中付近に要素を追加・削除するためには、隙間を作ったり埋めたりするために多くの要素が移動してしまう。
  #add(i,x)#・#remove(i)#の実行時間が#n#と#i#に依存するのはこのためだ。

  \item 配列は伸び縮みしない。
  backing arrayのサイズより多くの要素をデータ構造に入れるときには、新しい配列を割当て古い配列の要素をそちらにコピーしなければならず、この操作のコストは大きい。
\end{itemize}

3つめは特に重要だ。
上記の表に記載された実行時間はbacking arrayの拡大・縮小にかかるコストを含んでいない。
後述するように、注意深く設計すれば、backing arrayの拡大・縮小のコストを加味しても\emph{平均的な}実行時間にはほぼ影響しない。
より正確に言うと、空のデータ構造からはじめて、#add(i,x)#・#remove(i)#を#m#回実行するときの、backing arrayを拡大・縮小するのにかかる時間の合計は$O(m)$である。
コストの大きい操作もあるが、#m#個の操作にわたって均せば、ひとつの操作あたりのコストは$O(1)$なのだ。

\cpponly{
この章およびこの本を通じて配列の要素数を保持すると便利である。
C++のふつうの配列は要素数を保持していないため、要素数を保持する配列クラス#array#を定義する。
このクラスは標準的なC++の配列#a#と整数#length#により簡単に実装できる。}
\cppimport{ods/array.a.length}
\cpponly{
#array#の大きさは作成時に指定する。
}
\cppimport{ods/array.array(len)}
\cpponly{配列の要素を添え字により指定できる。}
\cppimport{ods/array.operator[]}
\cpponly{また、ある配列を他の配列に割り当てる操作は、ポインタの操作により定数時間で実行できる。}
% XXX: このarray.operator=の実装は怪しい
\cppimport{ods/array.operator=}

\section{ArrayStack：配列を使った高速なスタック操作}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%

#ArrayStack#は\emph{backing array}（配列#a#と呼ぶことにする）を使った#List#インターフェースの実装だ。
リストの#i#番目の要素を#a[i]#とする。
ほとんどの場合、配列#a#は実際に必要な要素数よりも多くの要素を格納できる。
そのため整数#n#によって実際に#a#に入っている要素数を表す。
つまり、リストの要素は#a[0]#,\ldots,#a[n-1]#に格納されている。
また、関係$#a.length# \ge #n#$が常に成り立つ。

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{基本}
#get(i)#や#set(i,x)#を使って#ArrayStack#の要素を読み書きする方法は簡単である。
必要に応じて境界チェック\footnote{訳注：#get(i)#や#set(i,x)#における境界チェック(bounds-checking)とは、添字iが、最初の要素の添字である0以上であり、かつ、最後の要素の添字である#a.length - 1#以下だと確認することである。% 境界チェックを行なうとエラーは防げるが遅くなる。<- 知ってる人には冗長だし、知らない人は混乱しそうなので消しました。
}をしたあと単に#a[i]#を返すか、#a[i]#を書き換えるかすればよいのだ。
% 少なくともC++ではStroustrup教授が[]演算子については授業で言ってた

\codeimport{ods/ArrayStack.get(i).set(i,x)}

#ArrayStack#に要素を追加・削除するための実装を\figref{arraystack}に示す。
#add(i,x)#ではまず#a#が既に一杯かどうかを調べる。
もしそうなら#resize()#を呼び出して#a#を大きくする。
#resize()#の実装方法については後述する。
#resize()#を終えた直後には$#a.length# > #n#$になっていることを知っていれば今は十分である。
あとは#x#が入れるために$#a[i]#,\ldots,#a[n-1]#$をひとつずつ右に移動させ、#a[i]#を#x#にして、#n#を1増やせばよい。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption{#ArrayStack#に対する#add(i,x)#・#remove(i)#の実行例。矢印は要素のコピーを表す。#resize()#を呼ぶ操作にはアスタリスクを付した。}
  \figlabel{arraystack}
\end{figure}

% XXX: add はバグっているのではないか。
% n==a.length でも resize の必要があるのではないか
\codeimport{ods/ArrayStack.add(i,x)}
#resize()#のコストを無視すれば、#add(i, x)#のコストは#x#を入れる場所を作るためにシフトする要素数に比例する。つまり（resizeのことを無視した）この操作の実行時間は$O(#n#-#i#)$である。

#remove(i)#も同様に実装できる。
$#a[i+1]#,\ldots,#a[n-1]#$を左にひとつシフトし、#n#の値をひとつ小さくする。（#a[i]#は書き換えられる前に控えておく。）
そして、配列の長さに対して要素数が少なすぎないか、具体的には$#a.length# \ge 3#n#$かどうかを確認する。
もしそうなら#resize()#を呼んで#a#を小さくする。

\codeimport{ods/ArrayStack.remove(i)}
#resize()#が呼ばれるかもしれないが、そのコストを無視すれば#remove(i)#のコストはシフトする要素数に比例し、$O(#n#-#i#)$である。
% TODO サンプルコードが a[n-1]（最終要素）を NULL に書き換えておらず、a[n-2]とa[n-1]がダブっている！どうコードを書き換えるべき？ <- 今すぐStroustrup先生から宗旨変えして各操作に境界値チェックをつけよう！
\subsection{拡張と収縮}

#resize()#の実装は単純だ。
大きさ$2#n#$の新しい配列#b#を割当て、#n#個の#a#の要素を#b#のはじめの#n#個としてコピーする。そして#a#を#b#に置き換える。
よって#resize()#の呼び出し後は$#a.length# = 2#n#$が成り立つ。
\footnote{訳注：\lemref{arraystack-amortized}の証明でも言及されているが、$#n# = 0$ かつ $#a.length# = 1$のときに限ってこの式は成り立たないことがある。}

\codeimport{ods/ArrayStack.resize()}

#resize()#の実際のコストの計算も簡単だ。
大きさ$2#n#$の配列#b#を割当て、#n#個の要素をコピーする。
これは$O(#n#)$の時間がかかる。

前節からの実行時間分析は#resize()#のコストを無視していた。この節では\emph{償却解析}と呼ばれる手法でこれを解決する。
この手法は個々の#add(i, x)#・#remove(i)#における#resize()#のコストを求めるわけではない。
代わりに、#add(i, x)#・#remove(i)#からなる#m#個の一連の操作の間に呼ばれる#resize()#の実行時間の合計を考える。特に、次の補題を示す。
% 以上の部分はpdfにしたときに別段落に分けられていたので、原文に沿って一段落にまとめた
\begin{lem}\lemlabel{arraystack-amortized}
  空の#ArrayStack#が作られたあと、$m\ge 1$回の#add(i, x)#・#remove(i)#からなる操作の列が順に実行されるとき、#resize()#の実行時間は合計#O(m)#である。
\end{lem}

\begin{proof}
#resize()#が呼ばれるとき、その前の#resize()#呼び出しから#add#・#remove#が実行された回数は$#n#/2-1$回以上であることを後半で示す。

このとき、$i$回目の#resize()#呼び出しの際の#n#を$#n#_i$、$r$を#resize()#の呼び出し回数とすれば、#add(i,x)#と#remove(i)#の合計呼び出し回数は次の関係を満たす。
\[
  \sum_{i=1}^{r} (#n#_i/2-1) \le m \enspace
\]

これを変形すると次の式が得られる。
\[
  \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace
\]

$r \leq m$より#resize()#呼び出しのための実行時間の合計は次のようになる。
\[
\sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace
\]

あとは$(i-1)$回目の#resize()#から$i$回目の#resize()#の間に、#add(i,x)#か#remove(i)#が呼ばれる回数の合計が$#n#_i/2 - 1$以上であることを示せばよい。 % XXX 原文は at least ni/2 だったのだが、 これ文脈からして at least ni/2 - 1 では？ <- っぽいですね

ふたつの場合が考えられる。
ひとつは#resize()#が#add(i,x)#の中で呼ばれる場合で、これはbacking arrayが一杯になるとき、つまり$#a.length# = #n#=#n#_i$が成り立つときだ。
この一つ前に行った#resize()#操作について考えよう。この#resize()#の直後、#a#の大きさは#a.length#だが#a#の要素数は$#a.length#/2=#n#_i/2$以下であった。
しかし#a#の要素数は今では$#n#_i=#a.length#$なのだから、前の#resize()#から$#n#_i/2$回以上は#add(i,x)#が呼ばれたことがわかる。

もうひとつ考えられるのは#resize()#が#remove(i)#の中で呼ばれる場合で、このとき$#a.length# \ge 3#n#=3#n#_i$である。
ひとつ前の（つまり、i-1回目の）#resize()#の直後では#a#の要素数は$#a.length/2#-1$以上であった
\footnote{この数式における${}-1$は、特別なケース$#n#=0$かつ$#a.length# = 1$を考慮したものだ。}。
今#a#には$#n#_i\le#a.length#/3$個の要素が入っている。
よって、直前の#resize()#以降に実行された#remove(i)#の回数の下界は次のように計算できる。

  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1
  \end{align*}

いずれの場合でも、$(i-1)$から$i$回目の#resize()#の間に#add(i,x)#か#remove(i)#が呼ばれる回数の合計は$#n#_i/2-1$以上である。
\end{proof}

\subsection{要約}

次の定理は#ArrayStack#の性能を整理するものだ。

\begin{thm}\thmlabel{arraystack}
  #ArrayStack#は#List#インターフェースを実装する。
  #resize()#にかかる時間を無視した場合の#ArrayStack#における各操作の実行時間をまとめる。
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+#n#-#i#)$である
    \footnote{訳注：#n#=#i#=0のときであってもビッグオー記法で書けるように、1が足されていることに注意。}。
	% XXX: この1は必要なのか？
  \end{itemize}
  空の#ArrayStack#に対して任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このとき#resize()#にかかる時間の合計は$O(m)$である。
\end{thm}

#ArrayStack#というデータ構造は#Stack#インターフェースを実装する効率的な方法である。
特に#push(x)#は#add(n,x)#、#pop()#は#remove(n-1)#である。
またこのいずれの操作の償却実行時間も$O(1)$である。

\section{#FastArrayStack#：最適化されたArrayStack}
% TALK ここでは 「add(i, x)とremove(i)」のように「と（並立助詞）」を使っている一方で、
% これ以前は「add(i, x)・remove(i)」のように黒丸で並立している。「と」のほうが紛らわしさがないのでは？
% （・は乗算に見えなくもない）
#ArrayStack#が主にやっていることは、データの（#add(i,x)#と#remove(i)#のための）シフトと（#resize()#のための）コピーである。
素朴な実装では#for#ループを使うだろう。
しかしデータのシフトやコピーに特化したより効率的な機能が使えるかもしれない。
C言語には#memmove(d,s,n)#と#memcpy(d,s,n)#関数がある
\footnote{訳注：#memmove(d,s,n)#はdestination（移動先）にsource（移動元）からnバイトをコピーする関数である。#memcpy(d,s,n)#との違いは移動元と移動先の領域が重なっていてもよいことである。}。
C++言語には#std::copy(a0,a1,b)#アルゴリズムがある。
Javaには#System.arraycopy(s,i,d,j,n)#メソッドがある。
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\cppimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
\javaimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
% XXX: この add もバグっているのではないか。
% (1) n==a.length でも resize の必要があるのではないか
% (2) i==1 && n==0 のとき first > last だが大丈夫か?

これらの関数は最適化されており、#for#ループを使う場合と比べてかなり高速な機械語の命令を使うかもしれない。
これらの関数を使っても漸近的な実行時間は小さくならないのだが、試してみる価値のある最適化ではある。

C++やJavaの実装でこれらの関数を使って2〜3倍の高速化を実現したこともある。実際どのくらい速くなるのかは環境によるので是非試してみてほしい。

\section{ArrayQueue：配列を使ったキュー}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%
この節ではFIFO（先入れ先出し）キューを実装するデータ構造#ArrayQueue#を紹介する。
このデータ構造では（#add(x)#によって）要素は追加されたのと同じ順番で（#remove()#によって）削除される。

FIFOキューの実装に#ArrayStack#を使うのはよくない。
一方の端から要素を追加し他方から要素を削除するので、賢明な選択ではないのだ。
ふたつの操作のいずれかはリストの先頭を変更することになる。すなわち$#i#=0$で#add(i,x)#か#remove(i)#を呼びだす。
このとき#n#に比例する実行時間がかかってしまうのである。

配列を使ったキューの効率的な実装は、もし無限長の配列#a#があれば簡単だろう。
次に削除する要素を追跡するインデックス#j#と、キューの要素数#n#を記録しておけばよい。
そうすればキューの要素は以下の場所に入っている。
\[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \]
まず#j#, #n#を0に初期化する。
要素を追加するときは、#a[j+n]#に要素を入れて、#n#をひとつ増やす。
要素を削除するときは、#a[j]#から要素を取り出し、#j#をひとつ増やし、#n#をひとつ減らす。

この方法の明らかな問題点は無限長の配列が必要なことだ。
#ArrayQueue#は有限長の配列#a#と\emph{剰余算術}で無限長の配列を模倣する。
\index{modular arithmetic}%
剰余算術とは例えば時間の計算をするときに使っているものだ。
例えば10:00に5時間を足すと3:00になる。
形式的に書けば次のようになる。
\[
    10 + 5 = 15 \equiv 3 \pmod{12}
\]
数式の後半は「12を法として15と3は合同である」と読む。
$\bmod$は次のように二項演算と考えてもよい。
\[
   15 \bmod 12 = 3
\]

整数$a$と正整数$m$について、ある整数kが存在し$a = km + r$をみたす整数 #r# $\in \{0, \ldots, m-1 \} $を$a \bmod m $と書く。
簡単に言うと、$ r $とは$ a $を$ m $で割った余りである。
CやC++、Ruby、Javaなど多くのプログラミング言語ではmod演算子は\%で表される。 % TODO: YJ In C/C++ modulo is only defined where both a and m are positive integers. Modulo is implementation dependent if a is negative (it killed me a MONTH).

剰余算術は無限長の配列を模倣するのに便利である。
$#i#\bmod #a.length#$は常に$0,\ldots,#a.length-1#$の値を取ることを利用して、配列の中にキューの要素をうまく入れられるのだ。

\[
#a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]# \enspace
\]

これは#a#を\emph{循環配列}として使っている。
\index{circular array}%
\index{array!circular}%
配列の添字が$#a.length#-1$を超えると、配列の先頭に戻ってくるのである。

残りの問題は#ArrayQueue#の要素数が#a#の大きさを超えてはならないことだ。

\codeimport{ods/ArrayQueue.a.j.n}

#ArrayQueue#に対して#add(x)#・#remove()#からなる操作の列を実行する様子を\figref{arrayqueue}に示す。
#add(x)#はまず#a#が一杯かどうかを確認し、必要に応じて#resize()#を呼んで#a#の容量を増やす。
続いて#x#を#a[(j+n)%a.length]#に入れて、#n#をひとつ増やせばよい。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption{#ArrayQueue#に対する#add(x)#・#remove()#の実行例。矢印は要素のコピーを表す。#resize()#の発生する呼び出しにはアスタリスクを付した。}
  \figlabel{arrayqueue}
\end{figure}

\codeimport{ods/ArrayQueue.add(x)}
% XXX: この add もバグっているのではないか。
% (1) n==a.length でも resize の必要があるのではないか

#remove()#の実装では、まず#a[j]#をあとで返せるように保存しておく。
つづいてnを1減らし、jを1増やしi#a.length#で剰余を取る。
最後に保存しておいた#a[j]#を返す。
もし必要なら#resize()#を読んで#a#を小さくする。

\codeimport{ods/ArrayQueue.remove()}

最後になるが、#resize()#操作は#ArrayStack#のものとよく似ている。
大きさ$2#n#$の新しい配列#b#を割当て、
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
を
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
にコピーし、$#j#=0$とする。

\codeimport{ods/ArrayQueue.resize()}
% FIXME このコード、max(1, n*2)となってて他と揃ってないから、max(2*n,1)としたい
\subsection{要約}

次の定理は#ArrayQueue#の性能を整理するものだ。

\begin{thm}
  #ArrayQueue#は(FIFO)#Queue#インターフェースの実装である。
  #resize()#のコストを無視すると、#ArrayStack#は#add(x)#・#remove()#の実行時間は$O(1)$である。
  さらに、空の#ArrayStack#に対して長さ#m#の任意の#add(i,x)#・#remove(i)#からなる操作の列を実行するとき、#resize()#にかかる時間の合計は$O(m)$である。
\end{thm}

\section{ArrayDeque：配列を使った高速な双方向キュー}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
前節の#ArrayQueue#は、一方の端からは追加が、他方の端からは削除が効率的にできる列を表現するデータ構造だった。#ArrayDeque#は両端で効率的な追加と削除ができるデータ構造である。
#ArrayQueue#を表現するために使った循環配列をここでもまた使って#List#インタフェースを実装する。
\footnote{訳注：第一章で言及されたように、#ArrayDeque#は#List#インターフェースを実装するデータ構造である。#ArrayDeque#という名称は、#Deque#インターフェイスの全ての操作をO(1)で実装しているという事実を強調するために用いられている。}
\codeimport{ods/ArrayDeque.a.j.n}

#ArrayDeque#における#get(i)#と#set(i,x)#の実装は難しくない。
配列の要素$#a[#{#(j+i)#\bmod #a.length#}#]#$を読み書きすればよいのだ。

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

#add(i,x)#の実装はもう少し興味深い。
まず、#a#が一杯かどうかを確認し、必要に応じて#resize()#を呼ぶ。
ここで、#i#が小さいとき（0に近いとき）と大きいとき（#n#に近いとき）に、特に効率的に操作したいのだということを覚えておいてほしい。
続いて、$#i#<#n#/2$かどうかを確認する。
もしそうなら、$#a[0]#,\ldots,#a[i-1]#$をそれぞれひとつずつ左にずらす。
そうでないなら、$#a[i]#,\ldots,#a[n-1]#$をそれぞれひとつずつ右にずらす。
#add(i,x)#と#remove(x)#の説明として\figref{arraydeque}を見てほしい。
% TODO YJ 以下の図、コメント部分の端が切れてる
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption{#ArrayDeque#に対する#add(i, x)#・#remove(i)#の実行例。矢印は要素のコピーを表す。}
  \figlabel{arraydeque}
\end{figure}

\codeimport{ods/ArrayDeque.add(i,x)}

このようにシフトを行うことで、#add(i,x)#は$\min\{ #i#, #n#-#i# \}$より多くの要素をシフトしなくてよい。
よって#add(i,x)#の（#resize()#のことを無視した）実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。

#remove(i)#の実装も似たようなものだ。
$#a[0]#,\ldots,#a[i-1]#$をそれぞれ右にひとつずつシフトするか、$#a[i+1]#,\ldots,#a[n-1]#$をそれぞれ左にひとつずつシフトするか、$#i#<#n#/2$かどうかに応じてどちらかを行う。
やはり#remove(i)#も$O(1+\min\{#i#,#n#-#i#\})$だけの時間で要素を動かし終えることができる。

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{要約}

次の定理は#ArrayDeque#の性能を整理するものだ。
  #ArrayDeque#は#List#インターフェースを実装する。
  #resize()#のコストを無視すると、#ArrayDeque#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。
  \footnote{訳注：これらの結果から、#ArrayDeque#が確かに#Deque#インターフェースの全ての操作をO(1)で実現していることを確認できる。つまり、両端に対する#add(i,x)#・#remove(i)#の実行時間は、O(1)で済む。}
  \end{itemize}
  空の#ArrayDeque#から任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このとき#resize()#にかかる時間の合計は$O(m)$である。

\section{#DualArrayDeque#：2つのスタックから作った双方向キュー}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%

次は2つの#ArrayStack#を使って#ArrayDeque#に近い性能を示すデータ構造#DualArrayDeque#を紹介する。
#DualArrayDeque#の漸近的な性能は#ArrayDeque#より優れているわけではないのだが、2つのシンプルなデータ構造を組み合わせてより高度なデータ構造を作る良い例なのでここで学ぶ価値がある。

#DualArrayDeque#は、2つの#ArrayStack#を使ってリストを表現する。
#ArrayStack#では終端付近の要素を高速に修正できたことを思い出してほしい。
#DualArrayDeque#は#front#と#back#という名のふたつの#ArrayStack#を後ろ合わせに配置する。
そのため両端での高速な操作が可能だ。

\codeimport{ods/DualArrayDeque.front.back}

#DualArrayDeque#は要素数#n#を明示的に保持しない。
要素数は$#n#=#front.size()# + #back.size()#$によって求めることが出来るからだ。
ただし#DualArrayDeque#の解析では相変わらず#n#で要素数を表すことにする。

\codeimport{ods/DualArrayDeque.size()}

ひとつめの#ArrayStack#である#front#には$0,\ldots,#front.size()#-1$番目の要素を、逆さまの順番で格納する。
もうひとつの#ArrayStack#である#back#には$#front.size()#,\ldots,#size()#-1$番目の要素を普通の順番で格納する。
こうして、#front#か#back#に対する#get(i)#か#set(i,x)#を適切に呼び出すことで、#get(i)#・#set(i,x)#を$O(1)$の実行時間で実現できる。

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

#front#には逆順に要素を蓄えているので、インデックス$#i#<#front.size()#$は#front#の$#front.size()#-#i#-1$番目の要素である。

#DualArrayDeque#における要素の追加・削除は\figref{dualarraydeque}を見てほしい。
#add(i,x)#は#front#か#back#を必要に応じて操作する。

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dualarraydeque}
  \end{center}
  \caption{#DualArrayDeque#に対する#add(i,x)#・#remove(i)#の実行例。矢印は要素のコピーを表す。#balance()#の発生する呼び出しにはアスタリスクを付した。}
  \figlabel{dualarraydeque}
\end{figure}

\codeimport{ods/DualArrayDeque.add(i,x)}

#add(i,x)#は#front#と#back#のバランスを調整するために#balance()#を呼び出す。
#balance()#の実装は後で説明するが、$#size()#<2$であるときを除いて#front.size()#と#back.size()#が三倍以上離れないことを#balance()#は保証することを知っておけば十分である。
具体的には$3\cdot#front.size()# \ge #back.size()#$と$3\cdot#back.size()# \ge #front.size()#$であることを保証する。

つづいて#add(i,x)#のうち#balance()#のコストを無視した実行時間を求める。
$#i#<#front.size()#$のとき#add(i,x)#は$#front.add(front.size()-i-1,x)#$で実装できる。
#front#は#ArrayStack#なのでこのコストは次のようになる。
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(1+#i#) \enspace
  \eqlabel{das-front}
\end{equation}
一方
$#i#\ge#front.size()#$のとき#add(i,x)#は$#back.add(i-front.size(),x)#$で実装できる。
このコストは次のようになる。
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(1+#n#-#i#) \enspace
  \eqlabel{das-back}
\end{equation}

$#i#<#n#/4$のときはひとつめのケース\myeqref{das-front}に該当する。
$#i#\ge 3#n#/4$のときはふたつめのケース\myeqref{das-back}に該当する
\footnote{訳注：たとえばi=0かつn=0の場合は\myeqref{das-back}に該当する。}。
$#n#/4\le#i#<3#n#/4$のときは、#front#と#back#どちらを操作するかわからない。
しかし$#i#\ge #n#/4$かつ$#n#-#i#>#n#/4$なので、いずれの場合も実行時間は$O(#n#)=O(#i#)=O(#n#-#i#)$である。
% TALK O(n)=O(i)=O(n-i) って O(n)=O(i+1)=O(n-i+1)と書いたほうがわかり易くないか？
% せっかく (2.1)がfrontの場合、(2.2)がbackの場合と言っているのだから、
% 変に１を抜くより参照を明確にしたほうが読みやすいと思うのだが
以上をまとめると次のようになる。
% TODO caprice から YJ これ「実行時間」に直したいけど数式に日本語入れるとバグるんだっけ？保留
\[
     \mbox{Running time of } #add(i,x)# \le
          \left\{\begin{array}{ll}
            O(1+#i#) & \mbox{if $#i#< #n#/4$} \\
            O(#n#) & \mbox{if $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{if $#i# \ge 3#n#/4$}
          \end{array}\right . % YJ: 数式の最後の点はlatexのコンパイルのためのものなので必要
\]
ゆえに#add(i,x)#の実行時間は#balance()#のコストを無視すれば$O(1+\min\{#i#, #n#-#i#\})$である。

#remove(i)#の実行時間と解析は同様のため、省略する。

\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{バランスの調整}

最後に#add(i,x)#と#remove(i)#によって実行される#balance()#の説明をする。
この操作は#front#・#back#のどちらも大きく（または小さく）なりすぎないことを保証するものだ。
要素数が2以上のとき、#front#も#back#も$#n#/4$以上の要素を含むようにするのだ。
そうでないときは要素を動かして、#front#・#back#がそれぞれちょうど$\lfloor#n#/2\rfloor$・$\lceil#n#/2\rceil$個の要素を持つようにする。

\codeimport{ods/DualArrayDeque.balance()}

#balance()#の解析は簡単である。
#balance()#がバランス調整をするとき$O(#n#)$個の要素を動かすので$O(#n#)$の時間がかかる。
#balance()#は#add(i,x)#・#remove(i)#で毎回呼ばれるのでこれは一見好ましくない。
しかし次の補題より、#balance()#のための平均時間は定数であることがわかる。

\begin{lem}\lemlabel{dualarraydeque-amortized}
  空の#DualArrayDeque#に対して$m\ge 1$個の#add(i,x)#・#remove(i)#からなる操作の列を順に実行する。
  このとき全ての#balance()#の呼び出しの実行時間の合計は$O(m)$である。
\end{lem}

\begin{proof}
  #balance()#が要素を動かすとき、前に#balance()#が要素を動かしたときから呼ばれた#add(i,x)#・#remove(i)#の合計数は$#n#/2-1$以下であることを示す。
  \lemref{arraystack-amortized}の証明と同様に、これを示せば#balance()#の合計時間が$O(m)$であることを示すには十分である。

  ここでは
  \emph{ポテンシャル法}
  \index{potential}%
  \index{potential method}%
  として知られる手法を解析に用いる。
  #DualArrayDeque#の\emph{ポテンシャル}$\Phi$を#front#と#back#の要素数の差と定義する。
  \[  \Phi = |#front.size()# - #back.size()#| \enspace \]
  このポテンシャルの興味深い性質は、バランス調整を行わない#add(i,x)#・#remove(i)#の呼び出しはポテンシャルを最大でも1しか増やさないことだ。 % TODO caprice もっと自然に訳出したい

  次の式が成り立つことから、#balance()#が要素を動かした直後にはポテンシャル$\Phi_0$は1以下であることがわかるだろう。
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace\]

  要素を動かす#balance()#の直前には$3#front.size()# < #back.size()#$であったと仮定しても一般性を失わない。
  次の式が成り立つ。
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  このときのポテンシャル$\Phi_1$は次のように評価できる。
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  以上より、前に#balance()#によって要素を動かしてから、#add(i,x)#・#remove(i)#が呼ばれた回数は$\Phi_1-\Phi_0 > #n#/2-1$ 以上である。
\end{proof}

\subsection{要約}

次の定理は#DualArrayDeque#の性質をまとめたものだ。
\begin{thm}\thmlabel{dualarraydeque}
  #DualArrayDeque#は#List#インターフェースを実装する。
  #resize()#と#balance()#のコストを無視すると、#DualArrayDeque#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+\min\{#i#,#n#-#i#\})$である。
  \end{itemize}
  また、空の#DualArrayDeque#から任意の$m$個の#add(i,x)#と#remove(i)#からなる操作の列を実行したとき、#resize()#と#balance()#にかかる時間の合計は$O(m)$である。
\end{thm}

\section{RootishArrayStack：空間効率に優れた配列スタック}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%

ここまで紹介してきたデータ構造には共通の欠点がある。
データは1つか2つの配列に入れ、配列のサイズを変更しないようにしているので、配列に隙間が多い傾向がある点だ。
例えば#resize()#直後の#ArrayStack#では配列#a#は半分しか埋まっていない。
3分の1しか埋まっていないことさえある。

この節では、その無駄なスペースの問題を解決する#RootishArrayStack#というデータ構造を紹介する
\footnote{訳注：RootishArrayStackは前節までのデータ構造と比べると日常での遭遇頻度が極端に落ちる印象を受けるので、初学者は飛ばしてもよい。ただし課題設定と設計アイデアは興味深い。}。
#RootishArrayStack#は#n#個の要素を$O(\sqrt{#n#})$個の配列に格納する。
この配列ではデータが格納されていない場所は常に$O(\sqrt{#n#})$以下である。 % TODO: YJ 配列のlocation = 場所?
残りのすべての場所にはデータが入っているのだ。
つまり#n#個の要素を入れるとき、無駄になるスペースは$O(\sqrt{#n#})$以下である。

#RootishArrayStack#は\emph{ブロック}と呼ばれる#r#個の配列に要素を格納する。この配列は$0,1,\ldots,#r#-1$と添字付けられる。
\figref{rootisharraystack}を見てほしい。
ブロック$b$は$b+1$個の要素を含む。
すなわち、#r#個のブロックが含む要素数の合計は次のように計算できる。
\[
  1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
この式が成り立つのは\figref{gauss}を見ればわかるだろう。

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
  \end{center}
  \caption{#RootishArrayStack#に対する#add(i,x)#・#remove(i)#の実行例。矢印は要素のコピーを表す。}
  \figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/gauss}
  \end{center}
  \caption{白い正方形の数は合わせて$1+2+3+\cdots+#r#$である。斜線を引いた正方形の数も同じである。白い正方形と斜線を引いた正方形を合わせてできる、正方形全体は、$#r#(#r#+1)$個の正方形からなる。}
  \figlabel{gauss}
\end{figure}

リストの要素はブロック内に順番に配置される。
0番目の要素はブロック0に、1・2番目の要素はブロック1に、3・4・5番目の要素はブロック2に格納される。
ここで問題になるのは、全体で#i#番目の要素が、どのブロックのどの位置に入っているかをどう知るかである。

#i#に対応する要素がどのブロックに入っているかさえ分かれば、ブロック内の位置は簡単に計算できる。 % YJ 日本語にすると曖昧なので追記
インデックス#i#の要素が#b#番目のブロックに入っているなら、$0,\ldots,#b#-1$番目のブロックにおける要素数の合計は$#b#(#b#+1)/2$である。
そのため、#i#は
\[
     #j# = #i# - #b#(#b#+1)/2
\]
として#b#番目のブロックの#j#番目の位置に入っている
\footnote{訳注：たとえばb=2かつi=3のとき、j=3-3=0となり、iに対応するのは2番目のブロックの0番目の要素である。}
。
#b#を求めること、つまりどのブロックに入っているのかを計算する方法はもう少し難しい。
#i#以下のインデックスを持つ要素は$#i#+1$個ある。
一方で、$0,\ldots,b$番目のブロックに入っている要素数の合計は$(#b#+1)(#b#+2)/2$である。
よって、#b#は次の式を満たす最小の整数である。
\[
    (#b#+1)(#b#+2)/2 \ge #i#+1 \enspace
\]
この式は次のように変形できる。
\[
    #b#^2 + 3#b# - 2#i# \ge  0 \enspace
\]
対応する2次方程式$#b#^2 + 3#b# - 2#i# =  0$はふたつの解$#b#=(-3 + \sqrt{9+8#i#}) / 2$と$#b#=(-3 - \sqrt{9+8#i#}) / 2$を持つ。
ふたつめの解は常に負の値なので捨ててよい。
よって、解は$#b# = (-3 + \sqrt{9+8#i#}) / 2$である。
この解は一般に整数とは限らない。
しかし元の不等式に戻ると$#b# \ge (-3 + \sqrt{9+8i}) / 2$を満たす最小の#b#が欲しかったのであった。
これは次のように書ける。
\[
   #b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace
\]

\codeimport{ods/RootishArrayStack.i2b(i)}
% TODO caprice 天井関数についての訳注をcode内に
% with this out of the way は「これを片付けたことを踏まえて」ぐらいか？
このインデックスiからブロック番号bへの変換関数#i2b#を用いれば、#get(i)#と#set(i,x)#は簡単に実装できる。
まず#b#を計算し、そのブロック内のインデックス#j#を求め、適切な操作を実行すればよい。

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

この章のデータ構造のどれかを使って#blocks#リストを表現すれば、#get(i)#も#set(i,x)#も定数時間で実行できる。

#add(i,x)#はもう手慣れたものだろう。
まずデータ構造が一杯かどうか、つまり$#r#(#r#+1)/2 = #n#$かどうかを確認する。
もしそうなら#grow()#を呼び出し新たなブロックを追加する。
その後$#i#,\ldots,#n#-1$番目の要素をそれぞれ右にひとつずらし、新たな#i#番目の要素を入れるための隙間を作る。

\codeimport{ods/RootishArrayStack.add(i,x)}

#grow()#メソッドはやってほしいことをしてくれる、つまり新しいブロックを追加してくれる。

\codeimport{ods/RootishArrayStack.grow()}

#grow()#のコストを無視すると、#add(i,x)#の操作はシフト操作のコストを考えれば十分で、これは$O(1+#n#-#i#)$、すなわち#ArrayStack#と同じになる。

#remove(i)#は#add(i,x)#に似ている。
$#i#+1,\ldots,#n#$番目の要素をそれぞれ左にひとつずつシフトし、ふたつ以上の空のブロックがあれば#shrink()#を呼び出し、使われていないブロックをひとつだけ残して削除する。

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

ここでもまた、#shrink()#のコストを無視すれば#remove(i)#のコストはシフトのコストを考えれば十分で、これは$O(#n#-#i#)$である
\footnote{訳注：remove(#i#)の場合は常に$i<n$すなわち$n-i>0$が成り立つため、#add(i,x)#の計算量のように1を足す必要がない。}
。

\subsection{拡張・収縮の分析}

上の#add(i,x)#・#remove(i)#の解析では#grow()#・#shrink()#のことを考慮していなかった。
まず、#ArrayStack.resize()#とは違い、#grow()#と#shrink()#は要素をコピーしないことに注意する。
つまり大きさ#r#の配列を割り当て・解放するだけである。
環境によって、これは定数時間で実行できたり、#r#に比例する時間がかかったりする。

#grow()#・#shrink()#を呼んだ直後の状況はわかりやすい。
最後のブロックは空で、その他のブロックは一杯である。
そのため、次の#grow()#・#shrink()#が呼ばれるのは、少なくとも$#r#-1$回要素が追加・削除された後である。
よって、#grow()#・#shrink()#に$O(#r#)$だけ時間がかかっても、そのコストは$#r#-1$回の#add(i,x)#・#remove(i)#で償却され、#grow()#・#shrink()#の償却コストは$O(1)$である。

\subsection{領域使用量}
\seclabel{rootishspaceusage}

次に、#RootishArrayStack#が使用する余分な領域の量を分析する。
特に、RootishArrayStackが使用する領域のうち、配列の要素であって、現在はデータが入っていないものを数えたい。
これを\emph{無駄な領域}ということにする。
\index{wasted space}%
% 原文は「データ」ではなくa list elementだが、このRootish Array Stackの節の最初で[previous data structures] store their dataと呼んでいるので、データと表記したほうが分かりやすいとcapriceは判断した

XXX: 以下、原著怪しい(?)ので確認
% TODO YJ: 空間量の議論の部分が怪しいという話？

#remove(i)#は、#RootishArrayStack#が、一杯でないブロックを2つ以下しか持たないことを保証する。
よって#n#個の要素を含む#RootishArrayStack#が用意するブロック数を#r#とすれば次の関係が成り立つ。 % 用意 allocate?
\[
    (#r#-2)(#r#-1)/2 \le #n# \enspace
\]
ここでまた二次式の解を考えれば次の式が成り立つ。
\[
   #r# \le (3+\sqrt{1+8#n#})/2 = O(\sqrt{#n#}) \enspace
\]
末尾のブロックふたつの大きさは#r#と#r-1#なので、これらのブロックによって生じる無駄な領域の量は$2#r#-1 = O(\sqrt{#n#})$以下である。
もしこれらのブロックを（例えば）#ArrayStack#に入れれば、#r#個のブロックを入れる#List#による無駄な領域の量も$O(#r#)=O(\sqrt{#n#})$である。
#n#個の要素と関連情報を保持するのに必要なその他の領域は$O(1)$である。
以上より、#RootishArrayStack#の無駄な領域の量は合計$O(\sqrt{#n#})$である。

% TODO YJ: 例えばstackの長さがk -> k+1に伸びる度に配列をreallocateするとする戦略の方が各操作が終了した後の空間量は効率的であるが、ここでの主張は「ほんの一瞬でも」必要な空間量である。この戦略だと配列のコピーの一瞬だけk + (k+1)の領域が必要になる。よってこの戦略の空間量はO(n)である。
この空間領域量は、空からはじまり要素をひとつずつ追加できるデータ構造の中で最適であることを示す。
正確にいうと、#n#個の要素を追加する際にはどこかのタイミングで（ほんの一瞬かもしれないが）$\sqrt{#n#}$以上の無駄な領域が生じることを示す。

空のデータ構造に#n#個の要素をひとつずつ追加していくとする。
完了したときには、#r#個のブロックに分散して#n#個のアイテムがデータ構造に入っている。
$#r#\ge \sqrt{#n#}$なら、#r#個のブロックを追跡するために#r#個のポインタ（参照）を使い、ポインタは無駄な領域である
\footnote{訳注：たとえば続く第3章では、この考えを更に進めて#r#=#n#個のブロックを追跡するために#n#個のポインタを用いる、連結リストと呼ばれるデータ構造を紹介する。}
。
一方で$#r# < \sqrt{#n#}$なら鳩の巣原理より大きさ$#n#/#r# > \sqrt{#n#}$以上のブロックが存在する。
このブロックがはじめて割当てられた瞬間を考える。
このブロックは割当てられたとき空なので、$\sqrt{#n#}$の無駄な領域が生じている。
以上より、#n#個の要素を挿入するまでのあるタイミングでデータ構造は$\sqrt{#n#}$の無駄な領域を生じることが示された。

\subsection{要約}

次の定理は#RootishArrayStack#についての議論をまとめたものだ。
\begin{thm}\thmlabel{rootisharraystack}
  #RootishArrayStack#は#List#インターフェースを実装する。
  #grow()#・#shrink()#のコストを無視すると、#RootishArrayStack#における各操作の実行時間は、
  \begin{itemize}
    \item #get(i)#・#set(i,x)#の実行時間は$O(1)$である。
    \item #add(i,x)#・#remove(i)#の実行時間は$O(1+#n#-#i#)$である。
  \end{itemize}
  空の#RootishArrayStack#から任意の$m$個の#add(i,x)#・#remove(i)#からなる操作の列を実行する。このときすべての#grow()#・#shrink()#にかかる時間の合計は$O(m)$である。

  要素数#n#の#RootishArrayStack#が使う（ワード単位で測った）使用領域量\footnote{\secref{model}で説明した、どのようにメモリ量を測るかという話を思い出してほしい。}は$#n# +O(\sqrt{#n#})$である。
\end{thm}

\notpcode{
\subsection{平方根の計算方法}
% TODO caprice から spinute ここ index直しといて
\index{square roots}%
計算モデルについて学んだことのある読者は、上で説明された#RootishArrayStack#が途中で平方根を計算しているため、これまで使ってきたワードRAMモデル(\secref{model})という計算モデルでは表現し切れていないことに気づいたかもしれない
\footnote{訳注：計算モデルとは、計算の過程を理論的に調べるための道具である。我々はここまでwビットのワードRAMモデルを使用して#add(i, x)#などの過程にかかる操作コストを調べてきたが、\secref{model}で言及されたように、ワードRAMモデルの基本的な操作は算術演算、比較、ビット単位の論理演算であり、平方根を取る操作は含まれていない。すなわち我々は平方根を取る操作がどれくらいのコストを必要とするのかを知らない。}
。平方根を取る操作は基本的な操作であるとは一般的にみなされていないため、ワードRAMモデルの中に含まれていない。

この節では、平方根を取る操作が効率的に実装できることを示す。特に、「長さが$O(\sqrt{#n#})$の２つの配列#sqrttab#と#logtab#を作り、実行時間が$O(\sqrt{#n#})$であるような前処理」のあとで、どんな自然数$#x#\in\{0,\ldots,#n#\}$についても定数時間で$\lfloor\sqrt{#x#}\rfloor$が計算できることを示す。
% TALK これ、原文は integer (整数)になってるけど負を含まないから自然数のほうが良いか？

次の補題は、自然数#x#の平方根を計算する問題が、関連する値 #x'#の平方根を計算する問題に帰着できることを示している。

\begin{lem}\lemlabel{root}
   二つの数 $#x#\ge 1$ と $#x'#=#x#-a$ について、$0\le a\le\sqrt{#x#}$だと仮定する。このとき、
   $\sqrt{x'} \ge \sqrt{#x#}-1$が常に成り立つ。
\end{lem}

\begin{proof}
以下を示せばよい。
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
両辺の二乗を取ると、
\[
 #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
となり、整理すると
\[
 \sqrt{#x#} \ge 1
\]
となる。これはどんな $#x#\ge 1$についても常に成り立つ。
\end{proof}
% caprice これめっちゃ頭いいな　こんなん思いつくんかい

% \begin{lem}\lemlabel{root}
% Let $#x#\ge 1$ and let $#x'#=#x#-a$, where $0\le a\le\sqrt{#x#}$.  Then
%    $\sqrt{x'} \ge \sqrt{#x#}-1$.
% \end{lem}

% \begin{proof}
% It suffices to show that
% \[
% \sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
% \]
% Square both sides of this inequality to get
% \[
%  #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
% \]
% and gather terms to get 
% \[
%  \sqrt{#x#} \ge 1
% \]
% which is clearly true for any $#x#\ge 1$.
% \end{proof}

あらゆる自然数$#x#\in\{0,\ldots,#n#\}$の平方根を計算する問題を考えるにあたり、まずは一部のxについて考えることから始める。自然数xが$2^{#r#} \le
#x# < 2^{#r#+1}$を満たす、すなわち$\lfloor\log #x#\rfloor=#r#$を満たす場合を考える。このとき、自然数xは２進数表現を用いて$#r#+1$ビットで表せる
% TODO caprice この注いらないかもしれない　\footnote{訳注：２進数表現とは、0と1というふたつの数字を用いて整数を表す表記法である。10進法表記の0, 1, 2, 3, 4は、２進法表記では３ビットを用いると000, 001, 010, 011, 100として表記される。コンピュータは0と1の２つの状態を取れる部品を用いて数を表現するため、ある数を2進数表現した際に何ビット必要なのかが重要となる。}
。ここで、$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$について考えると、この#x'#は\lemref{root}を満たすため$\sqrt{#x#}-\sqrt{#x'#} \le 1$が成り立つ。更に、#x'#の下位$\lfloor #r#/2\rfloor$ビットは0である。すると残りのビット数から考えて、#x'#は
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
通りの値しか取れない
\footnote{訳注：中辺は単に左辺に２をかけて導出し、中辺から右辺にかけては$2^{#r#} \le#x# < 2^{#r#+1}$から導出される$2^{#r#/2} \le \sqrt{#x#}$を用いた。右辺から、#x'#の可能な値の数が$O(\sqrt{#n#})$でバウンドされていると分かる。}
。これは、可能な#x'#の値に関してそれぞれ$\lfloor\sqrt{#x'#}\rfloor$の値を格納する配列#sqrttab#を利用できることを意味している
\footnote{訳注：つまり、平方根はプログラム中で何千回も使いまわされる処理でありうるのだから、その処理を高速化できるように、空間計算量を少し犠牲にして中間結果を配列#sqrttab#に入れることで、時間計算量を改善しようという算段である。#x'#の数が高々$2^{#r#+1-\lfloor #r#/2\rfloor}$通り（たとえば一般的なコンピュータにおけるr=32の場合は10万通り程度）しかない、ビッグオー記法で表記すれば$2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\sqrt{#x#}$という結果から$O(\sqrt{#n#})$通りしかないのだから、この工夫は実を結ぶ。}
。もう少し正確に言えば、配列#sqrttab#の各要素の値は
\[
   #sqrttab#[i]
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
である。こうすることで、各要素#sqrttab#[i]はあらゆる$#x#\in\{i2^{\lfloor r/2\rfloor},\ldots,(i+1)2^{\lfloor r/2\rfloor}-1\}$について、$\sqrt{#x#}$の値からおよそ2しか離れていない。言い換えれば、配列要素$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$について、#s#は$\lfloor\sqrt{#x#}\rfloor$か、$\lfloor\sqrt{#x#}\rfloor-1$か、
$\lfloor\sqrt{#x#}\rfloor-2$のいずれかの値を取る
\footnote{x #>># n は右シフト演算と呼ばれ、xを表すビットそれぞれをnビットずつ右にずらす。算術上は、$x$から$x mod 2^n$を引いた（すなわち、下位nビットを全て0にした）あとに$2^n$で割った場合と同じ効果を持つ。}
。$(#s#+1)^2 > #x#$となるまで#s#をインクリメントすることで、xの平方根を下に丸めた自然数$\lfloor\sqrt{#x#}\rfloor$の値を特定できる。
% TALK sqrttab[i] is within 2 of root x ってあるけど、$\lfloor\sqrt{#x#}\rfloor-2$の値を取ったときは微妙に2を超えてないか？今は「およそ２」としたが、どうすべきか？

% TALK 他の場所でも「２進法表現」と訳されているようだが、「２進表記」か「２進記数法」のほうが良いか？

% TALK これ、r=64の場合、sqrttabのサイズが約85億と、果てしなく大きくなってしまわないか？そのことに関して注を入れた方がよいのでは？

Start by restricting the problem a little, and assume that $2^{#r#} \le
#x# < 2^{#r#+1}$, so that $\lfloor\log #x#\rfloor=#r#$, i.e., #x# is an
integer having $#r#+1$ bits in its binary representation.  We can take
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$.  Now, #x'# satisfies
the conditions of \lemref{root}, so $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Furthermore, #x'# has all of its lower-order $\lfloor #r#/2\rfloor$ bits
equal to 0, so there are only
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
possible values of #x'#.  This means that we can use an array, #sqrttab#,
that stores the value of $\lfloor\sqrt{#x'#}\rfloor$ for each possible
value of #x'#.  A little more precisely, we have
\[
   #sqrttab#[i] 
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
In this way, $#sqrttab#[i]$ is within 2 of $\sqrt{#x#}$ for all
$#x#\in\{i2^{\lfloor r/2\rfloor},\ldots,(i+1)2^{\lfloor r/2\rfloor}-1\}$.
Stated another way, the array entry 
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ is either equal to
$\lfloor\sqrt{#x#}\rfloor$,
$\lfloor\sqrt{#x#}\rfloor-1$, or
$\lfloor\sqrt{#x#}\rfloor-2$.  From #s# we can determine the value
of $\lfloor\sqrt{#x#}\rfloor$ by
incrementing #s# until 
$(#s#+1)^2 > #x#$.
} % notpcode

\javaimport{ods/FastSqrt.sqrt(x,r)}
\cppimport{ods/FastSqrt.sqrt(x,r)}
\notpcode{
Now, this only works for $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ and
#sqrttab# is a special table that only works for a particular value
of $#r#=\lfloor\log #x#\rfloor$.  To overcome this, we could compute
$\lfloor\log #n#\rfloor$ different #sqrttab# arrays, one for each possible
value of $\lfloor\log #x#\rfloor$. The sizes of these tables form an exponential sequence whose largest value is at most $4\sqrt{#n#}$, so the total size of all tables is $O(\sqrt{#n#})$.

However, it turns out that more than one #sqrttab# array is unnecessary;
we only need one #sqrttab# array for the value $#r#=\lfloor\log
#n#\rfloor$.  Any value #x# with $\log#x#=#r'#<#r#$ can be \emph{upgraded}
by multiplying #x# by $2^{#r#-#r'#}$ and using the equation
\[
    \sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
The quantity $2^{#r#-#r#'}x$ is in the range
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ so we can look up its square root
in #sqrttab#.  The following code implements this idea to compute
$\lfloor\sqrt{#x#}\rfloor$ for all non-negative integers #x# in the
range $\{0,\ldots,2^{30}-1\}$ using an array, #sqrttab#, of size $2^{16}$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x)}
\cppimport{ods/FastSqrt.sqrt(x)}
\notpcode{
Something we have taken for granted thus far is the question of how
to compute
$#r#'=\lfloor\log#x#\rfloor$.  Again, this is a problem that can be solved
with an array, #logtab#, of size $2^{#r#/2}$.  In this case, the
code is particularly simple, since $\lfloor\log #x#\rfloor$ is just the
index of the most significant 1 bit in the binary representation of #x#.
This means that, for $#x#>2^{#r#/2}$, we can right-shift the bits of
#x# by $#r#/2$ positions before using it as an index into #logtab#.
The following code does this using an array #logtab# of size $2^{16}$ to compute
$\lfloor\log #x#\rfloor$ for all #x# in the range $\{1,\ldots,2^{32}-1\}$.
} % notpcode
\javaimport{ods/FastSqrt.log(x)}
\cppimport{ods/FastSqrt.log(x)}
\notpcode{
Finally, for completeness, we include the following code that initializes #logtab# and #sqrttab#:
} % notpcode
\javaimport{ods/FastSqrt.inittabs()}
\cppimport{ods/FastSqrt.inittabs()}
\notpcode{
To summarize, the computations done by the #i2b(i)# method can be
implemented in constant time on the word-RAM using $O(\sqrt{n})$ extra
memory to store the #sqrttab# and #logtab# arrays.  These arrays can be
rebuilt when #n# increases or decreases by a factor of two, and the cost
of this rebuilding can be amortized over the number of #add(i,x)# and
#remove(i)# operations that caused the change in #n# in the same way that
the cost of #resize()# is analyzed in the #ArrayStack# implementation.
} % notpcode

\section{ディスカッションと練習問題}

この章で説明したデータ構造は伝承のようなものだ。
30年以上前の実装さえ見つかる。
例えば、ここで扱った#ArrayStack#・#ArrayQueue#・#ArrayDeque#の実装を簡単に一般化できるスタック・キュー・双方向キューがKnuth \cite[Section~2.2.2]{k97v1}により議論されている。

恐らくBrodnik \etal\ \cite{bcdms99}が#RootishArrayStack#を記述し、\secref{rootishspaceusage}で述べたような下界$\sqrt{n}$を示した最初の文献である。
彼らは他の洗練されたブロックサイズの選び方も示しており、これは#i2b(i)#の中で冪根の計算をせずに済むものだ。
このやり方では#i#番目の要素を含むブロックは$\lfloor\log (#i#+1)\rfloor$番目のもので、これは単に$#i#+1$の二進表現における最高位の桁である。
この計算をするための命令を提供するコンピュータ・アーキテクチャもある。
\javaonly{Javaでは、#Integer#クラスの#numberOfLeadingZeros(i)#メソッドにより、簡単に$\lfloor\log (#i#+1)\rfloor$を計算できる。}

#RootishArrayStack#に関連するデータ構造として、Goodrich and Kloss \cite{gk99}の二段階の\emph{階層ベクトル}がある。
\index{tiered-vector}%
この構造体は#get(i,x)#・#set(i,x)#を定数時間で実行できる。
#add(i,x)#・#remove(i)#の実行時間は$O(\sqrt{#n#})$である。
これと同じような実行時間は\excref{rootisharraystack-fast}で扱う#RootishArrayStack#のより練られた実装によっても達成できる。

\begin{exc}
  #List#の#addAll(i,c)#操作は#Collection# #c#の要素をすべてリストの#i#番目の位置に順に挿入する。
  （#add(i,x)#は$#c#=\{#x#\}$とした特殊な場合である。）
  この章で説明したデータ構造において
  #addAll(i,c)#を#add(i,x)#繰り返し実行して実装するのはなぜ効率がよくないのかを説明せよ。
  またより効率的な実装を考え、実装せよ。
\end{exc}

\begin{exc}
  \emph{#RandomQueue#}を設計・実装せよ。
  \index{RandomQueue@#RandomQueue#}%
  これは#Queue#インターフェースの実装で、#remove()#操作はそのときキューに入っている要素から一様な確率でひとつ選んで取り出すものである。
  （#RandomQueue#はカバンに要素を入れておき、中を見ずに適当に要素を取り出すようなものだと考えればよい。）

  ただし#RandomQueue#における#add(x)#・#remove()#の償却実行時間は定数でなければならないとする。
\end{exc}

\begin{exc}
  #Treque# (triple-ended queue)を設計・実装せよ。
  \index{Treque@#Treque#}%
  #Treque#は#List#の実装であって、
  #get(i)#・#set(i,x)#は定数時間で実行でき、
  #add(i,x)#・#remove(i)#の実行時間は次のように表せるものだ。
  \[
     O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace
  \]
  つまり、両端あるいは中央に近い位置の修正が高速なデータ構造である。
\end{exc}

\begin{exc}
  #rotate(a,r)#操作を実装せよ。
  配列#a#を「回転」する、すなわち$#i#\in\{0,\ldots,#a.length#\}$のすべてについて#a[i]#を$#a#[(#i#+#r#)\bmod #a.length#]$に動かすものだ。
\end{exc}

\begin{exc}
  #List#の回転操作#rotate(r)#を実装せよ。
  これはリストの#i#番目の要素を$(#i#+#r#)\bmod #n#$番目に移す。
  ただし#ArrayDeque#や#DualArrayDeque#に対しての#rotate(r)#の実行時間は$O(1+\min\{#r#,#n#-#r#\})$でなければならないとする。
\end{exc}

\notpcode{
\begin{exc}
% TODO: YJ I guess memcpy is not possible in Ruby?
%   この問は\lang\ 版からは除外されている。
  #ArrayDeque#を実装せよ。
  ただし、#add(i,x)#・#remove(i)#・#resize()#におけるシフト処理は高速な#System.arraycopy(s,i,d,j,n)#を利用して実現すること。
\end{exc}
}

\begin{exc}
  #%#演算を用いずに#ArrayDeque#を実装せよ。（この演算に多くの時間がかかる環境があるのだ。）
  #a.length#が2の冪なら次の式が成り立つことを利用してよい。
  \[  #k%a.length#=#k&(a.length-1)# \enspace
  \]
  なお#&#はビット単位のand演算オペレータである。
\end{exc}

\begin{exc}
  剰余演算を一切使わない#ArrayDeque#の実装を考えよ。
  すべてのデータは配列内の連続した領域に順番に並んでいることを利用してよい。
  データがこの配列の先頭・末尾の外にはみ出たときは、#rebuild()#操作を実行する。
  全ての操作の償却実行時間は#ArrayDeque#と同じになるように注意すること。

  \noindent ヒント：#rebuild()#の実装方法がポイントだ。
  データがどちらの端からもハミ出ない状態に$#n#/2$回以下の操作で辿りつかなければならない。

  実装したプログラムの性能を元の#ArrayDeque#と比較せよ。
  実装を（#System.arraycopy(a,i,b,i,n)#を使って）最適化し、#ArrayDeque#の性能を上回るかどうか確認せよ。
\end{exc}

\begin{exc}
  #RootishArrayStack#を修正し、無駄な領域量は$O(\sqrt{#n#})$だが、#add(i,x)#・#remove(i,x)#の実行時間が$O(1+\min\{#i#,#n#-#i#\})$であるデータ構造を設計・実装せよ。
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
  #RootishArrayStack#を修正し、無駄な領域量は$O(\sqrt{#n#})$だが、#add(i,x)#・#remove(i,x)#の実行時間が$O(1+\min\{\sqrt{#n#},#n#-#i#\})$であるデータ構造を設計・実装せよ。（\secref{selist}が参考になるだろう。）
\end{exc}

\begin{exc}
  #RootishArrayStack#を修正し、無駄な領域量は$O(\sqrt{#n#})$だが、#add(i,x)#・#remove(i,x)#の実行時間が$O(1+\min\{#i#,\sqrt{#n#},#n#-#i#\})$であるデータ構造を設計・実装せよ。（\secref{selist}が参考になるだろう。）
\end{exc}

\begin{exc}
  #CubishArrayStack#を設計・実装せよ。
  \index{CubishArrayStack@#CubishArrayStack#}%
  #CubishArrayStack#は#List#インターフェースを実装する三段階のデータ構造であって、無駄な領域量が$O(#n#^{2/3})$であるものだ。
  #get(i)#・#set(i,x)#は定数時間で実行できる。
  #add(i,x)#・#remove(i)#の償却実行時間は$O(#n#^{1/3})$である。
\end{exc}
